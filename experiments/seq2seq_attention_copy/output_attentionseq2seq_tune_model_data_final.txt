nohup: ignoring input
INFO:tensorflow:Loading config from /data/lily/dw633/seq2seq/nl2sql/experiments/seq2seq_attention_copy/AttentionSeq2Seq_tune_model_data/config/train.yml
INFO:tensorflow:Final Config:
batch_size: 16
buckets: 25,50,100,200
input_pipeline_dev:
  class: ParallelTextAndMaskInputPipeline
  params:
    decoder_mask_files: [data/datasets/data_final_processed/dev/dev_decoder_mask.txt]
    source_files: [data/datasets/data_final_processed/dev/dev_encode.txt]
    target_files: [data/datasets/data_final_processed/dev/dev_decode.txt]
input_pipeline_train:
  class: ParallelTextAndMaskInputPipeline
  params:
    decoder_mask_files: [data/datasets/data_final_processed/train/train_decoder_mask.txt]
    source_files: [data/datasets/data_final_processed/train/train_encode.txt]
    target_files: [data/datasets/data_final_processed/train/train_decode.txt]
keep_checkpoint_max: 0
model: AttentionSeq2Seq
model_params:
  attention.class: seq2seq.decoders.attention.AttentionLayerDot
  attention.params: {num_units: 150}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 150}
      dropout_input_keep_prob: 0.5
      dropout_output_keep_prob: 0.5
      num_layers: 1
  embedding.dim: 100
  embedding.file: data/glove/glove.6B.100d.txt
  embedding.tune: true
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 150}
      dropout_input_keep_prob: 0.5
      dropout_output_keep_prob: 0.5
      num_layers: 1
  optimizer.learning_rate: 0.0005
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  source.max_seq_len: 45
  source.reverse: false
  target.max_seq_len: 125
  vocab_source: data/datasets/data_final_processed/encode_vocab.txt
  vocab_target: data/datasets/data_final_processed/decode_vocab.txt
output_dir: AttentionSeq2Seq_tune_model_data
save_checkpoints_steps: 1000
train_steps: 40000

INFO:tensorflow:Creating ParallelTextAndMaskInputPipeline in mode=train
INFO:tensorflow:
ParallelTextAndMaskInputPipeline:
  !!python/unicode 'build_schema_map_table': false
  !!python/unicode 'build_schema_text_table': false
  !!python/unicode 'decoder_mask_files': [data/datasets/data_final_processed/train/train_decoder_mask.txt]
  !!python/unicode 'num_epochs': null
  !!python/unicode 'shuffle': true
  !!python/unicode 'source_delimiter': !!python/unicode ' '
  !!python/unicode 'source_files': [data/datasets/data_final_processed/train/train_encode.txt]
  !!python/unicode 'target_delimiter': !!python/unicode ' '
  !!python/unicode 'target_files': [data/datasets/data_final_processed/train/train_decode.txt]

INFO:tensorflow:Creating ParallelTextAndMaskInputPipeline in mode=eval
INFO:tensorflow:
ParallelTextAndMaskInputPipeline:
  !!python/unicode 'build_schema_map_table': false
  !!python/unicode 'build_schema_text_table': false
  !!python/unicode 'decoder_mask_files': [data/datasets/data_final_processed/dev/dev_decoder_mask.txt]
  !!python/unicode 'num_epochs': 1
  !!python/unicode 'shuffle': false
  !!python/unicode 'source_delimiter': !!python/unicode ' '
  !!python/unicode 'source_files': [data/datasets/data_final_processed/dev/dev_encode.txt]
  !!python/unicode 'target_delimiter': !!python/unicode ' '
  !!python/unicode 'target_files': [data/datasets/data_final_processed/dev/dev_decode.txt]

INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 0, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4b0abf7050>, '_model_dir': 'AttentionSeq2Seq_tune_model_data', '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 4, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_evaluation_master': '', '_master': ''}
INFO:tensorflow:Training model for 1000 steps
2018-08-22 02:23:20.622523: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-08-22 02:23:20.863637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:05:00.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-08-22 02:23:20.863713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

WARNING:tensorflow:From seq2seq/models/model_base.py:108: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_global_step
WARNING:tensorflow:From /home/lily/dw633/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 02:24:44.492066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Saving checkpoints for 1 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-1 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 8.13626, step = 1
INFO:tensorflow:global_step/sec: 3.09195
INFO:tensorflow:loss = 4.45654, step = 101 (32.343 sec)
INFO:tensorflow:global_step/sec: 3.08246
INFO:tensorflow:loss = 4.38374, step = 201 (32.442 sec)
INFO:tensorflow:global_step/sec: 2.903
INFO:tensorflow:loss = 4.3485, step = 301 (34.447 sec)
INFO:tensorflow:global_step/sec: 3.13414
INFO:tensorflow:loss = 4.48099, step = 401 (31.909 sec)
INFO:tensorflow:global_step/sec: 2.99684
INFO:tensorflow:loss = 4.53333, step = 501 (33.366 sec)
INFO:tensorflow:global_step/sec: 3.02737
INFO:tensorflow:loss = 4.23234, step = 601 (33.031 sec)
INFO:tensorflow:global_step/sec: 3.15884
INFO:tensorflow:loss = 3.80273, step = 701 (31.657 sec)
INFO:tensorflow:global_step/sec: 3.08756
INFO:tensorflow:loss = 3.38912, step = 801 (32.390 sec)
INFO:tensorflow:global_step/sec: 3.26133
INFO:tensorflow:loss = 3.09476, step = 901 (30.662 sec)
INFO:tensorflow:Saving checkpoints for 1000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-1000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 2.91167.
INFO:tensorflow:Evaluating model now.
2018-08-22 02:30:18.087553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-06:30:19
2018-08-22 02:30:19.580570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-1000
INFO:tensorflow:Finished evaluation at 2018-08-22-06:30:34
INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 3.28979
INFO:tensorflow:Training model for 1000 steps
2018-08-22 02:30:35.134174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 02:30:38.121059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-1000
INFO:tensorflow:Saving checkpoints for 1001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-1001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 2.83789, step = 1001
INFO:tensorflow:global_step/sec: 2.98327
INFO:tensorflow:loss = 2.82251, step = 1101 (33.523 sec)
INFO:tensorflow:global_step/sec: 2.96392
INFO:tensorflow:loss = 2.86232, step = 1201 (33.737 sec)
INFO:tensorflow:global_step/sec: 3.07104
INFO:tensorflow:loss = 2.27531, step = 1301 (32.562 sec)
INFO:tensorflow:global_step/sec: 3.1364
INFO:tensorflow:loss = 2.50483, step = 1401 (31.884 sec)
INFO:tensorflow:global_step/sec: 2.97929
INFO:tensorflow:loss = 2.02912, step = 1501 (33.566 sec)
INFO:tensorflow:global_step/sec: 2.99394
INFO:tensorflow:loss = 2.25518, step = 1601 (33.399 sec)
INFO:tensorflow:global_step/sec: 2.94839
INFO:tensorflow:loss = 2.07908, step = 1701 (33.917 sec)
INFO:tensorflow:global_step/sec: 3.10495
INFO:tensorflow:loss = 2.19917, step = 1801 (32.206 sec)
INFO:tensorflow:global_step/sec: 3.42407
INFO:tensorflow:loss = 1.8035, step = 1901 (29.207 sec)
INFO:tensorflow:Saving checkpoints for 2000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-2000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 1.98819.
INFO:tensorflow:Evaluating model now.
2018-08-22 02:36:12.650244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-06:36:14
2018-08-22 02:36:14.118136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-2000
INFO:tensorflow:Finished evaluation at 2018-08-22-06:36:28
INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, loss = 2.68376
INFO:tensorflow:Training model for 1000 steps
2018-08-22 02:36:29.062093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 02:36:31.802983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-2000
INFO:tensorflow:Saving checkpoints for 2001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-2001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 1.75113, step = 2001
INFO:tensorflow:global_step/sec: 3.144
INFO:tensorflow:loss = 1.68934, step = 2101 (31.808 sec)
INFO:tensorflow:global_step/sec: 3.09083
INFO:tensorflow:loss = 1.76442, step = 2201 (32.356 sec)
INFO:tensorflow:global_step/sec: 3.04789
INFO:tensorflow:loss = 1.95457, step = 2301 (32.807 sec)
INFO:tensorflow:global_step/sec: 3.00336
INFO:tensorflow:loss = 1.65479, step = 2401 (33.296 sec)
INFO:tensorflow:global_step/sec: 3.06396
INFO:tensorflow:loss = 1.53644, step = 2501 (32.638 sec)
INFO:tensorflow:global_step/sec: 3.13716
INFO:tensorflow:loss = 1.68182, step = 2601 (31.876 sec)
INFO:tensorflow:global_step/sec: 3.02091
INFO:tensorflow:loss = 1.33509, step = 2701 (33.103 sec)
INFO:tensorflow:global_step/sec: 3.06515
INFO:tensorflow:loss = 1.90179, step = 2801 (32.624 sec)
INFO:tensorflow:global_step/sec: 3.22413
INFO:tensorflow:loss = 1.62417, step = 2901 (31.017 sec)
INFO:tensorflow:Saving checkpoints for 3000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-3000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 1.4449.
INFO:tensorflow:Evaluating model now.
2018-08-22 02:42:04.971547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-06:42:06
2018-08-22 02:42:06.561763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-3000
INFO:tensorflow:Finished evaluation at 2018-08-22-06:42:21
INFO:tensorflow:Saving dict for global step 3000: global_step = 3000, loss = 2.61573
INFO:tensorflow:Training model for 1000 steps
2018-08-22 02:42:21.401680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 02:42:24.450333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-3000
INFO:tensorflow:Saving checkpoints for 3001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-3001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 1.86607, step = 3001
INFO:tensorflow:global_step/sec: 3.12456
INFO:tensorflow:loss = 1.47637, step = 3101 (32.007 sec)
INFO:tensorflow:global_step/sec: 3.07868
INFO:tensorflow:loss = 1.48688, step = 3201 (32.480 sec)
INFO:tensorflow:global_step/sec: 3.08072
INFO:tensorflow:loss = 1.26355, step = 3301 (32.463 sec)
INFO:tensorflow:global_step/sec: 2.96642
INFO:tensorflow:loss = 1.64879, step = 3401 (33.709 sec)
INFO:tensorflow:global_step/sec: 3.0954
INFO:tensorflow:loss = 1.4803, step = 3501 (32.305 sec)
INFO:tensorflow:global_step/sec: 3.05813
INFO:tensorflow:loss = 1.46675, step = 3601 (32.701 sec)
INFO:tensorflow:global_step/sec: 3.01881
INFO:tensorflow:loss = 1.40435, step = 3701 (33.126 sec)
INFO:tensorflow:global_step/sec: 3.13696
INFO:tensorflow:loss = 1.4336, step = 3801 (31.877 sec)
INFO:tensorflow:global_step/sec: 3.21595
INFO:tensorflow:loss = 1.30773, step = 3901 (31.095 sec)
INFO:tensorflow:Saving checkpoints for 4000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-4000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 1.41738.
INFO:tensorflow:Evaluating model now.
2018-08-22 02:47:58.266625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-06:47:59
2018-08-22 02:47:59.887605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-4000
INFO:tensorflow:Finished evaluation at 2018-08-22-06:48:14
INFO:tensorflow:Saving dict for global step 4000: global_step = 4000, loss = 2.59856
INFO:tensorflow:Training model for 1000 steps
2018-08-22 02:48:14.854653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 02:48:17.963535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-4000
INFO:tensorflow:Saving checkpoints for 4001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-4001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 1.5225, step = 4001
INFO:tensorflow:global_step/sec: 3.01978
INFO:tensorflow:loss = 1.36936, step = 4101 (33.117 sec)
INFO:tensorflow:global_step/sec: 3.10805
INFO:tensorflow:loss = 1.43292, step = 4201 (32.174 sec)
INFO:tensorflow:global_step/sec: 3.01045
INFO:tensorflow:loss = 1.56012, step = 4301 (33.217 sec)
INFO:tensorflow:global_step/sec: 3.11227
INFO:tensorflow:loss = 1.40687, step = 4401 (32.133 sec)
INFO:tensorflow:global_step/sec: 3.01151
INFO:tensorflow:loss = 1.27953, step = 4501 (33.203 sec)
INFO:tensorflow:global_step/sec: 3.04472
INFO:tensorflow:loss = 1.39801, step = 4601 (32.844 sec)
INFO:tensorflow:global_step/sec: 2.99792
INFO:tensorflow:loss = 1.2717, step = 4701 (33.357 sec)
INFO:tensorflow:global_step/sec: 3.06551
INFO:tensorflow:loss = 1.29657, step = 4801 (32.622 sec)
INFO:tensorflow:global_step/sec: 3.32021
INFO:tensorflow:loss = 1.17279, step = 4901 (30.118 sec)
INFO:tensorflow:Saving checkpoints for 5000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 1.32765.
INFO:tensorflow:Evaluating model now.
2018-08-22 02:53:52.925438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-06:53:54
2018-08-22 02:53:54.342233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-5000
INFO:tensorflow:Finished evaluation at 2018-08-22-06:54:09
INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 2.67455
INFO:tensorflow:Training model for 1000 steps
2018-08-22 02:54:09.250039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 02:54:12.217574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-5000
INFO:tensorflow:Saving checkpoints for 5001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-5001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 1.40066, step = 5001
INFO:tensorflow:global_step/sec: 3.03891
INFO:tensorflow:loss = 1.10787, step = 5101 (32.910 sec)
INFO:tensorflow:global_step/sec: 2.99707
INFO:tensorflow:loss = 1.18966, step = 5201 (33.363 sec)
INFO:tensorflow:global_step/sec: 2.97497
INFO:tensorflow:loss = 1.34691, step = 5301 (33.615 sec)
INFO:tensorflow:global_step/sec: 3.17032
INFO:tensorflow:loss = 1.17459, step = 5401 (31.542 sec)
INFO:tensorflow:global_step/sec: 3.10637
INFO:tensorflow:loss = 1.098, step = 5501 (32.193 sec)
INFO:tensorflow:global_step/sec: 2.88374
INFO:tensorflow:loss = 1.11872, step = 5601 (34.676 sec)
INFO:tensorflow:global_step/sec: 3.04479
INFO:tensorflow:loss = 1.24078, step = 5701 (32.843 sec)
INFO:tensorflow:global_step/sec: 3.04217
INFO:tensorflow:loss = 1.12812, step = 5801 (32.871 sec)
INFO:tensorflow:global_step/sec: 3.22505
INFO:tensorflow:loss = 1.16667, step = 5901 (31.010 sec)
INFO:tensorflow:Saving checkpoints for 6000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-6000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.989989.
INFO:tensorflow:Evaluating model now.
2018-08-22 02:59:49.701594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-06:59:51
2018-08-22 02:59:51.376057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-6000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:00:06
INFO:tensorflow:Saving dict for global step 6000: global_step = 6000, loss = 2.75451
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:00:06.231121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:00:09.163570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-6000
INFO:tensorflow:Saving checkpoints for 6001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-6001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.937372, step = 6001
INFO:tensorflow:global_step/sec: 2.98029
INFO:tensorflow:loss = 1.13429, step = 6101 (33.554 sec)
INFO:tensorflow:global_step/sec: 2.93273
INFO:tensorflow:loss = 1.06631, step = 6201 (34.101 sec)
INFO:tensorflow:global_step/sec: 3.02222
INFO:tensorflow:loss = 0.966711, step = 6301 (33.085 sec)
INFO:tensorflow:global_step/sec: 3.22168
INFO:tensorflow:loss = 1.13264, step = 6401 (31.041 sec)
INFO:tensorflow:global_step/sec: 3.0464
INFO:tensorflow:loss = 1.0713, step = 6501 (32.824 sec)
INFO:tensorflow:global_step/sec: 3.10013
INFO:tensorflow:loss = 1.03919, step = 6601 (32.259 sec)
INFO:tensorflow:global_step/sec: 3.10965
INFO:tensorflow:loss = 0.969145, step = 6701 (32.156 sec)
INFO:tensorflow:global_step/sec: 3.10899
INFO:tensorflow:loss = 1.04596, step = 6801 (32.164 sec)
INFO:tensorflow:global_step/sec: 3.336
INFO:tensorflow:loss = 1.11698, step = 6901 (29.977 sec)
INFO:tensorflow:Saving checkpoints for 7000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-7000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.878671.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:05:42.755098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:05:44
2018-08-22 03:05:44.400231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-7000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:05:59
INFO:tensorflow:Saving dict for global step 7000: global_step = 7000, loss = 2.78649
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:05:59.240912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:06:02.172254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-7000
INFO:tensorflow:Saving checkpoints for 7001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-7001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 1.23585, step = 7001
INFO:tensorflow:global_step/sec: 2.99963
INFO:tensorflow:loss = 0.936939, step = 7101 (33.338 sec)
INFO:tensorflow:global_step/sec: 3.05533
INFO:tensorflow:loss = 1.0226, step = 7201 (32.731 sec)
INFO:tensorflow:global_step/sec: 3.02291
INFO:tensorflow:loss = 1.0383, step = 7301 (33.082 sec)
INFO:tensorflow:global_step/sec: 3.08595
INFO:tensorflow:loss = 0.767988, step = 7401 (32.402 sec)
INFO:tensorflow:global_step/sec: 3.0273
INFO:tensorflow:loss = 0.816746, step = 7501 (33.033 sec)
INFO:tensorflow:global_step/sec: 3.05443
INFO:tensorflow:loss = 0.914186, step = 7601 (32.739 sec)
INFO:tensorflow:global_step/sec: 3.09726
INFO:tensorflow:loss = 0.886577, step = 7701 (32.287 sec)
INFO:tensorflow:global_step/sec: 2.97585
INFO:tensorflow:loss = 0.897826, step = 7801 (33.603 sec)
INFO:tensorflow:global_step/sec: 3.29532
INFO:tensorflow:loss = 0.695693, step = 7901 (30.348 sec)
INFO:tensorflow:Saving checkpoints for 8000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-8000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.930367.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:11:37.050509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:11:38
2018-08-22 03:11:38.733715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-8000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:11:53
INFO:tensorflow:Saving dict for global step 8000: global_step = 8000, loss = 2.82341
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:11:53.819482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:11:56.801817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-8000
INFO:tensorflow:Saving checkpoints for 8001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-8001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.836392, step = 8001
INFO:tensorflow:global_step/sec: 3.0371
INFO:tensorflow:loss = 0.75023, step = 8101 (32.926 sec)
INFO:tensorflow:global_step/sec: 3.08436
INFO:tensorflow:loss = 0.859553, step = 8201 (32.422 sec)
INFO:tensorflow:global_step/sec: 3.07119
INFO:tensorflow:loss = 0.769069, step = 8301 (32.562 sec)
INFO:tensorflow:global_step/sec: 3.08381
INFO:tensorflow:loss = 0.779803, step = 8401 (32.430 sec)
INFO:tensorflow:global_step/sec: 3.01442
INFO:tensorflow:loss = 0.928725, step = 8501 (33.171 sec)
INFO:tensorflow:global_step/sec: 2.93346
INFO:tensorflow:loss = 0.908715, step = 8601 (34.090 sec)
INFO:tensorflow:global_step/sec: 3.12642
INFO:tensorflow:loss = 0.784453, step = 8701 (31.985 sec)
INFO:tensorflow:global_step/sec: 3.13946
INFO:tensorflow:loss = 0.664742, step = 8801 (31.854 sec)
INFO:tensorflow:global_step/sec: 3.1753
INFO:tensorflow:loss = 0.751191, step = 8901 (31.493 sec)
INFO:tensorflow:Saving checkpoints for 9000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-9000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.755081.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:17:32.468390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:17:33
2018-08-22 03:17:34.151590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-9000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:17:48
INFO:tensorflow:Saving dict for global step 9000: global_step = 9000, loss = 2.82831
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:17:48.836454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:17:51.748645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-9000
INFO:tensorflow:Saving checkpoints for 9001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-9001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.715707, step = 9001
INFO:tensorflow:global_step/sec: 3.07297
INFO:tensorflow:loss = 0.738021, step = 9101 (32.545 sec)
INFO:tensorflow:global_step/sec: 3.12478
INFO:tensorflow:loss = 0.697686, step = 9201 (32.000 sec)
INFO:tensorflow:global_step/sec: 3.04908
INFO:tensorflow:loss = 0.765929, step = 9301 (32.797 sec)
INFO:tensorflow:global_step/sec: 3.18972
INFO:tensorflow:loss = 0.568738, step = 9401 (31.351 sec)
INFO:tensorflow:global_step/sec: 2.95847
INFO:tensorflow:loss = 0.654548, step = 9501 (33.803 sec)
INFO:tensorflow:global_step/sec: 3.01859
INFO:tensorflow:loss = 0.632271, step = 9601 (33.126 sec)
INFO:tensorflow:global_step/sec: 3.04029
INFO:tensorflow:loss = 0.721572, step = 9701 (32.892 sec)
INFO:tensorflow:global_step/sec: 3.10954
INFO:tensorflow:loss = 0.695548, step = 9801 (32.159 sec)
INFO:tensorflow:global_step/sec: 3.2066
INFO:tensorflow:loss = 0.563878, step = 9901 (31.187 sec)
INFO:tensorflow:Saving checkpoints for 10000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.62138.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:23:25.614168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:23:26
2018-08-22 03:23:27.160812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-10000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:23:41
INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, loss = 2.85568
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:23:42.026125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:23:44.998031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-10000
INFO:tensorflow:Saving checkpoints for 10001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-10001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.636213, step = 10001
INFO:tensorflow:global_step/sec: 3.06523
INFO:tensorflow:loss = 0.597762, step = 10101 (32.625 sec)
INFO:tensorflow:global_step/sec: 2.9241
INFO:tensorflow:loss = 0.755039, step = 10201 (34.201 sec)
INFO:tensorflow:global_step/sec: 3.13267
INFO:tensorflow:loss = 0.723045, step = 10301 (31.919 sec)
INFO:tensorflow:global_step/sec: 3.09694
INFO:tensorflow:loss = 0.59811, step = 10401 (32.290 sec)
INFO:tensorflow:global_step/sec: 2.96232
INFO:tensorflow:loss = 0.590732, step = 10501 (33.758 sec)
INFO:tensorflow:global_step/sec: 3.0106
INFO:tensorflow:loss = 0.798042, step = 10601 (33.218 sec)
INFO:tensorflow:global_step/sec: 2.93477
INFO:tensorflow:loss = 0.593241, step = 10701 (34.072 sec)
INFO:tensorflow:global_step/sec: 2.96785
INFO:tensorflow:loss = 0.612291, step = 10801 (33.694 sec)
INFO:tensorflow:global_step/sec: 3.31127
INFO:tensorflow:loss = 0.670953, step = 10901 (30.200 sec)
INFO:tensorflow:Saving checkpoints for 11000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-11000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.635429.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:29:22.207696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:29:23
2018-08-22 03:29:23.835653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-11000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:29:38
INFO:tensorflow:Saving dict for global step 11000: global_step = 11000, loss = 2.90064
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:29:38.670154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:29:41.673944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-11000
INFO:tensorflow:Saving checkpoints for 11001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-11001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.363048, step = 11001
INFO:tensorflow:global_step/sec: 3.08382
INFO:tensorflow:loss = 0.540657, step = 11101 (32.428 sec)
INFO:tensorflow:global_step/sec: 2.94536
INFO:tensorflow:loss = 0.619032, step = 11201 (33.951 sec)
INFO:tensorflow:global_step/sec: 3.00523
INFO:tensorflow:loss = 0.66694, step = 11301 (33.278 sec)
INFO:tensorflow:global_step/sec: 3.01973
INFO:tensorflow:loss = 0.612134, step = 11401 (33.114 sec)
INFO:tensorflow:global_step/sec: 3.14872
INFO:tensorflow:loss = 0.505722, step = 11501 (31.759 sec)
INFO:tensorflow:global_step/sec: 3.06046
INFO:tensorflow:loss = 0.541786, step = 11601 (32.674 sec)
INFO:tensorflow:global_step/sec: 3.04092
INFO:tensorflow:loss = 0.633924, step = 11701 (32.887 sec)
INFO:tensorflow:global_step/sec: 2.96021
INFO:tensorflow:loss = 0.569921, step = 11801 (33.781 sec)
INFO:tensorflow:global_step/sec: 3.18398
INFO:tensorflow:loss = 0.491682, step = 11901 (31.407 sec)
INFO:tensorflow:Saving checkpoints for 12000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-12000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.558902.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:35:17.901517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:35:19
2018-08-22 03:35:19.500067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-12000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:35:34
INFO:tensorflow:Saving dict for global step 12000: global_step = 12000, loss = 2.9237
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:35:34.233350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:35:37.114490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-12000
INFO:tensorflow:Saving checkpoints for 12001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-12001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.498427, step = 12001
INFO:tensorflow:global_step/sec: 2.97972
INFO:tensorflow:loss = 0.432993, step = 12101 (33.561 sec)
INFO:tensorflow:global_step/sec: 3.05891
INFO:tensorflow:loss = 0.573861, step = 12201 (32.691 sec)
INFO:tensorflow:global_step/sec: 3.05355
INFO:tensorflow:loss = 0.548595, step = 12301 (32.749 sec)
INFO:tensorflow:global_step/sec: 3.20969
INFO:tensorflow:loss = 0.589735, step = 12401 (31.159 sec)
INFO:tensorflow:global_step/sec: 3.13014
INFO:tensorflow:loss = 0.584289, step = 12501 (31.944 sec)
INFO:tensorflow:global_step/sec: 2.85947
INFO:tensorflow:loss = 0.41362, step = 12601 (34.971 sec)
INFO:tensorflow:global_step/sec: 3.03636
INFO:tensorflow:loss = 0.447064, step = 12701 (32.933 sec)
INFO:tensorflow:global_step/sec: 2.96286
INFO:tensorflow:loss = 0.576698, step = 12801 (33.752 sec)
INFO:tensorflow:global_step/sec: 3.21763
INFO:tensorflow:loss = 0.532729, step = 12901 (31.078 sec)
INFO:tensorflow:Saving checkpoints for 13000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-13000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.439537.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:41:12.324744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:41:13
2018-08-22 03:41:13.998104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-13000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:41:28
INFO:tensorflow:Saving dict for global step 13000: global_step = 13000, loss = 2.95078
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:41:28.948929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:41:31.971532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-13000
INFO:tensorflow:Saving checkpoints for 13001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-13001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.422123, step = 13001
INFO:tensorflow:global_step/sec: 3.01738
INFO:tensorflow:loss = 0.377562, step = 13101 (33.146 sec)
INFO:tensorflow:global_step/sec: 3.06092
INFO:tensorflow:loss = 0.388193, step = 13201 (32.666 sec)
INFO:tensorflow:global_step/sec: 3.03959
INFO:tensorflow:loss = 0.465814, step = 13301 (32.901 sec)
INFO:tensorflow:global_step/sec: 3.04164
INFO:tensorflow:loss = 0.460225, step = 13401 (32.876 sec)
INFO:tensorflow:global_step/sec: 3.0468
INFO:tensorflow:loss = 0.437652, step = 13501 (32.823 sec)
INFO:tensorflow:global_step/sec: 3.05231
INFO:tensorflow:loss = 0.52263, step = 13601 (32.760 sec)
INFO:tensorflow:global_step/sec: 2.92152
INFO:tensorflow:loss = 0.461888, step = 13701 (34.229 sec)
INFO:tensorflow:global_step/sec: 3.05143
INFO:tensorflow:loss = 0.463937, step = 13801 (32.771 sec)
INFO:tensorflow:global_step/sec: 3.21218
INFO:tensorflow:loss = 0.455035, step = 13901 (31.134 sec)
INFO:tensorflow:Saving checkpoints for 14000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-14000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.474703.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:47:09.703761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:47:11
2018-08-22 03:47:11.467673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-14000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:47:26
INFO:tensorflow:Saving dict for global step 14000: global_step = 14000, loss = 2.98837
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:47:26.406914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:47:29.397127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-14000
INFO:tensorflow:Saving checkpoints for 14001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-14001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.393648, step = 14001
INFO:tensorflow:global_step/sec: 3.14205
INFO:tensorflow:loss = 0.532582, step = 14101 (31.827 sec)
INFO:tensorflow:global_step/sec: 2.94718
INFO:tensorflow:loss = 0.456191, step = 14201 (33.932 sec)
INFO:tensorflow:global_step/sec: 3.13993
INFO:tensorflow:loss = 0.42024, step = 14301 (31.846 sec)
INFO:tensorflow:global_step/sec: 3.11994
INFO:tensorflow:loss = 0.413863, step = 14401 (32.052 sec)
INFO:tensorflow:global_step/sec: 3.02787
INFO:tensorflow:loss = 0.401442, step = 14501 (33.027 sec)
INFO:tensorflow:global_step/sec: 2.98445
INFO:tensorflow:loss = 0.455234, step = 14601 (33.508 sec)
INFO:tensorflow:global_step/sec: 2.94905
INFO:tensorflow:loss = 0.359362, step = 14701 (33.908 sec)
INFO:tensorflow:global_step/sec: 3.04491
INFO:tensorflow:loss = 0.269122, step = 14801 (32.842 sec)
INFO:tensorflow:global_step/sec: 3.28593
INFO:tensorflow:loss = 0.382912, step = 14901 (30.433 sec)
INFO:tensorflow:Saving checkpoints for 15000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.389447.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:53:05.301710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:53:06
2018-08-22 03:53:06.948266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-15000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:53:21
INFO:tensorflow:Saving dict for global step 15000: global_step = 15000, loss = 3.00677
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:53:21.590826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:53:24.586672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-15000
INFO:tensorflow:Saving checkpoints for 15001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-15001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.317469, step = 15001
INFO:tensorflow:global_step/sec: 3.17319
INFO:tensorflow:loss = 0.337216, step = 15101 (31.515 sec)
INFO:tensorflow:global_step/sec: 2.95758
INFO:tensorflow:loss = 0.466806, step = 15201 (33.811 sec)
INFO:tensorflow:global_step/sec: 3.15169
INFO:tensorflow:loss = 0.424257, step = 15301 (31.732 sec)
INFO:tensorflow:global_step/sec: 3.0879
INFO:tensorflow:loss = 0.365532, step = 15401 (32.381 sec)
INFO:tensorflow:global_step/sec: 3.038
INFO:tensorflow:loss = 0.393925, step = 15501 (32.917 sec)
INFO:tensorflow:global_step/sec: 3.10047
INFO:tensorflow:loss = 0.394897, step = 15601 (32.253 sec)
INFO:tensorflow:global_step/sec: 3.14227
INFO:tensorflow:loss = 0.339143, step = 15701 (31.825 sec)
INFO:tensorflow:global_step/sec: 2.92746
INFO:tensorflow:loss = 0.322322, step = 15801 (34.159 sec)
INFO:tensorflow:global_step/sec: 3.19195
INFO:tensorflow:loss = 0.369048, step = 15901 (31.329 sec)
INFO:tensorflow:Saving checkpoints for 16000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-16000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.512098.
INFO:tensorflow:Evaluating model now.
2018-08-22 03:58:57.026777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-07:58:58
2018-08-22 03:58:58.657243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-16000
INFO:tensorflow:Finished evaluation at 2018-08-22-07:59:13
INFO:tensorflow:Saving dict for global step 16000: global_step = 16000, loss = 3.04544
INFO:tensorflow:Training model for 1000 steps
2018-08-22 03:59:13.491183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 03:59:16.490476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-16000
INFO:tensorflow:Saving checkpoints for 16001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-16001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.384173, step = 16001
INFO:tensorflow:global_step/sec: 3.05736
INFO:tensorflow:loss = 0.383967, step = 16101 (32.709 sec)
INFO:tensorflow:global_step/sec: 3.07023
INFO:tensorflow:loss = 0.381353, step = 16201 (32.571 sec)
INFO:tensorflow:global_step/sec: 2.97695
INFO:tensorflow:loss = 0.400277, step = 16301 (33.591 sec)
INFO:tensorflow:global_step/sec: 3.07887
INFO:tensorflow:loss = 0.334737, step = 16401 (32.482 sec)
INFO:tensorflow:global_step/sec: 3.08514
INFO:tensorflow:loss = 0.317376, step = 16501 (32.411 sec)
INFO:tensorflow:global_step/sec: 3.04833
INFO:tensorflow:loss = 0.400975, step = 16601 (32.806 sec)
INFO:tensorflow:global_step/sec: 2.94732
INFO:tensorflow:loss = 0.382584, step = 16701 (33.928 sec)
INFO:tensorflow:global_step/sec: 3.10828
INFO:tensorflow:loss = 0.291903, step = 16801 (32.173 sec)
INFO:tensorflow:global_step/sec: 3.25427
INFO:tensorflow:loss = 0.314293, step = 16901 (30.727 sec)
INFO:tensorflow:Saving checkpoints for 17000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-17000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.350166.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:04:51.096947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:04:52
2018-08-22 04:04:52.704854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-17000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:05:07
INFO:tensorflow:Saving dict for global step 17000: global_step = 17000, loss = 3.05076
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:05:07.487168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:05:10.472613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-17000
INFO:tensorflow:Saving checkpoints for 17001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-17001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.354247, step = 17001
INFO:tensorflow:global_step/sec: 3.13333
INFO:tensorflow:loss = 0.291367, step = 17101 (31.919 sec)
INFO:tensorflow:global_step/sec: 3.00928
INFO:tensorflow:loss = 0.284274, step = 17201 (33.228 sec)
INFO:tensorflow:global_step/sec: 3.01578
INFO:tensorflow:loss = 0.36808, step = 17301 (33.159 sec)
INFO:tensorflow:global_step/sec: 2.97805
INFO:tensorflow:loss = 0.281207, step = 17401 (33.578 sec)
INFO:tensorflow:global_step/sec: 3.06237
INFO:tensorflow:loss = 0.295178, step = 17501 (32.655 sec)
INFO:tensorflow:global_step/sec: 3.09074
INFO:tensorflow:loss = 0.23786, step = 17601 (32.354 sec)
INFO:tensorflow:global_step/sec: 3.07478
INFO:tensorflow:loss = 0.362708, step = 17701 (32.523 sec)
INFO:tensorflow:global_step/sec: 3.07306
INFO:tensorflow:loss = 0.319154, step = 17801 (32.541 sec)
INFO:tensorflow:global_step/sec: 3.22293
INFO:tensorflow:loss = 0.281126, step = 17901 (31.028 sec)
INFO:tensorflow:Saving checkpoints for 18000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-18000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.320531.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:10:44.782711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:10:46
2018-08-22 04:10:46.376916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-18000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:11:01
INFO:tensorflow:Saving dict for global step 18000: global_step = 18000, loss = 3.10306
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:11:01.241053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:11:04.243508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-18000
INFO:tensorflow:Saving checkpoints for 18001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-18001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.20456, step = 18001
INFO:tensorflow:global_step/sec: 3.09701
INFO:tensorflow:loss = 0.320768, step = 18101 (32.289 sec)
INFO:tensorflow:global_step/sec: 3.02691
INFO:tensorflow:loss = 0.277903, step = 18201 (33.039 sec)
INFO:tensorflow:global_step/sec: 3.08164
INFO:tensorflow:loss = 0.378358, step = 18301 (32.448 sec)
INFO:tensorflow:global_step/sec: 3.12238
INFO:tensorflow:loss = 0.319445, step = 18401 (32.027 sec)
INFO:tensorflow:global_step/sec: 2.93359
INFO:tensorflow:loss = 0.391112, step = 18501 (34.088 sec)
INFO:tensorflow:global_step/sec: 3.02157
INFO:tensorflow:loss = 0.315965, step = 18601 (33.097 sec)
INFO:tensorflow:global_step/sec: 2.99163
INFO:tensorflow:loss = 0.248129, step = 18701 (33.425 sec)
INFO:tensorflow:global_step/sec: 3.11833
INFO:tensorflow:loss = 0.376434, step = 18801 (32.069 sec)
INFO:tensorflow:global_step/sec: 3.3489
INFO:tensorflow:loss = 0.350249, step = 18901 (29.860 sec)
INFO:tensorflow:Saving checkpoints for 19000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-19000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.285899.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:16:38.021879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:16:39
2018-08-22 04:16:39.652889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-19000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:16:54
INFO:tensorflow:Saving dict for global step 19000: global_step = 19000, loss = 3.09523
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:16:54.456168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:16:57.492834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-19000
INFO:tensorflow:Saving checkpoints for 19001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-19001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.353679, step = 19001
INFO:tensorflow:global_step/sec: 3.03061
INFO:tensorflow:loss = 0.334362, step = 19101 (32.998 sec)
INFO:tensorflow:global_step/sec: 2.91391
INFO:tensorflow:loss = 0.439257, step = 19201 (34.318 sec)
INFO:tensorflow:global_step/sec: 3.12849
INFO:tensorflow:loss = 0.407161, step = 19301 (31.966 sec)
INFO:tensorflow:global_step/sec: 3.07454
INFO:tensorflow:loss = 0.255649, step = 19401 (32.524 sec)
INFO:tensorflow:global_step/sec: 2.96884
INFO:tensorflow:loss = 0.317057, step = 19501 (33.683 sec)
INFO:tensorflow:global_step/sec: 2.99918
INFO:tensorflow:loss = 0.297755, step = 19601 (33.342 sec)
INFO:tensorflow:global_step/sec: 2.94244
INFO:tensorflow:loss = 0.269015, step = 19701 (33.987 sec)
INFO:tensorflow:global_step/sec: 3.13542
INFO:tensorflow:loss = 0.272465, step = 19801 (31.892 sec)
INFO:tensorflow:global_step/sec: 3.28164
INFO:tensorflow:loss = 0.220979, step = 19901 (30.474 sec)
INFO:tensorflow:Saving checkpoints for 20000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.234134.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:22:34.188816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:22:35
2018-08-22 04:22:35.773305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-20000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:22:50
INFO:tensorflow:Saving dict for global step 20000: global_step = 20000, loss = 3.13917
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:22:50.388843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:22:53.378395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-20000
INFO:tensorflow:Saving checkpoints for 20001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-20001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.409725, step = 20001
INFO:tensorflow:global_step/sec: 3.08935
INFO:tensorflow:loss = 0.217579, step = 20101 (32.370 sec)
INFO:tensorflow:global_step/sec: 3.03672
INFO:tensorflow:loss = 0.222868, step = 20201 (32.931 sec)
INFO:tensorflow:global_step/sec: 3.08882
INFO:tensorflow:loss = 0.229817, step = 20301 (32.374 sec)
INFO:tensorflow:global_step/sec: 3.11654
INFO:tensorflow:loss = 0.267804, step = 20401 (32.090 sec)
INFO:tensorflow:global_step/sec: 3.03147
INFO:tensorflow:loss = 0.351715, step = 20501 (32.985 sec)
INFO:tensorflow:global_step/sec: 2.96921
INFO:tensorflow:loss = 0.247896, step = 20601 (33.678 sec)
INFO:tensorflow:global_step/sec: 2.98498
INFO:tensorflow:loss = 0.250657, step = 20701 (33.502 sec)
INFO:tensorflow:global_step/sec: 3.00826
INFO:tensorflow:loss = 0.263779, step = 20801 (33.243 sec)
INFO:tensorflow:global_step/sec: 3.19958
INFO:tensorflow:loss = 0.360229, step = 20901 (31.253 sec)
INFO:tensorflow:Saving checkpoints for 21000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-21000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.272924.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:28:29.802342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:28:31
2018-08-22 04:28:31.392341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-21000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:28:46
INFO:tensorflow:Saving dict for global step 21000: global_step = 21000, loss = 3.1272
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:28:46.151404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:28:49.200146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-21000
INFO:tensorflow:Saving checkpoints for 21001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-21001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.266069, step = 21001
INFO:tensorflow:global_step/sec: 3.07139
INFO:tensorflow:loss = 0.211955, step = 21101 (32.562 sec)
INFO:tensorflow:global_step/sec: 3.04504
INFO:tensorflow:loss = 0.279513, step = 21201 (32.838 sec)
INFO:tensorflow:global_step/sec: 3.05471
INFO:tensorflow:loss = 0.301831, step = 21301 (32.736 sec)
INFO:tensorflow:global_step/sec: 3.08356
INFO:tensorflow:loss = 0.252285, step = 21401 (32.430 sec)
INFO:tensorflow:global_step/sec: 3.03327
INFO:tensorflow:loss = 0.286395, step = 21501 (32.968 sec)
INFO:tensorflow:global_step/sec: 3.05332
INFO:tensorflow:loss = 0.339424, step = 21601 (32.750 sec)
INFO:tensorflow:global_step/sec: 2.87814
INFO:tensorflow:loss = 0.240106, step = 21701 (34.745 sec)
INFO:tensorflow:global_step/sec: 2.99261
INFO:tensorflow:loss = 0.257663, step = 21801 (33.416 sec)
INFO:tensorflow:global_step/sec: 3.30385
INFO:tensorflow:loss = 0.266773, step = 21901 (30.270 sec)
INFO:tensorflow:Saving checkpoints for 22000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-22000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.282128.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:34:25.219084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:34:26
2018-08-22 04:34:26.818364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-22000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:34:41
INFO:tensorflow:Saving dict for global step 22000: global_step = 22000, loss = 3.16936
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:34:41.735250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:34:44.685806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-22000
INFO:tensorflow:Saving checkpoints for 22001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-22001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.166038, step = 22001
INFO:tensorflow:global_step/sec: 3.06349
INFO:tensorflow:loss = 0.273104, step = 22101 (32.644 sec)
INFO:tensorflow:global_step/sec: 3.00988
INFO:tensorflow:loss = 0.322354, step = 22201 (33.226 sec)
INFO:tensorflow:global_step/sec: 3.11591
INFO:tensorflow:loss = 0.207776, step = 22301 (32.091 sec)
INFO:tensorflow:global_step/sec: 2.9926
INFO:tensorflow:loss = 0.208921, step = 22401 (33.416 sec)
INFO:tensorflow:global_step/sec: 3.00591
INFO:tensorflow:loss = 0.275893, step = 22501 (33.268 sec)
INFO:tensorflow:global_step/sec: 3.04906
INFO:tensorflow:loss = 0.336257, step = 22601 (32.799 sec)
INFO:tensorflow:global_step/sec: 3.0733
INFO:tensorflow:loss = 0.241188, step = 22701 (32.537 sec)
INFO:tensorflow:global_step/sec: 3.12972
INFO:tensorflow:loss = 0.272499, step = 22801 (31.952 sec)
INFO:tensorflow:global_step/sec: 3.22172
INFO:tensorflow:loss = 0.261417, step = 22901 (31.040 sec)
INFO:tensorflow:Saving checkpoints for 23000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-23000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.36513.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:40:19.956345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:40:21
2018-08-22 04:40:21.640158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-23000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:40:36
INFO:tensorflow:Saving dict for global step 23000: global_step = 23000, loss = 3.17858
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:40:36.503304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:40:39.447171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-23000
INFO:tensorflow:Saving checkpoints for 23001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-23001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.19159, step = 23001
INFO:tensorflow:global_step/sec: 2.99552
INFO:tensorflow:loss = 0.329287, step = 23101 (33.385 sec)
INFO:tensorflow:global_step/sec: 2.94323
INFO:tensorflow:loss = 0.237128, step = 23201 (33.977 sec)
INFO:tensorflow:global_step/sec: 3.08799
INFO:tensorflow:loss = 0.286005, step = 23301 (32.384 sec)
INFO:tensorflow:global_step/sec: 3.12433
INFO:tensorflow:loss = 0.256761, step = 23401 (32.007 sec)
INFO:tensorflow:global_step/sec: 3.06741
INFO:tensorflow:loss = 0.261192, step = 23501 (32.600 sec)
INFO:tensorflow:global_step/sec: 3.05751
INFO:tensorflow:loss = 0.258268, step = 23601 (32.706 sec)
INFO:tensorflow:global_step/sec: 2.89208
INFO:tensorflow:loss = 0.215533, step = 23701 (34.579 sec)
INFO:tensorflow:global_step/sec: 3.05958
INFO:tensorflow:loss = 0.247308, step = 23801 (32.682 sec)
INFO:tensorflow:global_step/sec: 3.28386
INFO:tensorflow:loss = 0.221603, step = 23901 (30.453 sec)
INFO:tensorflow:Saving checkpoints for 24000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-24000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.223691.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:46:16.851661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:46:18
2018-08-22 04:46:18.442086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-24000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:46:33
INFO:tensorflow:Saving dict for global step 24000: global_step = 24000, loss = 3.20369
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:46:33.578040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:46:36.502982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-24000
INFO:tensorflow:Saving checkpoints for 24001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-24001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.190917, step = 24001
INFO:tensorflow:global_step/sec: 3.13129
INFO:tensorflow:loss = 0.186776, step = 24101 (31.935 sec)
INFO:tensorflow:global_step/sec: 3.00258
INFO:tensorflow:loss = 0.220144, step = 24201 (33.306 sec)
INFO:tensorflow:global_step/sec: 3.09292
INFO:tensorflow:loss = 0.20781, step = 24301 (32.331 sec)
INFO:tensorflow:global_step/sec: 3.01613
INFO:tensorflow:loss = 0.244273, step = 24401 (33.158 sec)
INFO:tensorflow:global_step/sec: 3.04752
INFO:tensorflow:loss = 0.18307, step = 24501 (32.810 sec)
INFO:tensorflow:global_step/sec: 2.87487
INFO:tensorflow:loss = 0.195142, step = 24601 (34.785 sec)
INFO:tensorflow:global_step/sec: 3.00402
INFO:tensorflow:loss = 0.212301, step = 24701 (33.288 sec)
INFO:tensorflow:global_step/sec: 3.05813
INFO:tensorflow:loss = 0.233985, step = 24801 (32.703 sec)
INFO:tensorflow:global_step/sec: 3.31297
INFO:tensorflow:loss = 0.185442, step = 24901 (30.182 sec)
INFO:tensorflow:Saving checkpoints for 25000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.286163.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:52:12.649801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:52:14
2018-08-22 04:52:14.256333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-25000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:52:29
INFO:tensorflow:Saving dict for global step 25000: global_step = 25000, loss = 3.21883
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:52:29.229060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:52:32.182502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-25000
INFO:tensorflow:Saving checkpoints for 25001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-25001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.276861, step = 25001
INFO:tensorflow:global_step/sec: 3.01056
INFO:tensorflow:loss = 0.340053, step = 25101 (33.220 sec)
INFO:tensorflow:global_step/sec: 3.01047
INFO:tensorflow:loss = 0.202378, step = 25201 (33.214 sec)
INFO:tensorflow:global_step/sec: 3.08491
INFO:tensorflow:loss = 0.155879, step = 25301 (32.416 sec)
INFO:tensorflow:global_step/sec: 2.98803
INFO:tensorflow:loss = 0.180177, step = 25401 (33.469 sec)
INFO:tensorflow:global_step/sec: 3.06004
INFO:tensorflow:loss = 0.227873, step = 25501 (32.678 sec)
INFO:tensorflow:global_step/sec: 2.94813
INFO:tensorflow:loss = 0.235835, step = 25601 (33.919 sec)
INFO:tensorflow:global_step/sec: 2.92615
INFO:tensorflow:loss = 0.196232, step = 25701 (34.175 sec)
INFO:tensorflow:global_step/sec: 2.99496
INFO:tensorflow:loss = 0.208365, step = 25801 (33.389 sec)
INFO:tensorflow:global_step/sec: 3.27642
INFO:tensorflow:loss = 0.204487, step = 25901 (30.522 sec)
INFO:tensorflow:Saving checkpoints for 26000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-26000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.176297.
INFO:tensorflow:Evaluating model now.
2018-08-22 04:58:11.921142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-08:58:13
2018-08-22 04:58:13.571008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-26000
INFO:tensorflow:Finished evaluation at 2018-08-22-08:58:28
INFO:tensorflow:Saving dict for global step 26000: global_step = 26000, loss = 3.25512
INFO:tensorflow:Training model for 1000 steps
2018-08-22 04:58:28.551544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 04:58:31.623182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-26000
INFO:tensorflow:Saving checkpoints for 26001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-26001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.170386, step = 26001
INFO:tensorflow:global_step/sec: 2.97111
INFO:tensorflow:loss = 0.237022, step = 26101 (33.659 sec)
INFO:tensorflow:global_step/sec: 3.00648
INFO:tensorflow:loss = 0.153664, step = 26201 (33.263 sec)
INFO:tensorflow:global_step/sec: 3.14471
INFO:tensorflow:loss = 0.138478, step = 26301 (31.797 sec)
INFO:tensorflow:global_step/sec: 3.05038
INFO:tensorflow:loss = 0.178393, step = 26401 (32.784 sec)
INFO:tensorflow:global_step/sec: 3.04277
INFO:tensorflow:loss = 0.206933, step = 26501 (32.864 sec)
INFO:tensorflow:global_step/sec: 2.95676
INFO:tensorflow:loss = 0.229041, step = 26601 (33.822 sec)
INFO:tensorflow:global_step/sec: 3.02223
INFO:tensorflow:loss = 0.206284, step = 26701 (33.087 sec)
INFO:tensorflow:global_step/sec: 3.08414
INFO:tensorflow:loss = 0.170415, step = 26801 (32.424 sec)
INFO:tensorflow:global_step/sec: 3.29695
INFO:tensorflow:loss = 0.136788, step = 26901 (30.332 sec)
INFO:tensorflow:Saving checkpoints for 27000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-27000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.227266.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:04:07.856625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:04:09
2018-08-22 05:04:09.459362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-27000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:04:24
INFO:tensorflow:Saving dict for global step 27000: global_step = 27000, loss = 3.25434
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:04:24.628819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:04:27.566447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-27000
INFO:tensorflow:Saving checkpoints for 27001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-27001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.134762, step = 27001
INFO:tensorflow:global_step/sec: 2.97103
INFO:tensorflow:loss = 0.150152, step = 27101 (33.659 sec)
INFO:tensorflow:global_step/sec: 3.01842
INFO:tensorflow:loss = 0.168838, step = 27201 (33.131 sec)
INFO:tensorflow:global_step/sec: 3.01149
INFO:tensorflow:loss = 0.179984, step = 27301 (33.208 sec)
INFO:tensorflow:global_step/sec: 3.13309
INFO:tensorflow:loss = 0.252039, step = 27401 (31.915 sec)
INFO:tensorflow:global_step/sec: 2.89515
INFO:tensorflow:loss = 0.179322, step = 27501 (34.540 sec)
INFO:tensorflow:global_step/sec: 3.02465
INFO:tensorflow:loss = 0.17523, step = 27601 (33.061 sec)
INFO:tensorflow:global_step/sec: 3.02196
INFO:tensorflow:loss = 0.258131, step = 27701 (33.093 sec)
INFO:tensorflow:global_step/sec: 3.18738
INFO:tensorflow:loss = 0.146537, step = 27801 (31.373 sec)
INFO:tensorflow:global_step/sec: 3.19796
INFO:tensorflow:loss = 0.126766, step = 27901 (31.271 sec)
INFO:tensorflow:Saving checkpoints for 28000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-28000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.212766.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:10:04.384523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:10:05
2018-08-22 05:10:05.964469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-28000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:10:20
INFO:tensorflow:Saving dict for global step 28000: global_step = 28000, loss = 3.28686
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:10:21.050257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:10:24.054976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-28000
INFO:tensorflow:Saving checkpoints for 28001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-28001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.103222, step = 28001
INFO:tensorflow:global_step/sec: 3.06525
INFO:tensorflow:loss = 0.239972, step = 28101 (32.625 sec)
INFO:tensorflow:global_step/sec: 3.14034
INFO:tensorflow:loss = 0.16652, step = 28201 (31.842 sec)
INFO:tensorflow:global_step/sec: 2.97508
INFO:tensorflow:loss = 0.242846, step = 28301 (33.613 sec)
INFO:tensorflow:global_step/sec: 3.08648
INFO:tensorflow:loss = 0.204409, step = 28401 (32.402 sec)
INFO:tensorflow:global_step/sec: 3.12317
INFO:tensorflow:loss = 0.161934, step = 28501 (32.016 sec)
INFO:tensorflow:global_step/sec: 2.89584
INFO:tensorflow:loss = 0.24804, step = 28601 (34.532 sec)
INFO:tensorflow:global_step/sec: 2.92345
INFO:tensorflow:loss = 0.161212, step = 28701 (34.206 sec)
INFO:tensorflow:global_step/sec: 3.05017
INFO:tensorflow:loss = 0.197628, step = 28801 (32.786 sec)
INFO:tensorflow:global_step/sec: 3.38692
INFO:tensorflow:loss = 0.197353, step = 28901 (29.526 sec)
INFO:tensorflow:Saving checkpoints for 29000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-29000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.204977.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:15:58.869042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:16:00
2018-08-22 05:16:00.456317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-29000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:16:14
INFO:tensorflow:Saving dict for global step 29000: global_step = 29000, loss = 3.31202
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:16:15.001034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:16:17.976299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-29000
INFO:tensorflow:Saving checkpoints for 29001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-29001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.122512, step = 29001
INFO:tensorflow:global_step/sec: 3.01524
INFO:tensorflow:loss = 0.183529, step = 29101 (33.169 sec)
INFO:tensorflow:global_step/sec: 2.99554
INFO:tensorflow:loss = 0.174818, step = 29201 (33.380 sec)
INFO:tensorflow:global_step/sec: 3.12348
INFO:tensorflow:loss = 0.198599, step = 29301 (32.017 sec)
INFO:tensorflow:global_step/sec: 3.12245
INFO:tensorflow:loss = 0.213378, step = 29401 (32.025 sec)
INFO:tensorflow:global_step/sec: 2.96888
INFO:tensorflow:loss = 0.185889, step = 29501 (33.684 sec)
INFO:tensorflow:global_step/sec: 3.14589
INFO:tensorflow:loss = 0.130919, step = 29601 (31.787 sec)
INFO:tensorflow:global_step/sec: 2.94246
INFO:tensorflow:loss = 0.196738, step = 29701 (33.985 sec)
INFO:tensorflow:global_step/sec: 3.07963
INFO:tensorflow:loss = 0.196629, step = 29801 (32.470 sec)
INFO:tensorflow:global_step/sec: 3.20718
INFO:tensorflow:loss = 0.172031, step = 29901 (31.183 sec)
INFO:tensorflow:Saving checkpoints for 30000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.17222.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:21:53.046457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:21:54
2018-08-22 05:21:54.707186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-30000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:22:09
INFO:tensorflow:Saving dict for global step 30000: global_step = 30000, loss = 3.32679
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:22:09.450856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:22:12.468177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-30000
INFO:tensorflow:Saving checkpoints for 30001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-30001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.16629, step = 30001
INFO:tensorflow:global_step/sec: 3.02583
INFO:tensorflow:loss = 0.146725, step = 30101 (33.049 sec)
INFO:tensorflow:global_step/sec: 3.01047
INFO:tensorflow:loss = 0.187563, step = 30201 (33.220 sec)
INFO:tensorflow:global_step/sec: 3.10607
INFO:tensorflow:loss = 0.157817, step = 30301 (32.192 sec)
INFO:tensorflow:global_step/sec: 3.12256
INFO:tensorflow:loss = 0.265235, step = 30401 (32.024 sec)
INFO:tensorflow:global_step/sec: 2.96031
INFO:tensorflow:loss = 0.205124, step = 30501 (33.782 sec)
INFO:tensorflow:global_step/sec: 3.01413
INFO:tensorflow:loss = 0.176156, step = 30601 (33.179 sec)
INFO:tensorflow:global_step/sec: 3.11082
INFO:tensorflow:loss = 0.167056, step = 30701 (32.144 sec)
INFO:tensorflow:global_step/sec: 3.10689
INFO:tensorflow:loss = 0.186107, step = 30801 (32.187 sec)
INFO:tensorflow:global_step/sec: 3.18804
INFO:tensorflow:loss = 0.151622, step = 30901 (31.367 sec)
INFO:tensorflow:Saving checkpoints for 31000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-31000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.172838.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:27:47.626714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:27:49
2018-08-22 05:27:49.308026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-31000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:28:04
INFO:tensorflow:Saving dict for global step 31000: global_step = 31000, loss = 3.35289
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:28:04.151815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:28:07.066485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-31000
INFO:tensorflow:Saving checkpoints for 31001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-31001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.127416, step = 31001
INFO:tensorflow:global_step/sec: 3.00426
INFO:tensorflow:loss = 0.188062, step = 31101 (33.287 sec)
INFO:tensorflow:global_step/sec: 2.90843
INFO:tensorflow:loss = 0.12204, step = 31201 (34.384 sec)
INFO:tensorflow:global_step/sec: 3.15275
INFO:tensorflow:loss = 0.168631, step = 31301 (31.720 sec)
INFO:tensorflow:global_step/sec: 3.12407
INFO:tensorflow:loss = 0.207749, step = 31401 (32.007 sec)
INFO:tensorflow:global_step/sec: 2.93035
INFO:tensorflow:loss = 0.169838, step = 31501 (34.125 sec)
INFO:tensorflow:global_step/sec: 3.16789
INFO:tensorflow:loss = 0.162758, step = 31601 (31.567 sec)
INFO:tensorflow:global_step/sec: 3.04466
INFO:tensorflow:loss = 0.19743, step = 31701 (32.846 sec)
INFO:tensorflow:global_step/sec: 3.00831
INFO:tensorflow:loss = 0.143145, step = 31801 (33.240 sec)
INFO:tensorflow:global_step/sec: 3.23834
INFO:tensorflow:loss = 0.167876, step = 31901 (30.881 sec)
INFO:tensorflow:Saving checkpoints for 32000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-32000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.181022.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:33:40.981702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:33:42
2018-08-22 05:33:42.423755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-32000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:33:57
INFO:tensorflow:Saving dict for global step 32000: global_step = 32000, loss = 3.36836
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:33:57.664660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:34:00.477844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-32000
INFO:tensorflow:Saving checkpoints for 32001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-32001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.0994101, step = 32001
INFO:tensorflow:global_step/sec: 2.98279
INFO:tensorflow:loss = 0.179128, step = 32101 (33.526 sec)
INFO:tensorflow:global_step/sec: 3.01577
INFO:tensorflow:loss = 0.14882, step = 32201 (33.160 sec)
INFO:tensorflow:global_step/sec: 3.03818
INFO:tensorflow:loss = 0.232508, step = 32301 (32.915 sec)
INFO:tensorflow:global_step/sec: 3.09917
INFO:tensorflow:loss = 0.249274, step = 32401 (32.269 sec)
INFO:tensorflow:global_step/sec: 3.10256
INFO:tensorflow:loss = 0.175502, step = 32501 (32.229 sec)
INFO:tensorflow:global_step/sec: 2.99064
INFO:tensorflow:loss = 0.181703, step = 32601 (33.438 sec)
INFO:tensorflow:global_step/sec: 2.97538
INFO:tensorflow:loss = 0.0866238, step = 32701 (33.610 sec)
INFO:tensorflow:global_step/sec: 3.07184
INFO:tensorflow:loss = 0.179278, step = 32801 (32.554 sec)
INFO:tensorflow:global_step/sec: 3.20044
INFO:tensorflow:loss = 0.222732, step = 32901 (31.245 sec)
INFO:tensorflow:Saving checkpoints for 33000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-33000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.133032.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:39:36.500425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:39:37
2018-08-22 05:39:38.044358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-33000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:39:52
INFO:tensorflow:Saving dict for global step 33000: global_step = 33000, loss = 3.36821
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:39:53.116351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:39:56.140825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-33000
INFO:tensorflow:Saving checkpoints for 33001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-33001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.165672, step = 33001
INFO:tensorflow:global_step/sec: 3.02825
INFO:tensorflow:loss = 0.121578, step = 33101 (33.027 sec)
INFO:tensorflow:global_step/sec: 2.95658
INFO:tensorflow:loss = 0.167819, step = 33201 (33.820 sec)
INFO:tensorflow:global_step/sec: 3.07198
INFO:tensorflow:loss = 0.118262, step = 33301 (32.552 sec)
INFO:tensorflow:global_step/sec: 3.07359
INFO:tensorflow:loss = 0.13141, step = 33401 (32.536 sec)
INFO:tensorflow:global_step/sec: 3.09832
INFO:tensorflow:loss = 0.145347, step = 33501 (32.277 sec)
INFO:tensorflow:global_step/sec: 3.07072
INFO:tensorflow:loss = 0.178122, step = 33601 (32.565 sec)
INFO:tensorflow:global_step/sec: 3.06639
INFO:tensorflow:loss = 0.0826239, step = 33701 (32.611 sec)
INFO:tensorflow:global_step/sec: 2.98499
INFO:tensorflow:loss = 0.1155, step = 33801 (33.501 sec)
INFO:tensorflow:global_step/sec: 3.2807
INFO:tensorflow:loss = 0.112762, step = 33901 (30.484 sec)
INFO:tensorflow:Saving checkpoints for 34000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-34000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.21126.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:45:30.862417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:45:32
2018-08-22 05:45:32.454697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-34000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:45:46
INFO:tensorflow:Saving dict for global step 34000: global_step = 34000, loss = 3.38813
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:45:46.931090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:45:50.009191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-34000
INFO:tensorflow:Saving checkpoints for 34001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-34001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.0948417, step = 34001
INFO:tensorflow:global_step/sec: 3.06485
INFO:tensorflow:loss = 0.103046, step = 34101 (32.628 sec)
INFO:tensorflow:global_step/sec: 3.06762
INFO:tensorflow:loss = 0.120969, step = 34201 (32.602 sec)
INFO:tensorflow:global_step/sec: 3.10216
INFO:tensorflow:loss = 0.142096, step = 34301 (32.234 sec)
INFO:tensorflow:global_step/sec: 3.08524
INFO:tensorflow:loss = 0.158406, step = 34401 (32.411 sec)
INFO:tensorflow:global_step/sec: 3.06638
INFO:tensorflow:loss = 0.137008, step = 34501 (32.612 sec)
INFO:tensorflow:global_step/sec: 2.87259
INFO:tensorflow:loss = 0.151406, step = 34601 (34.813 sec)
INFO:tensorflow:global_step/sec: 3.02107
INFO:tensorflow:loss = 0.200763, step = 34701 (33.099 sec)
INFO:tensorflow:global_step/sec: 3.08286
INFO:tensorflow:loss = 0.206029, step = 34801 (32.438 sec)
INFO:tensorflow:global_step/sec: 3.17889
INFO:tensorflow:loss = 0.113503, step = 34901 (31.458 sec)
INFO:tensorflow:Saving checkpoints for 35000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.153346.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:51:25.850140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:51:27
2018-08-22 05:51:27.321936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-35000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:51:41
INFO:tensorflow:Saving dict for global step 35000: global_step = 35000, loss = 3.41273
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:51:41.995638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:51:44.888996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-35000
INFO:tensorflow:Saving checkpoints for 35001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-35001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.144835, step = 35001
INFO:tensorflow:global_step/sec: 2.97244
INFO:tensorflow:loss = 0.115167, step = 35101 (33.643 sec)
INFO:tensorflow:global_step/sec: 2.97626
INFO:tensorflow:loss = 0.181259, step = 35201 (33.600 sec)
INFO:tensorflow:global_step/sec: 3.10381
INFO:tensorflow:loss = 0.112887, step = 35301 (32.220 sec)
INFO:tensorflow:global_step/sec: 3.07689
INFO:tensorflow:loss = 0.0996669, step = 35401 (32.498 sec)
INFO:tensorflow:global_step/sec: 3.00183
INFO:tensorflow:loss = 0.189873, step = 35501 (33.314 sec)
INFO:tensorflow:global_step/sec: 3.09607
INFO:tensorflow:loss = 0.157854, step = 35601 (32.298 sec)
INFO:tensorflow:global_step/sec: 2.95457
INFO:tensorflow:loss = 0.103286, step = 35701 (33.846 sec)
INFO:tensorflow:global_step/sec: 3.07725
INFO:tensorflow:loss = 0.186851, step = 35801 (32.495 sec)
INFO:tensorflow:global_step/sec: 3.25642
INFO:tensorflow:loss = 0.10771, step = 35901 (30.710 sec)
INFO:tensorflow:Saving checkpoints for 36000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-36000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.129699.
INFO:tensorflow:Evaluating model now.
2018-08-22 05:57:21.287720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-09:57:22
2018-08-22 05:57:22.939615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-36000
INFO:tensorflow:Finished evaluation at 2018-08-22-09:57:37
INFO:tensorflow:Saving dict for global step 36000: global_step = 36000, loss = 3.44393
INFO:tensorflow:Training model for 1000 steps
2018-08-22 05:57:37.721156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 05:57:40.641190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-36000
INFO:tensorflow:Saving checkpoints for 36001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-36001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.199853, step = 36001
INFO:tensorflow:global_step/sec: 3.03163
INFO:tensorflow:loss = 0.176229, step = 36101 (32.987 sec)
INFO:tensorflow:global_step/sec: 3.01048
INFO:tensorflow:loss = 0.129559, step = 36201 (33.217 sec)
INFO:tensorflow:global_step/sec: 3.073
INFO:tensorflow:loss = 0.128637, step = 36301 (32.543 sec)
INFO:tensorflow:global_step/sec: 3.14865
INFO:tensorflow:loss = 0.145699, step = 36401 (31.761 sec)
INFO:tensorflow:global_step/sec: 2.98483
INFO:tensorflow:loss = 0.13766, step = 36501 (33.499 sec)
INFO:tensorflow:global_step/sec: 2.92625
INFO:tensorflow:loss = 0.100499, step = 36601 (34.173 sec)
INFO:tensorflow:global_step/sec: 2.93289
INFO:tensorflow:loss = 0.180652, step = 36701 (34.096 sec)
INFO:tensorflow:global_step/sec: 3.09853
INFO:tensorflow:loss = 0.109887, step = 36801 (32.276 sec)
INFO:tensorflow:global_step/sec: 3.25167
INFO:tensorflow:loss = 0.107763, step = 36901 (30.752 sec)
INFO:tensorflow:Saving checkpoints for 37000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-37000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.15885.
INFO:tensorflow:Evaluating model now.
2018-08-22 06:03:16.976772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-10:03:18
2018-08-22 06:03:18.497112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-37000
INFO:tensorflow:Finished evaluation at 2018-08-22-10:03:33
INFO:tensorflow:Saving dict for global step 37000: global_step = 37000, loss = 3.43806
INFO:tensorflow:Training model for 1000 steps
2018-08-22 06:03:33.405142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 06:03:36.210247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-37000
INFO:tensorflow:Saving checkpoints for 37001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-37001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.085736, step = 37001
INFO:tensorflow:global_step/sec: 3.15381
INFO:tensorflow:loss = 0.091385, step = 37101 (31.713 sec)
INFO:tensorflow:global_step/sec: 2.99834
INFO:tensorflow:loss = 0.139665, step = 37201 (33.348 sec)
INFO:tensorflow:global_step/sec: 3.08809
INFO:tensorflow:loss = 0.111125, step = 37301 (32.382 sec)
INFO:tensorflow:global_step/sec: 3.08717
INFO:tensorflow:loss = 0.104968, step = 37401 (32.392 sec)
INFO:tensorflow:global_step/sec: 2.99799
INFO:tensorflow:loss = 0.215991, step = 37501 (33.357 sec)
INFO:tensorflow:global_step/sec: 3.09237
INFO:tensorflow:loss = 0.120068, step = 37601 (32.337 sec)
INFO:tensorflow:global_step/sec: 3.07324
INFO:tensorflow:loss = 0.169466, step = 37701 (32.539 sec)
INFO:tensorflow:global_step/sec: 3.02231
INFO:tensorflow:loss = 0.114073, step = 37801 (33.087 sec)
INFO:tensorflow:global_step/sec: 3.18501
INFO:tensorflow:loss = 0.127169, step = 37901 (31.399 sec)
INFO:tensorflow:Saving checkpoints for 38000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-38000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.0926912.
INFO:tensorflow:Evaluating model now.
2018-08-22 06:09:09.540835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-10:09:11
2018-08-22 06:09:11.113762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-38000
INFO:tensorflow:Finished evaluation at 2018-08-22-10:09:25
INFO:tensorflow:Saving dict for global step 38000: global_step = 38000, loss = 3.45845
INFO:tensorflow:Training model for 1000 steps
2018-08-22 06:09:25.878939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 06:09:28.928063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-38000
INFO:tensorflow:Saving checkpoints for 38001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-38001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.0600559, step = 38001
INFO:tensorflow:global_step/sec: 3.08487
INFO:tensorflow:loss = 0.132874, step = 38101 (32.417 sec)
INFO:tensorflow:global_step/sec: 3.01257
INFO:tensorflow:loss = 0.218879, step = 38201 (33.197 sec)
INFO:tensorflow:global_step/sec: 3.02296
INFO:tensorflow:loss = 0.0721008, step = 38301 (33.077 sec)
INFO:tensorflow:global_step/sec: 3.05038
INFO:tensorflow:loss = 0.14708, step = 38401 (32.784 sec)
INFO:tensorflow:global_step/sec: 3.00082
INFO:tensorflow:loss = 0.130553, step = 38501 (33.324 sec)
INFO:tensorflow:global_step/sec: 3.00596
INFO:tensorflow:loss = 0.145592, step = 38601 (33.267 sec)
INFO:tensorflow:global_step/sec: 3.0372
INFO:tensorflow:loss = 0.155264, step = 38701 (32.924 sec)
INFO:tensorflow:global_step/sec: 3.19027
INFO:tensorflow:loss = 0.0944637, step = 38801 (31.346 sec)
INFO:tensorflow:global_step/sec: 3.22558
INFO:tensorflow:loss = 0.171786, step = 38901 (31.003 sec)
INFO:tensorflow:Saving checkpoints for 39000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-39000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.0888408.
INFO:tensorflow:Evaluating model now.
2018-08-22 06:15:03.260614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-10:15:04
2018-08-22 06:15:04.801857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-39000
INFO:tensorflow:Finished evaluation at 2018-08-22-10:15:19
INFO:tensorflow:Saving dict for global step 39000: global_step = 39000, loss = 3.45925
INFO:tensorflow:Training model for 1000 steps
2018-08-22 06:15:19.947173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=train
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
2018-08-22 06:15:22.992273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-39000
INFO:tensorflow:Saving checkpoints for 39001 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-39001 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:loss = 0.125126, step = 39001
INFO:tensorflow:global_step/sec: 3.05893
INFO:tensorflow:loss = 0.0940955, step = 39101 (32.692 sec)
INFO:tensorflow:global_step/sec: 2.96222
INFO:tensorflow:loss = 0.124804, step = 39201 (33.759 sec)
INFO:tensorflow:global_step/sec: 3.16943
INFO:tensorflow:loss = 0.091497, step = 39301 (31.553 sec)
INFO:tensorflow:global_step/sec: 3.07058
INFO:tensorflow:loss = 0.102386, step = 39401 (32.565 sec)
INFO:tensorflow:global_step/sec: 3.01887
INFO:tensorflow:loss = 0.148892, step = 39501 (33.126 sec)
INFO:tensorflow:global_step/sec: 3.02243
INFO:tensorflow:loss = 0.130707, step = 39601 (33.086 sec)
INFO:tensorflow:global_step/sec: 2.90477
INFO:tensorflow:loss = 0.14061, step = 39701 (34.427 sec)
INFO:tensorflow:global_step/sec: 3.12679
INFO:tensorflow:loss = 0.209945, step = 39801 (31.980 sec)
INFO:tensorflow:global_step/sec: 3.34792
INFO:tensorflow:loss = 0.0868156, step = 39901 (29.870 sec)
INFO:tensorflow:Saving checkpoints for 40000 into AttentionSeq2Seq_tune_model_data/model.ckpt.
INFO:tensorflow:AttentionSeq2Seq_tune_model_data/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO:tensorflow:Loss for final step: 0.140563.
INFO:tensorflow:Evaluating model now.
2018-08-22 06:20:57.759007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  !!python/unicode 'attention.class': !!python/unicode 'seq2seq.decoders.attention.AttentionLayerDot'
  !!python/unicode 'attention.params': {!!python/unicode 'num_units': 150}
  !!python/unicode 'bridge.class': !!python/unicode 'seq2seq.models.bridges.ZeroBridge'
  !!python/unicode 'bridge.params': {}
  !!python/unicode 'decoder.class': !!python/unicode 'seq2seq.decoders.AttentionDecoder'
  !!python/unicode 'decoder.params':
    !!python/unicode 'max_decode_length': 250
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'embedding.dim': 100
  !!python/unicode 'embedding.file': data/glove/glove.6B.100d.txt
  !!python/unicode 'embedding.init_scale': 0.04
  !!python/unicode 'embedding.share': false
  !!python/unicode 'embedding.tune': true
  !!python/unicode 'encoder.class': !!python/unicode 'seq2seq.encoders.BidirectionalRNNEncoder'
  !!python/unicode 'encoder.params':
    !!python/unicode 'rnn_cell':
      !!python/unicode 'cell_class': !!python/unicode 'LSTMCell'
      !!python/unicode 'cell_params': {!!python/unicode 'num_units': 150}
      !!python/unicode 'dropout_input_keep_prob': 0.5
      !!python/unicode 'dropout_output_keep_prob': 0.5
      !!python/unicode 'num_layers': 1
  !!python/unicode 'inference.beam_search.beam_width': 0
  !!python/unicode 'inference.beam_search.choose_successors_fn': !!python/unicode 'choose_top_k'
  !!python/unicode 'inference.beam_search.length_penalty_weight': 0.0
  !!python/unicode 'optimizer.clip_embed_gradients': 0.1
  !!python/unicode 'optimizer.clip_gradients': 5.0
  !!python/unicode 'optimizer.learning_rate': 0.0005
  !!python/unicode 'optimizer.lr_decay_rate': 0.99
  !!python/unicode 'optimizer.lr_decay_steps': 100
  !!python/unicode 'optimizer.lr_decay_type': !!python/unicode ''
  !!python/unicode 'optimizer.lr_min_learning_rate': 1.0e-12
  !!python/unicode 'optimizer.lr_staircase': false
  !!python/unicode 'optimizer.lr_start_decay_at': 0
  !!python/unicode 'optimizer.lr_stop_decay_at': 2147483647
  !!python/unicode 'optimizer.name': !!python/unicode 'Adam'
  !!python/unicode 'optimizer.params': {!!python/unicode 'epsilon': 8.0e-07}
  !!python/unicode 'optimizer.sync_replicas': 0
  !!python/unicode 'optimizer.sync_replicas_to_aggregate': 0
  !!python/unicode 'source.max_seq_len': 45
  !!python/unicode 'source.reverse': false
  !!python/unicode 'target.max_seq_len': 125
  !!python/unicode 'vocab_source': !!python/unicode 'data/datasets/data_final_processed/encode_vocab.txt'
  !!python/unicode 'vocab_target': !!python/unicode 'data/datasets/data_final_processed/decode_vocab.txt'

INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/encode_vocab.txt
INFO:tensorflow:Reading vocabulary from data/datasets/data_final_processed/decode_vocab.txt
INFO:tensorflow:Creating vocabulary lookup table of size 5884
INFO:tensorflow:Creating vocabulary lookup table of size 3419
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerDot in mode=eval
INFO:tensorflow:
AttentionLayerDot: {!!python/unicode 'num_units': 150}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  !!python/unicode 'init_scale': 0.04
  !!python/unicode 'max_decode_length': 250
  !!python/unicode 'rnn_cell':
    cell_class: LSTMCell
    cell_params: {num_units: 150}
    dropout_input_keep_prob: 0.5
    dropout_output_keep_prob: 0.5
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2018-08-22-10:20:59
2018-08-22 06:20:59.228641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Restoring parameters from AttentionSeq2Seq_tune_model_data/model.ckpt-40000
INFO:tensorflow:Finished evaluation at 2018-08-22-10:21:13
INFO:tensorflow:Saving dict for global step 40000: global_step = 40000, loss = 3.48491
INFO:tensorflow:Stop training model as max steps reached
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b0abf7710>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
added the to vector
added of to vector
added ? to vector
added . to vector
added what to vector
added and to vector
added are to vector
added in to vector
useracct not recognized; using random instead.
added name to vector
added is to vector
added names to vector
added all to vector
added circuits to vector
added country to vector
added that to vector
added with to vector
added have to vector
added for to vector
added , to vector
added find to vector
added who to vector
added student to vector
added record to vector
added how to vector
added many to vector
added show to vector
added each to vector
added than to vector
added list to vector
added which to vector
added by to vector
added most to vector
added concerts to vector
added id to vector
added average to vector
added a to vector
added students to vector
added first to vector
added has to vector
added order to vector
added not to vector
added mill to vector
added type to vector
added more to vector
added ids to vector
added or to vector
added last to vector
added whose to vector
added invoices to vector
added total to vector
added return to vector
added different to vector
added seasons to vector
added url to vector
added customers to vector
added to to vector
added checkin to vector
added count to vector
added at to vector
added there to vector
added constructor to vector
added standings to vector
added constructor to vector
added id to vector
added least to vector
added any to vector
added from to vector
added give to vector
added do to vector
added distinct to vector
added customer to vector
added addresses to vector
added city to vector
added weather to vector
added date to vector
added head to vector
added age to vector
added product to vector
added product to vector
added products to vector
added their to vector
added addresses to vector
added country to vector
added department to vector
added employees to vector
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added * to vector
added as to vector
added me to vector
added maximum to vector
added both to vector
added players to vector
added student to vector
added lessons to vector
added price to vector
added details to vector
added countries to vector
added ordered to vector
added staff to vector
added loan to vector
added amount to vector
added cities to vector
added document to vector
added descending to vector
added course to vector
added does to vector
added codes to vector
added on to vector
added types to vector
added highest to vector
added named to vector
added courses to vector
added people to vector
added one to vector
added 3 to vector
added item to vector
added title to vector
added film to vector
added text to vector
added description to vector
added documents to vector
added an to vector
added two to vector
added ascending to vector
added dates to vector
added directed to vector
added by to vector
added did to vector
added members to vector
added largest to vector
added used to vector
added races to vector
added circuit to vector
added id to vector
added were to vector
added minimum to vector
added qualifying to vector
added q3 to vector
added had to vector
added customers to vector
added state to vector
added they to vector
added alphabetical to vector
added customers to vector
added phone to vector
added employee to vector
added songs to vector
added salary to vector
added salary to vector
added corresponding to vector
added made to vector
added companies to vector
added review to vector
added rating to vector
added titles to vector
added after to vector
added customers to vector
added address to vector
added 2 to vector
added city to vector
added population to vector
added review to vector
added rank to vector
added was to vector
added sorted to vector
added higher to vector
added customers to vector
added company to vector
added some to vector
added team to vector
added flights to vector
added above to vector
added common to vector
added numbers to vector
added locations to vector
added where to vector
added top to vector
added party to vector
added account to vector
added 1 to vector
added matches to vector
added score to vector
added full to vector
added accounts to vector
added faculty to vector
added enrollment to vector
added checking to vector
added balance to vector
added payment to vector
added room to vector
added tracks to vector
added orders to vector
added years to vector
teamstats not recognized; using random instead.
firsttower not recognized; using random instead.
added between to vector
added rooms to vector
added produced to vector
added airport to vector
added song to vector
added less to vector
added school to vector
added project to vector
added before to vector
added been to vector
added 10 to vector
added greater to vector
added artists to vector
added wines to vector
added times to vector
added hours to vector
added departments to vector
firstblood not recognized; using random instead.
added lowest to vector
added greatest to vector
added contacts to vector
added gender to vector
added its to vector
added stations to vector
added participants to vector
added role to vector
added 5 to vector
added ) to vector
added ( to vector
added schools to vector
added qualifying to vector
added q2 to vector
added movie to vector
added but to vector
added section to vector
added building to vector
added user to vector
added profiles to vector
added email to vector
added customer to vector
added orders to vector
added customer to vector
added id to vector
added addresses to vector
added tv to vector
added status to vector
added time to vector
added no to vector
added high to vector
added schooler to vector
added grade to vector
added either to vector
added catalog to vector
added contents to vector
added capacity to vector
added races to vector
added live to vector
added instructors to vector
added claim to vector
added we to vector
added friends to vector
added across to vector
added states to vector
added located to vector
added class to vector
added below to vector
added authors to vector
added airports to vector
added 's to vector
added invoices to vector
added descriptions to vector
added 100 to vector
added official to vector
added movies to vector
added song to vector
added languages to vector
added it to vector
added program to vector
added college to vector
added those to vector
added teachers to vector
added home to vector
added game to vector
added games to vector
added club to vector
added artist to vector
added hall to vector
added of to vector
added fame to vector
added votes to vector
added transactions to vector
added songs to vector
added language to vector
added invoice to vector
added catalog to vector
added contents to vector
added height to vector
added every to vector
added customers to vector
added county to vector
added colleges to vector
added author to vector
added lake to vector
added area to vector
added professors to vector
added student to vector
added addresses to vector
added other to vector
added details to vector
added member to vector
added major to vector
added enrolled to vector
added template to vector
added ships to vector
added information to vector
added never to vector
added female to vector
added apartment to vector
added station to vector
added played to vector
added film to vector
added english to vector
added counties to vector
added checking to vector
added 2000 to vector
added battle to vector
added result to vector
added parties to vector
added opened to vector
added market to vector
added living to vector
added channel to vector
added player to vector
added weight to vector
added three to vector
added prices to vector
added detail to vector
added when to vector
added sex to vector
added record to vector
added results to vector
added result to vector
added id to vector
added person to vector
added job to vector
added car to vector
added tv to vector
added series to vector
added budget to vector
added 4 to vector
added without to vector
added projects to vector
added please to vector
added method to vector
added can to vector
added taught to vector
added problems to vector
added teach to vector
added sort to vector
added high to vector
added held to vector
added address to vector
added district to vector
added photos to vector
added color to vector
added program to vector
added requirement to vector
added category to vector
added word to vector
added start to vector
added savings to vector
added listed to vector
added among to vector
added airlines to vector
added 2010 to vector
added singers to vector
added registered to vector
added lower to vector
added founded to vector
added card to vector
added drivers to vector
added display to vector
added clubs to vector
added assigned to vector
added 20 to vector
added zip to vector
added use to vector
added oldest to vector
added institution to vector
added weather to vector
added events to vector
added dogs to vector
added credit to vector
added contains to vector
added birth to vector
added size to vector
added films to vector
added books to vector
added albums to vector
added 50 to vector
added value to vector
added shops to vector
added order to vector
added items to vector
added product to vector
added id to vector
added model to vector
added buildings to vector
added belong to vector
added associated to vector
added apartments to vector
added 1000 to vector
added them to vector
added region to vector
added male to vector
added campus to vector
added tourist to vector
added participated to vector
added older to vector
added genre to vector
added genre to vector
added movie to vector
added director to vector
added customer to vector
added address to vector
added history to vector
added address to vector
added id to vector
added star to vector
added rate to vector
added person to vector
added papers to vector
added meeting to vector
added letter to vector
added advisor to vector
added university to vector
added tell to vector
added routes to vector
added recently to vector
added matches to vector
added matches to vector
added duration to vector
added course to vector
added credits to vector
added channels to vector
added working to vector
added treatment to vector
added song to vector
added resolution to vector
added reported to vector
added policy to vector
added level to vector
added flight to vector
added cost to vector
added client to vector
added classes to vector
added aircrafts to vector
added ages to vector
added youngest to vector
added users to vector
added universities to vector
added students to vector
added in to vector
added detention to vector
added student to vector
added id to vector
added store to vector
added smallest to vector
added smaller to vector
added scores to vector
added revenue to vector
added regions to vector
added received to vector
added play to vector
added phones to vector
added attractions to vector
added wrestlers to vector
added won to vector
added station to vector
added services to vector
added mountains to vector
added industry to vector
added game to vector
added entrepreneurs to vector
added directed to vector
added contain to vector
added classroom to vector
added temperature to vector
added models to vector
added dorm to vector
added catalog to vector
added available to vector
added attended to vector
added allergy to vector
added aircraft to vector
added actors to vector
added transaction to vector
added series to vector
added sales to vector
added report to vector
added n't to vector
added headquarters to vector
added fewest to vector
added along to vector
added trips to vector
added took to vector
added taken to vector
added substring to vector
added shared to vector
added scientists to vector
added results to vector
added recent to vector
added race to vector
added positions to vector
added rankings to vector
added player to vector
added id to vector
added office to vector
added driver to vector
added directors to vector
added team to vector
added attendance to vector
added 30 to vector
added volumes to vector
added technicians to vector
added shop to vector
added invoice to vector
added lines to vector
added quantity to vector
added payments to vector
added over to vector
added money to vector
added manager to vector
added involved to vector
added gymnasts to vector
added end to vector
added election to vector
added dorms to vector
added delegates to vector
added branch to vector
added also to vector
added 200 to vector
added well to vector
added under to vector
added templates to vector
added poker to vector
added owner to vector
added music to vector
added make to vector
added items to vector
added host to vector
added festivals to vector
added branches to vector
added bookings to vector
added bigger to vector
added workshop to vector
added stadium to vector
added share to vector
added service to vector
added student to vector
added record to vector
added semester to vector
added playing to vector
added physician to vector
added pet to vector
added opening to vector
added aircraft to vector
added manufacturer to vector
added larger to vector
added hosts to vector
added home to vector
added enzymes to vector
added districts to vector
added devices to vector
added days to vector
added student to vector
added enrolment to vector
added courses to vector
added course to vector
added id to vector
added clients to vector
added cards to vector
added affected to vector
added 2009 to vector
added united to vector
added ratings to vector
added order to vector
added items to vector
added order to vector
added id to vector
added history to vector
added gas to vector
added expensive to vector
added degrees to vector
added competition to vector
added cars to vector
added book to vector
added allergies to vector
added about to vector
added train to vector
added teams to vector
added smith to vector
added concerts to vector
added season to vector
added salaries to vector
added program to vector
added professionals to vector
added pilots to vector
added per to vector
added offered to vector
added max to vector
added match to vector
added hardware to vector
added direct to vector
added distance to vector
added distance to vector
added correspond to vector
added balances to vector
added agency to vector
added 5000 to vector
added unit to vector
added participates to vector
added in to vector
added student to vector
added id to vector
added stores to vector
added complaints to vector
added staff to vector
added id to vector
added speed to vector
added published to vector
added process to vector
added president to vector
added popular to vector
added train to vector
added origin to vector
added organization to vector
added orchestras to vector
added old to vector
added contacts to vector
added last to vector
added name to vector
added got to vector
added contacts to vector
added first to vector
added name to vector
added train to vector
added destination to vector
added contact to vector
added album to vector
added younger to vector
added using to vector
added tryout to vector
added theme to vector
added storms to vector
added stadiums to vector
added spoken to vector
added settlement to vector
added released to vector
added physicians to vector
added personal to vector
added participate to vector
added city to vector
added latitude to vector
added gpa to vector
added competitions to vector
added bank to vector
added routes to vector
added airline to vector
added activity to vector
added work to vector
added san to vector
added red to vector
added mean to vector
added longest to vector
added hometown to vector
added votes to vector
added created to vector
added captain to vector
added amenities to vector
added 300 to vector
added video to vector
added trains to vector
added track to vector
added teacher to vector
added starting to vector
added spring to vector
added spent to vector
added source to vector
added ship to vector
added schoolers to vector
sawayn not recognized; using random instead.
added purchased to vector
added policies to vector
added percentage to vector
added paper to vector
added outcomes to vector
added open to vector
added makers to vector
added jobs to vector
added janessa to vector
added headquarter to vector
added frequently to vector
added format to vector
added event to vector
added dog to vector
added cylinders to vector
added climbers to vector
added claims to vector
added charge to vector
added captains to vector
added campuses to vector
added called to vector
added booking to vector
added alphabetically to vector
added 2008 to vector
added vocal to vector
added user to vector
added stock to vector
added range to vector
added procedures to vector
added point to vector
added pets to vector
added performed to vector
added passengers to vector
added musicals to vector
added membership to vector
added majors to vector
added loan to vector
added life to vector
added investors to vector
added instructor to vector
added hotels to vector
added his to vector
added hire to vector
added having to vector
added fee to vector
added fall to vector
added exhibitions to vector
added enrollments to vector
added checkin to vector
added day to vector
added characteristics to vector
added cat to vector
added 8 to vector
added written to vector
added visited to vector
added team to vector
added half to vector
added team to vector
added id to vector
added statuses to vector
added started to vector
added rating to vector
added rating to vector
added stars to vector
added shortest to vector
added reviewer to vector
added rated to vector
added pilot to vector
added perpetrators to vector
added performances to vector
added paragraphs to vector
added overall to vector
added other to vector
added once to vector
added maker to vector
added team to vector
added half to vector
added league to vector
added id to vector
added john to vector
added her to vector
added groups to vector
added family to vector
added entry to vector
added earnings to vector
added student to vector
added degree to vector
added organization to vector
added continent to vector
added concerts to vector
added candidate to vector
added body to vector
added bed to vector
added advisors to vector
added staff to vector
added active to vector
added 3000 to vector
added statement to vector
added singer to vector
added settlements to vector
added roles to vector
added reviewers to vector
added records to vector
added provided to vector
added professor to vector
added patients to vector
added part to vector
added outcome to vector
added out to vector
added m to vector
added horsepower to vector
added get to vector
added person to vector
added friend to vector
added friend to vector
added documents to vector
added document to vector
added id to vector
added cycle to vector
added containing to vector
added characteristic to vector
added built to vector
added attribute to vector
added activities to vector
added accounting to vector
added 2004 to vector
added unique to vector
added roller to vector
added research to vector
added representatives to vector
added rates to vector
added programs to vector
added products to vector
added product to vector
added name to vector
added problem to vector
added owned to vector
added nationalities to vector
added musician to vector
added middle to vector
added marketing to vector
added log to vector
added loans to vector
added artists to vector
added last to vector
added name to vector
added catalog to vector
added contents to vector
added length to vector
added artists to vector
added first to vector
added name to vector
added f to vector
added emails to vector
added editors to vector
added data to vector
added d to vector
added compute to vector
added student to vector
added city to vector
added code to vector
added cartoons to vector
added candidates to vector
added base to vector
appelations not recognized; using random instead.
acct-211 not recognized; using random instead.
added 90 to vector
added 40 to vector
added weeks to vector
added vocals to vector
added paper to vector
added venue to vector
added ship to vector
added tonnage to vector
added themes to vector
added submission to vector
added subject to vector
added stockings to vector
added speak to vector
added requested to vector
added publishers to vector
people_id not recognized; using id instead.
added patient to vector
added paid to vector
added nominated to vector
added nations to vector
added markets to vector
added london to vector
added tip to vector
added likes to vector
added store to vector
added last to vector
added update to vector
added hometowns to vector
added grapes to vector
added expectancy to vector
added elimination to vector
added done to vector
added described to vector
added conductors to vector
added complaint to vector
added builders to vector
added boston to vector
added belongs to vector
added banks to vector
added affiliated to vector
added 2002 to vector
added 10000 to vector
added visit to vector
added trip to vector
added sports to vector
added select to vector
added sections to vector
added rental to vector
added photos to vector
added owners to vector
added often to vector
added mill to vector
added notes to vector
added new to vector
laptimes not recognized; using random instead.
added milliseconds to vector
member_id not recognized; using id instead.
added medicine to vector
added manufacturers to vector
added los to vector
added station to vector
added longitude to vector
added latest to vector
added journalists to vector
added instruments to vector
added hall to vector
added famous to vector
added rooms to vector
added decor to vector
added computer to vector
added complaints to vector
added carriers to vector
added bus to vector
added bought to vector
added angeles to vector
added alice to vector
added 600 to vector
added 25 to vector
added 2014 to vector
added white to vector
added treatments to vector
added trained to vector
added together to vector
added submissions to vector
added party to vector
added services to vector
added service to vector
added id to vector
added s to vector
added rock to vector
added reviews to vector
added restaurant to vector
added railways to vector
added produce to vector
added prerequisite to vector
added place to vector
added much to vector
added mountain to vector
added medicines to vector
added managers to vector
added main to vector
added investor to vector
added hosted to vector
added goalie to vector
added francisco to vector
added food to vector
added file to vector
added exist to vector
added earliest to vector
added drama to vector
added denomination to vector
added award to vector
added amounts to vector
added affiliation to vector
added 2016 to vector
added 0 to vector
added york to vector
added training to vector
added ticket to vector
added these to vector
added teaches to vector
added support to vector
added studying to vector
added studios to vector
added storm to vector
added store to vector
added store to vector
added id to vector
added statistics to vector
added seating to vector
added saving to vector
added reviewed to vector
added review to vector
added release to vector
added related to vector
added ranks to vector
added rankings to vector
added ranking to vector
added publication to vector
added other to vector
added property to vector
added features to vector
added property to vector
added id to vector
added prix to vector
added ph to vector
added participants to vector
added organisation to vector
added order to vector
added items to vector
added order to vector
added item to vector
added id to vector
added oppose to vector
added only to vector
added movie to vector
added mid to vector
added medication to vector
added city to vector
added longitude to vector
added linda to vector
added lessons to vector
added international to vector
added industries to vector
added happened to vector
added grant to vector
added grand to vector
added faculties to vector
added ever to vector
added student to vector
added events to vector
added event to vector
added id to vector
added equal to vector
added engineer to vector
added customers to vector
added email to vector
added address to vector
added citizenship to vector
added cell to vector
added cartoon to vector
added broadcast to vector
added bikes to vector
added bike to vector
added attraction to vector
added assets to vector
added artworks to vector
added arriving to vector
added appointment to vector
added accessed to vector
added access to vector
added 500 to vector
added 2005 to vector
added 2001 to vector
added 1980 to vector
added yes to vector
added works to vector
added winery to vector
added wine to vector
added matches to vector
added version to vector
added uk to vector
added this to vector
added matches to vector
added surface to vector
added single to vector
added secretary to vector
school_id not recognized; using id instead.
added postseason to vector
added platforms to vector
added party to vector
added forms to vector
added party to vector
added id to vector
added operated to vector
added offers to vector
added museum to vector
added longer to vector
added line to vector
added lap to vector
added item to vector
added interact to vector
added guests to vector
added grouped to vector
added grape to vector
added government to vector
added football to vector
added user to vector
added profiles to vector
added followers to vector
added fault to vector
added expenses to vector
added exactly to vector
added episode to vector
added elections to vector
added departing to vector
added customers to vector
added customer to vector
added name to vector
added cross to vector
added crime to vector
added conferred to vector
added committee to vector
added cinema to vector
added cheapest to vector
added budgets to vector
added block to vector
added be to vector
added australian to vector
added attend to vector
added assessment to vector
added asia to vector
added agent to vector
added aged to vector
added accelerate to vector
added aberdeen to vector
added 1990 to vector
added 18 to vector
added 15 to vector
added wen to vector
added visitors to vector
added usa to vector
added twice to vector
added trade to vector
added tweets to vector
added text to vector
added teaching to vector
added sum to vector
added pitstops to vector
added stop to vector
added software to vector
added shipping to vector
added served to vector
added screen to vector
added scientist to vector
added scholarship to vector
added drivers to vector
added code to vector
added users to vector
added role to vector
added code to vector
added publisher to vector
added products to vector
added product to vector
added price to vector
added police to vector
added page to vector
added origins to vector
added organizations to vector
added offices to vector
added offer to vector
added teachers to vector
added middle to vector
added name to vector
added lounge to vector
added login to vector
added left to vector
added kyle to vector
added king to vector
added kind to vector
added invested to vector
added inventory to vector
added injury to vector
added head to vector
added gross to vector
added given to vector
added g to vector
added frequent to vector
added except to vector
added enzyme to vector
added engineers to vector
added airports to vector
added elevation to vector
added draft to vector
added documents to vector
added document to vector
added type to vector
added code to vector
added division to vector
added debates to vector
added death to vector
added customers to vector
added customer to vector
added phone to vector
added cover to vector
added river to vector
added country to vector
added name to vector
added country to vector
added country to vector
added id to vector
added compatible to vector
added coasters to vector
club_id not recognized; using id instead.
added classrooms to vector
added claimed to vector
added china to vector
channel_id not recognized; using id instead.
added categories to vector
added carole to vector
added builder to vector
branch_id not recognized; using id instead.
added pitching to vector
added postseason to vector
added bk to vector
added biology to vector
added banking to vector
added back to vector
appelation not recognized; using random instead.
added amenity to vector
added alphabetic to vector
added 2003 to vector
added 12000 to vector
added 100000 to vector
added addresses to vector
added zip to vector
added postcode to vector
added worked to vector
added winners to vector
added week to vector
added transcript to vector
added through to vector
added tests to vector
added taking to vector
added take to vector
added suppliers to vector
added starts to vector
added students to vector
added ssn to vector
added schooler to vector
added repair to vector
added prominence to vector
added customers to vector
added payment to vector
added method to vector
added code to vector
added pages to vector
order_items not recognized; using random instead.
added null to vector
added staff to vector
added nickname to vector
added media to vector
added lives to vector
added lesson to vector
added launched to vector
added student to vector
added lastname to vector
laptime not recognized; using random instead.
added kinds to vector
added jones to vector
added joined to vector
added hired to vector
added genres to vector
added gdp to vector
added fly to vector
added student to vector
firstname not recognized; using random instead.
added inventory to vector
added film to vector
added id to vector
added filed to vector
added employees to vector
added employee to vector
added id to vector
added earn to vector
added degree to vector
added programs to vector
added department to vector
added id to vector
added guests to vector
added date to vector
added of to vector
added birth to vector
added continents to vector
added conference to vector
added commission to vector
added colors to vector
added city to vector
added city to vector
added id to vector
added camera to vector
added lens to vector
added brand to vector
added belonging to vector
added bedrooms to vector
added band to vector
added american to vector
added certificate to vector
added airline to vector
added id to vector
added advise to vector
added actor to vector
added zach to vector
added winner to vector
added walk to vector
added us to vector
added up to vector
added tweets to vector
added table to vector
added system to vector
added swimmers to vector
added studio to vector
added stay to vector
added addresses to vector
added state to vector
added province to vector
added county to vector
added stage to vector
added shipment to vector
added serve to vector
added section to vector
added residents to vector
region_id not recognized; using id instead.
added reference to vector
added reached to vector
raceid not recognized; using id instead.
added student to vector
added program to vector
added id to vector
added professional to vector
added made to vector
added by to vector
added pid to vector
added customers to vector
added phone to vector
added number to vector
added performance to vector
added parks to vector
added team to vector
added park to vector
added paragraph to vector
added operate to vector
added occurred to vector
added nurses to vector
added nominees to vector
added neb to vector
added nancy to vector
added morning to vector
added monthly to vector
added modern to vector
added matches to vector
added minutes to vector
added minute to vector
added milk to vector
added methods to vector
added memberships to vector
added meetings to vector
added math to vector
added low to vector
added lines to vector
added lasted to vector
added kayaking to vector
added journal to vector
added jose to vector
added james to vector
added instrument to vector
added institutions to vector
added death to vector
added injured to vector
added individuals to vector
added heights to vector
added heads to vector
added handled to vector
added grades to vector
added go to vector
added genders to vector
added founder to vector
added constructors to vector
added constructor to vector
added id to vector
added flag to vector
added five to vector
added finance to vector
added fda to vector
added fastest to vector
added exhibition to vector
added europe to vector
added estimation to vector
added endowment to vector
added ending to vector
added ended to vector
added edwards to vector
driver_id not recognized; using id instead.
added detention to vector
added destruction to vector
added destroyed to vector
added decreasing to vector
added customer to vector
added address to vector
added history to vector
added date to vector
added to to vector
added customer to vector
added address to vector
added history to vector
added date to vector
added from to vector
added database to vector
added costs to vector
added copies to vector
added contestant to vector
added contents to vector
added conducted to vector
added office to vector
added locations to vector
added company to vector
added id to vector
added committees to vector
added come to vector
added cinemas to vector
added chinese to vector
added certificates to vector
added certificate to vector
added state to vector
added capital to vector
added canoeing to vector
added call to vector
added c to vector
added booked to vector
added bernhard to vector
added ben to vector
added rooms to vector
added beds to vector
added basketball to vector
added availability to vector
added austin to vector
added asian to vector
added architects to vector
added agencies to vector
added 9 to vector
added 80 to vector
added 70 to vector
added 7 to vector
added 32 to vector
added 2011 to vector
added 1989 to vector
added 1850 to vector
added 1500 to vector
added 150 to vector
added 111 to vector
added x to vector
added w to vector
added volume to vector
added visits to vector
added visitor to vector
added venues to vector
added users to vector
added user to vector
added id to vector
added tutors to vector
added tryouts to vector
added transcripts to vector
added then to vector
added detention to vector
added teacher to vector
added id to vector
added tables to vector
added constructors to vector
added constructor to vector
added reference to vector
added structure to vector
added status to vector
added station to vector
added id to vector
added mountain to vector
added state to vector
added name to vector
added spokesmen to vector
added spokesman to vector
added someone to vector
added team to vector
added so to vector
added side to vector
shop_id not recognized; using id instead.
added settled to vector
added services to vector
added service to vector
added type to vector
added code to vector
added rylan to vector
added retrieve to vector
added reserved to vector
added regional to vector
added ranges to vector
added ram to vector
added r to vector
added purchase to vector
added publications to vector
added public to vector
added products to vector
added product to vector
added description to vector
added prerequisites to vector
policy_id not recognized; using id instead.
added playlists to vector
added playlist to vector
added placed to vector
added pit to vector
added percent to vector
added parts to vector
added package to vector
added concerts to vector
added orchestra to vector
added nominee to vector
added musical to vector
added museums to vector
added written to vector
added by to vector
msid not recognized; using id instead.
added mode to vector
added ministers to vector
added medals to vector
added managed to vector
mailshots not recognized; using random instead.
added machines to vector
added lots to vector
added lived to vector
added la to vector
added l to vector
added kids to vector
added invoice to vector
added line to vector
added items to vector
added invoice to vector
added number to vector
added info to vector
added includes to vector
added h to vector
added governor to vector
added furnitures to vector
added french to vector
added forms to vector
added form to vector
added floors to vector
added final to vector
added farms to vector
added eliminated to vector
added earning to vector
added delegate to vector
added cv to vector
added matches to vector
added creation to vector
added content to vector
added concert to vector
added electoral to vector
added register to vector
added cmi to vector
added cross to vector
added reference to vector
added id to vector
added circuits to vector
added checkin to vector
added cid to vector
added churches to vector
added cancelled to vector
added california to vector
added browsers to vector
added browser to vector
added breed to vector
added born to vector
added asset to vector
added animal to vector
added altitude to vector
added africa to vector
added advises to vector
added addressed to vector
added accreditation to vector
added accelerators to vector
added abbreviation to vector
added 6 to vector
added 4000 to vector
added 2015 to vector
added 2007 to vector
added young to vector
added you to vector
added worth to vector
added wineries to vector
added web to vector
added vote to vector
added vice to vector
added vehicle to vector
added undergoing to vector
added typical to vector
added turcotte to vector
added tournament to vector
added titled to vector
added systems to vector
added swimmer to vector
added supplier to vector
added stories to vector
added sony to vector
added snatch to vector
added sizes to vector
added sic to vector
added shows to vector
added shipped to vector
added same to vector
added revenues to vector
added reservations to vector
added rented to vector
added psychology to vector
added tasks to vector
added project to vector
added id to vector
added produces to vector
added processed to vector
added populations to vector
added pop to vector
added users to vector
added password to vector
added parking to vector
paperid not recognized; using id instead.
added palo to vector
added our to vector
added customer to vector
added orders to vector
added order to vector
added status to vector
added code to vector
added order to vector
added items to vector
added order to vector
added quantity to vector
added customer to vector
added orders to vector
added order to vector
added date to vector
open_year not recognized; using random instead.
added fare to vector
added basis to vector
added night to vector
added mp3 to vector
added tip to vector
added month to vector
added monitor to vector
added mission to vector
added mill to vector
added midshipman to vector
added michael to vector
added mate to vector
added master to vector
added mark to vector
added march to vector
added customers to vector
added login to vector
added name to vector
location_id not recognized; using id instead.
added lieutenant to vector
added lens to vector
added lamberton to vector
added labels to vector
added label to vector
added july to vector
added jeromy to vector
added issue to vector
institution_id not recognized; using id instead.
added include to vector
added incident to vector
added team to vector
added hr to vector
added horses to vector
added honolulu to vector
added he to vector
added harris to vector
added happy to vector
added grants to vector
added granted to vector
added forenames to vector
added floor to vector
added festival to vector
added other to vector
added property to vector
added features to vector
added feature to vector
added id to vector
added feature to vector
added faults to vector
added facility to vector
added estimations to vector
added entries to vector
added entrepreneur to vector
added editor to vector
added east to vector
added dutch to vector
added during to vector
added durations to vector
driverid not recognized; using id instead.
added dollars to vector
added dno to vector
added distances to vector
added device to vector
added destinations to vector
added democratic to vector
added delivery to vector
added defender to vector
added tryout to vector
added decision to vector
stats1 not recognized; using random instead.
added deaths to vector
added dan to vector
customer_orders not recognized; using random instead.
added customers to vector
added customer to vector
added email to vector
added counts to vector
added contestants to vector
added conferences to vector
added conductor to vector
added completion to vector
added complete to vector
added coaster to vector
added coast to vector
added coaches to vector
added cmi to vector
added clothes to vector
added clara to vector
claim_id not recognized; using id instead.
added city to vector
added city to vector
added name to vector
added check to vector
added ceremony to vector
added central to vector
added cctv to vector
added caused to vector
added catalogs to vector
added carrier to vector
added canada to vector
added cameron to vector
added camera to vector
added tip to vector
added business to vector
added id to vector
added institution to vector
added building to vector
added id to vector
added party to vector
added services to vector
added booking to vector
added id to vector
added boeing to vector
added bob to vector
added black to vector
added player to vector
added birth to vector
added year to vector
added biggest to vector
added battles to vector
added bangla to vector
added awards to vector
added author to vector
added list to vector
added author to vector
added id to vector
added audio to vector
added audience to vector
added attendees to vector
added art to vector
added areas to vector
added appointments to vector
added alto to vector
added addresses to vector
added address to vector
added details to vector
added accidents to vector
added acc to vector
94107 not recognized; using random instead.
added 900 to vector
added 85 to vector
added 80000 to vector
added 8000 to vector
added 6000 to vector
added 50000 to vector
added 23 to vector
added 2017 to vector
added 2013 to vector
added 1999 to vector
added 1996 to vector
added 1995 to vector
added weather to vector
added zip to vector
added code to vector
added within to vector
added wisconsin to vector
added qualifying to vector
added constructor to vector
added id to vector
added winning to vector
added wind to vector
added wedding to vector
added values to vector
added usd to vector
added results to vector
added race to vector
added id to vector
added trust to vector
added trust to vector
added customers to vector
added town to vector
added city to vector
added rankings to vector
added tours to vector
added tourists to vector
tourist_attraction_id not recognized; using id instead.
added tokyo to vector
added texts to vector
added texas to vector
added terms to vector
added tax to vector
added tasks to vector
added task to vector
added tallest to vector
added study to vector
added professionals to vector
added street to vector
added department to vector
added stores to vector
added store to vector
added name to vector
added flight to vector
added stops to vector
added stocks to vector
added staying to vector
added statements to vector
added staffs to vector
songid not recognized; using id instead.
added showed to vector
added shorter to vector
added team to vector
added sf to vector
added senator to vector
added student to vector
added enrolment to vector
added semester to vector
added id to vector
added seats to vector
added seasons to vector
added science to vector
added team to vector
added sb to vector
added sang to vector
added runs to vector
added roy to vector
added resident to vector
added registration to vector
added receive to vector
added ratio to vector
added ranked to vector
added quantities to vector
added provide to vector
added properties to vector
added property to vector
added type to vector
added code to vector
added property to vector
added properties to vector
added profits to vector
added products to vector
added product to vector
added type to vector
added code to vector
added staff to vector
added picture to vector
added photo to vector
added performer to vector
added participant to vector
added own to vector
added outstanding to vector
added organization to vector
added contact to vector
added individuals to vector
added organization to vector
added id to vector
added organisations to vector
added oct to vector
added occupation to vector
added ny to vector
added north to vector
added nicknames to vector
added net to vector
added cyclist to vector
added nation to vector
added musicians to vector
added mobile to vector
added mills to vector
added millions to vector
added miles to vector
added measurement to vector
market_id not recognized; using id instead.
added managing to vector
added lot to vector
added losers to vector
added logs to vector
added order to vector
added deliveries to vector
added location to vector
added code to vector
added leave to vector
added kingdom to vector
added death to vector
added killed to vector
added journals to vector
added invoice to vector
added lines to vector
added invoice to vector
added id to vector
added interactions to vector
added increasing to vector
added if to vector
added humidity to vector
added hour to vector
added hold to vector
added team to vector
added hbp to vector
added hartford to vector
added pitching to vector
added postseason to vector
added gs to vector
added governments to vector
added gone to vector
added gold to vector
added gnp to vector
added furniture to vector
added frequency to vector
added france to vector
added flags to vector
added files to vector
added fewer to vector
added females to vector
added farm to vector
added expense to vector
added eliminations to vector
added donor to vector
added documents to vector
added document to vector
added name to vector
added document to vector
added types to vector
added document to vector
added description to vector
added student to vector
added department to vector
added name to vector
added departure to vector
added depart to vector
added damage to vector
customers' not recognized; using random instead.
added customers to vector
added customer to vector
added details to vector
added customers to vector
added customer to vector
added address to vector
added csu to vector
added team to vector
added cs to vector
added contacted to vector
added constructors to vector
constructorid not recognized; using id instead.
added comptroller to vector
added cloud to vector
added climb to vector
added church to vector
added chip to vector
added chapters to vector
added buying to vector
added business to vector
added budgeted to vector
added brittany to vector
added bridges to vector
book_id not recognized; using id instead.
added bonus to vector
added player to vector
added birth to vector
added city to vector
added team to vector
added bb to vector
added bathrooms to vector
added authorised to vector
added fault to vector
added log to vector
added asset to vector
added id to vector
added albums to vector
added artist to vector
added id to vector
added arrival to vector
added approval to vector
added affirmative to vector
added advised to vector
added adults to vector
added reference to vector
added address to vector
added types to vector
added address to vector
added type to vector
added code to vector
added financial to vector
added transactions to vector
added account to vector
added id to vector
added accommodate to vector
added accelerator to vector
added 800 to vector
added 737-800 to vector
added 60 to vector
200000 not recognized; using random instead.
added 1998 to vector
added 120 to vector
10000000 not recognized; using random instead.
added 000 to vector
added yet to vector
added wta to vector
added wrestler to vector
added wifi to vector
added weddings to vector
added wall to vector
added voting to vector
added voted to vector
added vincent to vector
added view to vector
added velocity to vector
added vehicles to vector
added lessons to vector
added vehicle to vector
added id to vector
added valid to vector
added utah to vector
added user to vector
added profiles to vector
added uid to vector
added ucla to vector
added tweeted to vector
added tried to vector
added treasurer to vector
added financial to vector
added transactions to vector
added transaction to vector
added id to vector
added route to vector
added train to vector
added id to vector
added tracking to vector
added playlist to vector
added tracks to vector
added track to vector
added id to vector
added todd to vector
added third to vector
added timed to vector
added locations to vector
added of to vector
added things to vector
added thing to vector
added id to vector
added technician to vector
added surgery to vector
added supplied to vector
added sung to vector
added subway to vector
students' not recognized; using random instead.
added striker to vector
added stored to vector
added pitching to vector
added stint to vector
added steven to vector
start_year not recognized; using random instead.
added trip to vector
added start to vector
added date to vector
added stands to vector
added standing to vector
added research to vector
added staff to vector
added staff to vector
added details to vector
added game to vector
added stadium to vector
added id to vector
added sport to vector
added spielberg to vector
added spend to vector
added speeds to vector
added speaks to vector
added spanish to vector
added spain to vector
added sonoma to vector
added smithson to vector
added skill to vector
singer_id not recognized; using id instead.
added short to vector
added shipment to vector
added items to vector
added shipment to vector
added id to vector
ship_id not recognized; using id instead.
added shark to vector
added shares to vector
added pitching to vector
added postseason to vector
added sh to vector
added selling to vector
added sea to vector
added lives to vector
added in to vector
added room to vector
added number to vector
added roles to vector
added role to vector
added description to vector
added review to vector
added rid to vector
added customer to vector
added event to vector
added notes to vector
added resident to vector
added id to vector
added residence to vector
added researcher to vector
added require to vector
added reputation to vector
added republics to vector
added republic to vector
added representative to vector
added relationship to vector
added reigns to vector
added regular to vector
added register to vector
added regarding to vector
added recorded to vector
added ray to vector
added raised to vector
question_id not recognized; using id instead.
qm-261 not recognized; using random instead.
added purchases to vector
added psychiatry to vector
added progress to vector
programid not recognized; using id instead.
procrastin-x not recognized; using random instead.
added processing to vector
added primary to vector
added primarily to vector
added pressure to vector
added prescribed to vector
added prescribe to vector
added premises to vector
added premise to vector
added customers to vector
added postal to vector
added code to vector
added postal to vector
added post to vector
added poll to vector
players' not recognized; using random instead.
added platform to vector
added physics to vector
added persons to vector
added perpetrator to vector
performance_id not recognized; using id instead.
added percentages to vector
added pending to vector
added payment to vector
added payment to vector
added id to vector
added past to vector
added participating to vector
added owns to vector
added addresses to vector
added other to vector
added address to vector
added details to vector
added otha to vector
added org to vector
added orange to vector
added nurse to vector
added notification to vector
added next to vector
added mpg to vector
added mpeg to vector
added mp4 to vector
added moyer to vector
added minister to vector
added min to vector
added medications to vector
added mancini to vector
manager_id not recognized; using id instead.
added manage to vector
added makes to vector
added maintenance to vector
added machine to vector
added lost to vector
added loss to vector
added logged to vector
added local to vector
added addresses to vector
added line to vector
added 2 to vector
added addresses to vector
added line to vector
added 1 to vector
added number to vector
added building to vector
added addresses to vector
added line to vector
added 1 to vector
added liked to vector
added lewis to vector
added lenses to vector
added left-footed to vector
added leaving to vector
added leader to vector
added lead to vector
added launch to vector
added latte to vector
added later to vector
added lacrosse to vector
added tags to vector
added kid to vector
added keyword to vector
added keyword to vector
added jazz to vector
added invoices to vector
added invoice to vector
added date to vector
added interaction to vector
added inn to vector
added individual to vector
added images to vector
added pitching to vector
added postseason to vector
added ibb to vector
added airlines to vector
added iata to vector
added organization to vector
added homepage to vector
homenick not recognized; using random instead.
added hkg to vector
added hiring to vector
added players to vector
added hand to vector
added gymnast to vector
added gruber to vector
added governors to vector
added goodwin to vector
added goods to vector
added good to vector
added goalies to vector
added giuliano to vector
added injury to vector
added accident to vector
added game to vector
added id to vector
added gallon to vector
added pitching to vector
added postseason to vector
added g to vector
added idp to vector
added four to vector
added files to vector
added formats to vector
added forces to vector
added followed to vector
added flight to vector
added stop to vector
added flight to vector
added id to vector
added flied to vector
added customers to vector
added fax to vector
added fast to vector
added fame to vector
facid not recognized; using id instead.
added everybody to vector
added evaluation to vector
added team to vector
added era to vector
added enter to vector
added enroll to vector
added trip to vector
added end to vector
added date to vector
employeeid not recognized; using id instead.
added elsa to vector
added ebba to vector
added e to vector
added duke to vector
added donation to vector
added domestic to vector
district_id not recognized; using id instead.
added dew to vector
added detention to vector
added detention to vector
added type to vector
added code to vector
added detailed to vector
added destiny to vector
dept_code not recognized; using random instead.
added denominations to vector
added definitions to vector
added customer to vector
added event to vector
added notes to vector
added date to vector
added moved to vector
added in to vector
added customers to vector
added date to vector
added became to vector
added customer to vector
added customer to vector
added addresses to vector
added date to vector
added address to vector
added to to vector
added customer to vector
added addresses to vector
added date to vector
added address to vector
added from to vector
customer_addresses not recognized; using random instead.
added rent to vector
added arrears to vector
added council to vector
added tax to vector
added id to vector
added consider to vector
added tracks to vector
added composer to vector
added components to vector
competition_id not recognized; using id instead.
added college to vector
added college to vector
added id to vector
added climber to vector
added climbed to vector
added meetings to vector
added client to vector
added id to vector
added clerical to vector
cis-220 not recognized; using random instead.
added circulation to vector
added chicago to vector
added chi to vector
added checked to vector
added characters to vector
added championships to vector
added teachers to vector
added cell to vector
added mobile to vector
added number to vector
added casey to vector
added case to vector
added calendar to vector
added bridge to vector
added player to vector
added birthday to vector
added employees to vector
added birth to vector
added date to vector
added billed to vector
added best to vector
added behavior to vector
added became to vector
added battle to vector
added based to vector
added player to vector
added award to vector
added vote to vector
added award to vector
added id to vector
added australia to vector
added attendances to vector
added atlanta to vector
added astrid to vector
added ashley to vector
added aruba to vector
added arranged to vector
added view to vector
added unit to vector
added status to vector
added apartment to vector
added id to vector
added apg to vector
added anthony to vector
added allergic to vector
aircraft_id not recognized; using id instead.
added airbus to vector
added air to vector
added afghanistan to vector
added actual to vector
added accessible to vector
added acceptance to vector
added a340-300 to vector
added 841 to vector
added 72 to vector
added 700 to vector
added 5th to vector
5000000 not recognized; using random instead.
added 46 to vector
added 3500 to vector
added 301 to vector
300000 not recognized; using random instead.
added 2012 to vector
2007-12-25 not recognized; using random instead.
added 2006 to vector
added 20000 to vector
1989-09-03 not recognized; using random instead.
1978-06-26 not recognized; using random instead.
added 1961 to vector
added 1950 to vector
added 18000 to vector
added 12 to vector
added 11 to vector
added wrote to vector
added writers to vector
added writer to vector
added pitching to vector
added postseason to vector
added wp to vector
added workshops to vector
workshop_group_id not recognized; using id instead.
added win to vector
added west to vector
added went to vector
added weights to vector
added ways to vector
added volvo to vector
added visibility to vector
added visa to vector
added usps to vector
added uses to vector
added unsure to vector
university_id not recognized; using id instead.
added invoice to vector
added lines to vector
added unit to vector
added price to vector
added tutor to vector
added team to vector
added triple to vector
added traveled to vector
added transcript to vector
added contents to vector
added transcript to vector
added id to vector
trackid not recognized; using id instead.
added tourney to vector
added time to vector
added zone to vector
added time to vector
added zone to vector
added code to vector
added tickets to vector
added things to vector
added test to vector
added taxes to vector
added taller to vector
sweazy not recognized; using random instead.
added team to vector
added sv to vector
added product to vector
added suppliers to vector
added supplier to vector
added id to vector
added summary to vector
added successfully to vector
added string to vector
added department to vector
added stores to vector
added store to vector
added phone to vector
added steered to vector
added stayed to vector
added state to vector
added state to vector
added code to vector
sqlite_sequence not recognized; using random instead.
added south to vector
added skills to vector
added skills to vector
added required to vector
added to to vector
added fix to vector
added skill to vector
added id to vector
added silver to vector
added reserves to vector
added sailor to vector
added id to vector
added team to vector
added sho to vector
settlement_id not recognized; using id instead.
added dual to vector
added carrier to vector
added service to vector
added name to vector
added things to vector
added service to vector
added details to vector
added artists to vector
added name to vector
added sep to vector
added semesters to vector
added document to vector
added sections to vector
added images to vector
added section to vector
added id to vector
added second to vector
added takes to vector
added classes to vector
added section to vector
added id to vector
added sci to vector
added schedule to vector
added russia to vector
added route to vector
added properties to vector
added room to vector
added count to vector
added reversed to vector
added reverse to vector
resid not recognized; using id instead.
added reservation to vector
added film to vector
added release to vector
added year to vector
added reign to vector
added actual to vector
added orders to vector
added regular to vector
added order to vector
added id to vector
region_name not recognized; using random instead.
ref_document_types not recognized; using random instead.
added receipt to vector
added rankings to vector
added ran to vector
railway_id not recognized; using id instead.
added railway to vector
added radio to vector
added province to vector
added proteins to vector
added protein to vector
added production to vector
added products to vector
added product to vector
added details to vector
added products to vector
added product to vector
added category to vector
added code to vector
added producing to vector
added processes to vector
added staff to vector
added in to vector
added processes to vector
added process to vector
added id to vector
added procedure to vector
primary_conference not recognized; using random instead.
added presently to vector
added predominantly to vector
added weekly to vector
added weather to vector
added precipitation to vector
added practicing to vector
added ppt to vector
added possible to vector
positiontext not recognized; using random instead.
added porphyria to vector
added available to vector
added policies to vector
added policy to vector
added type to vector
added code to vector
added player to vector
added player to vector
added name to vector
added planet to vector
added places to vector
added pixels to vector
pilot_id not recognized; using id instead.
added time to vector
added interval to vector
added period to vector
added customers to vector
added payment to vector
added method to vector
added payed to vector
added performance to vector
added score to vector
added participant to vector
added id to vector
added part to vector
added faults to vector
added part to vector
added id to vector
added skills to vector
added required to vector
added to to vector
added fix to vector
added part to vector
added fault to vector
added id to vector
added products to vector
added parent to vector
added product to vector
added id to vector
added paper to vector
added paper to vector
added id to vector
owner{value}s not recognized; using random instead.
added research to vector
added outcomes to vector
added outcome to vector
added code to vector
added students to vector
added other to vector
added student to vector
added details to vector
added staff to vector
added other to vector
added staff to vector
added details to vector
other_item_details not recognized; using random instead.
added orton to vector
added projects to vector
added organisation to vector
added id to vector
added customer to vector
added orders to vector
added order to vector
added details to vector
added options to vector
added operating to vector
added online to vector
added ones to vector
added omim to vector
added officers to vector
added student to vector
added record to vector
added offering to vector
added id to vector
added occurs to vector
added nyc to vector
added death to vector
added note to vector
added nomination to vector
added network to vector
added negative to vector
added native to vector
museum_id not recognized; using id instead.
added moves to vector
added move to vector
added photos to vector
added mountain to vector
added id to vector
added motor to vector
added missions to vector
added minoring to vector
added meter to vector
added measure to vector
firstinhib not recognized; using random instead.
added masters to vector
added mascot to vector
marketing_region_code not recognized; using random instead.
added mar to vector
manager_name not recognized; using random instead.
added males to vector
added majoring to vector
added lucas to vector
added loser to vector
added delivery to vector
added route to vector
added locations to vector
added location to vector
added name to vector
added lexicographical to vector
level_of_membership not recognized; using random instead.
added led to vector
added league to vector
added latin to vector
added language to vector
added language to vector
added id to vector
langauges not recognized; using random instead.
added kills to vector
added kentucky to vector
added just to vector
added jun to vector
added journalist to vector
added joining to vector
added join to vector
job_id not recognized; using id instead.
added publication to vector
added jid to vector
added jetblue to vector
added italy to vector
added issues to vector
added isbn to vector
added team to vector
ipouts not recognized; using random instead.
added transactions to vector
added investor to vector
added id to vector
added program to vector
added introduction to vector
added works to vector
added interval to vector
added comment to vector
added instructor to vector
added instructor to vector
added id to vector
added injuries to vector
added indiana to vector
added incurred to vector
added including to vector
added included to vector
added incidents to vector
added illinois to vector
added il to vector
added airlines to vector
added icao to vector
ic_id not recognized; using id instead.
added review to vector
added item to vector
added id to vector
added hotel to vector
added professionals to vector
added home to vector
added phone to vector
added highly to vector
added handed to vector
added graduate to vector
added germany to vector
added georgia to vector
added matches to vector
gameid not recognized; using id instead.
added friendly to vector
added formed to vector
added force to vector
added florida to vector
added fl to vector
added financial to vector
added feet to vector
added feb to vector
added features to vector
added fault to vector
added log to vector
added parts to vector
added fault to vector
added log to vector
added entry to vector
added id to vector
added fates to vector
added factories to vector
exam_id not recognized; using id instead.
added student to vector
added events to vector
added event to vector
added type to vector
added code to vector
added evaluations to vector
added team to vector
added er to vector
added engineer to vector
added visits to vector
added engineer to vector
added id to vector
added employees to vector
added employee to vector
added name to vector
emp_num not recognized; using random instead.
added education to vector
edispl not recognized; using random instead.
added earned to vector
added earlier to vector
added dvd to vector
drivers' not recognized; using random instead.
added circulation to vector
added history to vector
added draft to vector
added number to vector
added team to vector
added dp to vector
added team to vector
added double to vector
added dose to vector
added lives to vector
added in to vector
added dorm to vector
added id to vector
added donors to vector
added donator to vector
added reference to vector
added document to vector
added types to vector
added document to vector
added type to vector
added description to vector
document_object_id not recognized; using id instead.
added document to vector
added sections to vector
added document to vector
added code to vector
added doctor to vector
director_id not recognized; using id instead.
directed_by not recognized; using random instead.
added difference to vector
added students to vector
added in to vector
added detention to vector
added detention to vector
added id to vector
added departments to vector
added department to vector
added name to vector
added denmark to vector
added dec to vector
added orders to vector
added date to vector
added order to vector
added placed to vector
customerid not recognized; using id instead.
added customers to vector
added customer to vector
added last to vector
added name to vector
added customers to vector
added customer to vector
added first to vector
added name to vector
customer_email_address not recognized; using random instead.
added checking to vector
added customer to vector
added id to vector
added currently to vector
added create to vector
added cows to vector
added gsi to vector
added course to vector
added offering to vector
added id to vector
added courses to vector
added course to vector
added name to vector
added courses to vector
added course to vector
added description to vector
county_id not recognized; using id instead.
added council to vector
added corresponds to vector
added contract to vector
added constructor to vector
added third to vector
added party to vector
added companies to vector
added company to vector
added name to vector
added comp to vector
added commonly to vector
added commander to vector
added colorado to vector
collection_id not recognized; using id instead.
added coach to vector
cname not recognized; using random instead.
added citizenships to vector
added citation to vector
added chromosome to vector
christop not recognized; using random instead.
added team to vector
added cg to vector
added certified to vector
added cats to vector
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added catalog to vector
added level to vector
added number to vector
added care to vector
added capacities to vector
added candidate to vector
added assessments to vector
added candidate to vector
added id to vector
added bulgarian to vector
added brazil to vector
added bosco to vector
blockfloor not recognized; using random instead.
blockcode not recognized; using random instead.
added birthdays to vector
added business to vector
added bid to vector
added beijing to vector
added bathroom to vector
added bangladesh to vector
added balls to vector
added bad to vector
added awarded to vector
added authorize to vector
added authority to vector
added august to vector
added attending to vector
artistid not recognized; using id instead.
added song to vector
added artist to vector
added name to vector
added approximately to vector
added appellations to vector
added appears to vector
added appeared to vector
amount_claimed not recognized; using random instead.
albumid not recognized; using id instead.
added airways to vector
added flight to vector
added airport to vector
added id to vector
added ground to vector
added service to vector
added airport to vector
added code to vector
added ahd to vector
added clients to vector
added agency to vector
added id to vector
added african to vector
added customers to vector
added address to vector
added line to vector
added 1 to vector
added order to vector
added deliveries to vector
added actual to vector
added order to vector
added id to vector
added actresses to vector
added film to vector
added actor to vector
added actor to vector
added id to vector
added acting to vector
added faculty to vector
added participates to vector
added in to vector
added activity to vector
added id to vector
added accepted to vector
added team to vector
added ab to vector
added 75 to vector
4560596484842 not recognized; using random instead.
added 36 to vector
added 35 to vector
added 30000 to vector
added 21 to vector
1986-11-13 not recognized; using random instead.
added 15000 to vector
added 140 to vector
added 1121 to vector
‘smith’ not recognized; using random instead.
added zinfandel to vector
added z to vector
years_working not recognized; using random instead.
year_join not recognized; using random instead.
added e to vector
added wy to vector
written_by not recognized; using random instead.
added writes to vector
added write to vector
wrestler_id not recognized; using id instead.
added worldwide to vector
added world to vector
worktitle not recognized; using random instead.
workshop_id not recognized; using id instead.
work_type not recognized; using random instead.
added williams to vector
widenius not recognized; using random instead.
added written to vector
added by to vector
added wid to vector
added wheels to vector
added weighing to vector
added weighed to vector
added weigh to vector
weekly_rank not recognized; using random instead.
added weekly to vector
added weber to vector
added way to vector
added waterbury to vector
added washington to vector
added ward to vector
added walter to vector
added player to vector
added award to vector
added vote to vector
added votes to vector
added first to vector
added voters to vector
added vivian to vector
added virginia to vector
added village to vector
venueid not recognized; using id instead.
added properties to vector
added vendor to vector
added requested to vector
added price to vector
added vat to vector
valid_answer_id not recognized; using id instead.
added staff to vector
added username to vector
added users to vector
added user to vector
added category to vector
added code to vector
added urls to vector
added urban to vector
unitprice not recognized; using random instead.
added undergraduates to vector
added uganda to vector
added user to vector
added id to vector
added tweet to vector
tv_show_id not recognized; using id instead.
tv_series not recognized; using random instead.
added trying to vector
added trucks to vector
added order to vector
added deliveries to vector
added truck to vector
added id to vector
added treatments to vector
added treatment to vector
added type to vector
added code to vector
added treated to vector
added transportation to vector
added transit to vector
added transactions to vector
added transaction to vector
added type to vector
added code to vector
added financial to vector
added transactions to vector
added transaction to vector
added type to vector
added financial to vector
added transactions to vector
added transaction to vector
added date to vector
added financial to vector
added transactions to vector
added transaction to vector
added comment to vector
added financial to vector
added transactions to vector
added transaction to vector
added amount to vector
added traditional to vector
added trachsel to vector
tourist_id not recognized; using id instead.
added totals to vector
total_passengers not recognized; using random instead.
added student to vector
added total to vector
added credit to vector
added tony to vector
added flight to vector
added to to vector
added airport to vector
added tn to vector
added tv to vector
added series to vector
added title to vector
added aka to vector
added time to vector
added slot to vector
added time to vector
added slot to vector
added id to vector
added postseason to vector
added ties to vector
added player to vector
added award to vector
added tie to vector
added third-rate to vector
added thing to vector
thesisin not recognized; using random instead.
added theater to vector
added territory to vector
added ten to vector
template_type_code not recognized; using random instead.
template_id not recognized; using id instead.
added teh to vector
added repair to vector
added assignment to vector
added technician to vector
added id to vector
firstbaron not recognized; using random instead.
team_name not recognized; using random instead.
added team to vector
added attributes to vector
added team to vector
added fifa to vector
added api to vector
added id to vector
added team to vector
added attributes to vector
added team to vector
added api to vector
added id to vector
added tarring to vector
added target to vector
added tami to vector
added taiwan to vector
added t to vector
added switzerland to vector
added sw to vector
added susan to vector
added surnames to vector
added supports to vector
added supported to vector
added supply to vector
added supplies to vector
added suffer to vector
added such to vector
added submitted to vector
added submit to vector
submission_id not recognized; using id instead.
added subjects to vector
added courses to vector
added subject to vector
added id to vector
added studied to vector
added student to vector
added enrolment to vector
added courses to vector
added student to vector
added enrolment to vector
added id to vector
added students to vector
added student to vector
added details to vector
student_course_registrations not recognized; using random instead.
added transcript to vector
added contents to vector
added student to vector
added course to vector
added id to vector
student_answer_text not recognized; using random instead.
student_answer_id not recognized; using id instead.
stu_num not recognized; using random instead.
street_address not recognized; using random instead.
storm_id not recognized; using id instead.
store_email_address not recognized; using random instead.
added steve to vector
statusid not recognized; using id instead.
added view to vector
added unit to vector
added status to vector
added status to vector
added date to vector
status_code not recognized; using random instead.
statement_id not recognized; using id instead.
starting_year not recognized; using random instead.
added starred to vector
added hotels to vector
added star to vector
added rating to vector
added code to vector
added stanley to vector
added stages to vector
added staff to vector
added in to vector
added processes to vector
added staff to vector
added role to vector
added code to vector
added staff to vector
added staff to vector
added name to vector
added st to vector
sportabout not recognized; using random instead.
added sponsors to vector
added sponsor to vector
spokesman_id not recognized; using id instead.
added specialized to vector
added special to vector
added speaking to vector
added span to vector
added soy to vector
added song to vector
added song to vector
added name to vector
added something to vector
added works to vector
added soloists to vector
soloistroles not recognized; using random instead.
soloistname not recognized; using random instead.
soloistinstrument not recognized; using random instead.
added sold to vector
added small to vector
added sky to vector
added dogs to vector
added size to vector
added code to vector
added shopping to vector
added documents to vector
added shipping to vector
added agent to vector
added code to vector
added shipments to vector
added shipments to vector
added shipment to vector
added tracking to vector
added number to vector
shipment_items not recognized; using random instead.
added shipments to vector
added shipment to vector
added date to vector
shieber not recognized; using random instead.
added shea to vector
added she to vector
share_in_percent not recognized; using random instead.
added severed to vector
added serving to vector
added serves to vector
added serial to vector
added september to vector
added sent to vector
added send to vector
added semesters to vector
added semester to vector
added name to vector
selbig not recognized; using random instead.
added sections to vector
added section to vector
added name to vector
added seatings to vector
added phone to vector
added screen to vector
added mode to vector
added scored to vector
added schooner to vector
satisfactory_yn not recognized; using random instead.
added sarah to vector
added sandwich to vector
sales_billion not recognized; using random instead.
salaray not recognized; using random instead.
added rv to vector
added run to vector
added royal to vector
added delivery to vector
added route to vector
added locations to vector
added route to vector
added id to vector
added rosalind to vector
rooms' not recognized; using random instead.
added rooms to vector
added room to vector
added type to vector
added code to vector
added rogers to vector
added rodrick to vector
added roberto to vector
added robert to vector
added rob to vector
added road to vector
added right-footed to vector
added revisions to vector
added resulted to vector
restypeid not recognized; using id instead.
added restriction to vector
added restriction to vector
added code to vector
added restaurants to vector
added response to vector
added respective to vector
added resource to vector
added residences to vector
added requires to vector
added required to vector
representative_id not recognized; using id instead.
repair_id not recognized; using id instead.
added renting to vector
added rentals to vector
added rental to vector
added rental to vector
added id to vector
release_date not recognized; using random instead.
added student to vector
added tests to vector
added taken to vector
added registration to vector
added id to vector
added student to vector
added course to vector
added registrations to vector
added registration to vector
added date to vector
added references to vector
ref_property_types not recognized; using random instead.
ref_detention_type not recognized; using random instead.
ref_address_types not recognized; using random instead.
added readers to vector
added reach to vector
added batting to vector
added postseason to vector
added rbi to vector
ratingdate not recognized; using random instead.
added rarest to vector
rank_of_the_year not recognized; using random instead.
radio_id not recognized; using id instead.
added queen to vector
added quality to vector
added purposes to vector
pu_man not recognized; using random instead.
added provinces to vector
added reference to vector
added property to vector
added types to vector
added property to vector
added type to vector
added description to vector
added properties to vector
added property to vector
added name to vector
added other to vector
added property to vector
added features to vector
added property to vector
added feature to vector
added description to vector
added properties to vector
added property to vector
added address to vector
added projects to vector
added project to vector
added details to vector
added ref to vector
added progress to vector
added status to vector
added progress to vector
added status to vector
added code to vector
added programming to vector
profits_billion not recognized; using random instead.
added treatments to vector
added professional to vector
added id to vector
prof_num not recognized; using random instead.
added products to vector
added production to vector
added type to vector
added code to vector
production_code not recognized; using random instead.
added products to vector
added product to vector
added size to vector
added invoice to vector
added line to vector
added items to vector
added product to vector
added quantity to vector
added products to vector
added product to vector
added color to vector
mailshot not recognized; using random instead.
added campaigns to vector
added product to vector
added category to vector
added documents to vector
added processes to vector
added process to vector
added status to vector
added code to vector
added documents to vector
added processes to vector
added process to vector
added outcome to vector
added code to vector
added problem to vector
added status to vector
added codes to vector
added problem to vector
added status to vector
added code to vector
added problems to vector
added problem to vector
added id to vector
added problem to vector
added log to vector
added problem to vector
added category to vector
added code to vector
added prior to vector
added prince to vector
press_id not recognized; using id instead.
added prescriptions to vector
added customer to vector
added addresses to vector
added premise to vector
added id to vector
added prague to vector
added pp to vector
added pounds to vector
added player to vector
added attributes to vector
added potential to vector
added postseasons to vector
postalcode not recognized; using random instead.
added possessed to vector
added fielding to vector
added postseason to vector
added pos to vector
added port to vector
added poorly to vector
added politics to vector
added player to vector
added award to vector
added vote to vector
added points to vector
added won to vector
added player to vector
added award to vector
added vote to vector
added points to vector
added max to vector
added fielding to vector
added postseason to vector
added po to vector
pname not recognized; using random instead.
playlistid not recognized; using id instead.
added player to vector
added player to vector
added fifa to vector
added api to vector
added id to vector
added player to vector
added player to vector
added api to vector
added id to vector
platform_id not recognized; using id instead.
planned_delivery_date not recognized; using random instead.
added planned to vector
added planes to vector
added hangar to vector
added plane to vector
added name to vector
added placement to vector
added pixel to vector
added pilot to vector
added skills to vector
added pilot to vector
added name to vector
added pick to vector
added phrase to vector
phone_id not recognized; using id instead.
added phl to vector
added pg to vector
petid not recognized; using id instead.
added students to vector
added personal to vector
added name to vector
added people to vector
added addresses to vector
added person to vector
added id to vector
added permanently to vector
added permanent to vector
added performing to vector
performer_id not recognized; using id instead.
perfomed not recognized; using random instead.
added percents to vector
added peeters to vector
added fielding to vector
added postseason to vector
added pb to vector
added payment to vector
added payment to vector
added date to vector
added payam to vector
patients' not recognized; using random instead.
added patents to vector
added patent to vector
added passwords to vector
added pass to vector
added partition to vector
added park to vector
added park to vector
added id to vector
added panama to vector
added dogs to vector
added owner to vector
added id to vector
overall_ranking not recognized; using random instead.
added outside to vector
added output to vector
added ottilie to vector
added financial to vector
added transactions to vector
added other to vector
added transaction to vector
added details to vector
added properties to vector
added other to vector
added property to vector
added details to vector
other_product_service_details not recognized; using random instead.
other_order_details not recognized; using random instead.
added part to vector
added faults to vector
added other to vector
added fault to vector
added details to vector
added customers to vector
added other to vector
added customer to vector
added details to vector
added accounts to vector
added other to vector
added account to vector
added details to vector
original_air_date not recognized; using random instead.
added organizer to vector
added organisations to vector
added organisation to vector
added type to vector
added customer to vector
added orders to vector
added order to vector
added status to vector
added order to vector
added items to vector
added order to vector
added item to vector
added status to vector
added code to vector
orchestra_id not recognized; using id instead.
added option to vector
opening_year not recognized; using random instead.
open_date not recognized; using random instead.
ondersma not recognized; using random instead.
added oil to vector
added organization to vector
added oid to vector
added ohio to vector
official_native_language not recognized; using random instead.
added occupancy to vector
num_of_staff not recognized; using random instead.
num_of_shops not recognized; using random instead.
added writer to vector
added num to vector
added of to vector
added episodes to vector
num_of_audience not recognized; using random instead.
added november to vector
added northridge to vector
added nor to vector
added non to vector
added newest to vector
added neither to vector
added nationals to vector
added napa to vector
nabozny not recognized; using random instead.
musical_id not recognized; using id instead.
music_festival not recognized; using random instead.
added multiple to vector
added multiplayer to vector
added mtw to vector
added culture to vector
added company to vector
added movie to vector
added id to vector
added works to vector
added movement to vector
added motors to vector
added mortgages to vector
added morningside to vector
added month to vector
added month to vector
added number to vector
added monterey to vector
monahan@example not recognized; using random instead.
added monaco to vector
mk_man not recognized; using random instead.
added minored to vector
added student to vector
added minor to vector
added mininum to vector
added millisecond to vector
added midfielder to vector
added mid-field to vector
added meters to vector
added memory to vector
added memories to vector
added discount to vector
added membership to vector
added credit to vector
added staff to vector
added in to vector
added meetings to vector
added meeting to vector
added id to vector
added sculptures to vector
added medium to vector
mediatypeid not recognized; using id instead.
added medhurst to vector
added food to vector
added service to vector
added meal to vector
added code to vector
added meaghan to vector
match_id not recognized; using id instead.
added cmi to vector
added cross to vector
added references to vector
added master to vector
added customer to vector
added id to vector
added massively to vector
added maryland to vector
added mary to vector
added martins to vector
added martinez to vector
marrotte not recognized; using random instead.
added marina to vector
added marcelle to vector
manufacturer{value}s not recognized; using random instead.
manufacturer_id not recognized; using id instead.
added man to vector
added malta to vector
added mall to vector
added malaysia to vector
makeid not recognized; using id instead.
major_id not recognized; using id instead.
added assets to vector
added maintenance to vector
added contract to vector
added id to vector
added customers to vector
added id to vector
added madlock to vector
machine_id not recognized; using id instead.
lysanne not recognized; using random instead.
ludie not recognized; using random instead.
added louisville to vector
added transactions to vector
added lots to vector
added lot to vector
added id to vector
added lose to vector
added loria to vector
added logon to vector
added customers to vector
added login to vector
added password to vector
lockmanfurt not recognized; using random instead.
added loaned to vector
added lisa to vector
added addresses to vector
added line to vector
added 3 to vector
added lighter to vector
added liberal to vector
added levels to vector
added leonie to vector
added lengths to vector
added len to vector
added leia to vector
added leaders to vector
added station to vector
added latitude to vector
added large to vector
added driver to vector
added standings to vector
added race to vector
added id to vector
added landing to vector
added land to vector
added lake to vector
added lacey to vector
added labs to vector
added kohler to vector
klr209 not recognized; using random instead.
added kirk to vector
added khanewal to vector
keyphraseid not recognized; using id instead.
added keyboard to vector
added kenton to vector
added kennedy to vector
added keeling to vector
added kb to vector
added kawa to vector
added kabul to vector
added justice to vector
added junction to vector
julianaside not recognized; using random instead.
journalist_id not recognized; using id instead.
journalid not recognized; using id instead.
journal_id not recognized; using id instead.
journal_committee not recognized; using random instead.
added jolie to vector
added johnson to vector
added staff to vector
added job to vector
added title to vector
added jerry to vector
added jerome to vector
added jerk to vector
added jeans to vector
added japanese to vector
added japan to vector
added january to vector
added jackson to vector
is_online not recognized; using random instead.
is_male not recognized; using random instead.
invoiceid not recognized; using id instead.
added invoices to vector
added invoice to vector
added details to vector
added rental to vector
added inventory to vector
added id to vector
added into to vector
added insurance to vector
instructors' not recognized; using random instead.
instid not recognized; using id instead.
added installed to vector
added installation to vector
added manager to vector
added half to vector
inseason not recognized; using random instead.
added fielding to vector
added postseason to vector
added inn to vector
added outs to vector
added injures to vector
teambans not recognized; using random instead.
matchid not recognized; using id instead.
added inhibitor to vector
added indonesia to vector
added organization to vector
added contact to vector
added individuals to vector
added individual to vector
added id to vector
added index to vector
added independence to vector
incorporated_in not recognized; using random instead.
added behavior to vector
added incident to vector
added incident to vector
added type to vector
added code to vector
added students to vector
added in to vector
added detention to vector
added incident to vector
added id to vector
added document to vector
added sections to vector
added images to vector
added image to vector
added id to vector
idorder not recognized; using random instead.
idclient not recognized; using random instead.
added iceland to vector
added huels to vector
host_id not recognized; using id instead.
added hornet to vector
added horizontal to vector
added hom to vector
added holding to vector
added hispanic to vector
added hires to vector
added employees to vector
added hire to vector
added date to vector
added hiram to vector
added hill to vector
hh_id not recognized; using id instead.
added helps to vector
added help to vector
added helena to vector
heffington not recognized; using random instead.
added heathrow to vector
added heaney to vector
added health to vector
added headquartered to vector
headphone_id not recognized; using id instead.
added headers to vector
head_id not recognized; using id instead.
added hbs to vector
added hawaii to vector
added happen to vector
added team to vector
added half to vector
added half to vector
added haiti to vector
added gust to vector
added apartment to vector
added bookings to vector
added guest to vector
added id to vector
added guest to vector
group_equity_shareholding not recognized; using random instead.
added group to vector
added greenland to vector
graztevski not recognized; using random instead.
added grants to vector
added grant to vector
added id to vector
gradepoints not recognized; using random instead.
added grade to vector
added conversion to vector
added grade to vector
added point to vector
added goroka to vector
added gordon to vector
added goodrich to vector
gleasonmouth not recognized; using random instead.
added girl to vector
added classification to vector
added gid to vector
added pitching to vector
added postseason to vector
added gf to vector
added gets to vector
added german to vector
added geometry to vector
genreid not recognized; using id instead.
added general to vector
gender_mfu not recognized; using random instead.
added customers to vector
added gender to vector
added guests to vector
added gender to vector
added code to vector
added gell to vector
added gelderland to vector
added gave to vector
added gatwick to vector
furniture_id not recognized; using id instead.
added document to vector
added functional to vector
added areas to vector
added functional to vector
added area to vector
added code to vector
added functional to vector
added flight to vector
added from to vector
added airport to vector
added frequently-used to vector
added franchises to vector
added team to vector
added franchise to vector
added franchise to vector
added id to vector
frami not recognized; using random instead.
added fourth-grade to vector
added founding to vector
added founders to vector
added fosse to vector
added forward to vector
added party to vector
added forms to vector
added form to vector
added id to vector
fnol_id not recognized; using id instead.
added fleet to vector
added fix to vector
firstdragon not recognized; using random instead.
added first-grade to vector
added finishes to vector
financial_transactions not recognized; using random instead.
finance_id not recognized; using id instead.
festival_id not recognized; using id instead.
feliciaberg not recognized; using random instead.
added feest to vector
added february to vector
added other to vector
added available to vector
added features to vector
added feature to vector
added type to vector
added code to vector
added other to vector
added available to vector
added features to vector
added feature to vector
added name to vector
added other to vector
added available to vector
added features to vector
added feature to vector
added description to vector
added fault to vector
added log to vector
added parts to vector
added fault to vector
added status to vector
added part to vector
added faults to vector
added fault to vector
added description to vector
added fate to vector
farm_id not recognized; using id instead.
added flight to vector
added fare to vector
added fare to vector
added id to vector
added fare to vector
added basis to vector
added fare to vector
added basis to vector
added code to vector
added students to vector
added family to vector
added name to vector
added failure to vector
added song to vector
added song to vector
added id to vector
added exxonmobil to vector
added experiences to vector
added expected to vector
added expectancies to vector
exhibition_id not recognized; using id instead.
added exercise to vector
added excluding to vector
eventtype not recognized; using random instead.
added evelina to vector
added european to vector
added euro to vector
added estimates to vector
added estimate to vector
essn not recognized; using random instead.
equipment_id not recognized; using id instead.
added episodes to vector
added engineering to vector
added time to vector
added interval to vector
added end to vector
added time to vector
added en to vector
added empty to vector
added employment to vector
added else to vector
election_id not recognized; using id instead.
added certificate to vector
added employee to vector
added id to vector
effective_date not recognized; using random instead.
added effective to vector
added effect to vector
added countries to vector
added education to vector
added score to vector
added eduardo to vector
added edmonton to vector
editor_id not recognized; using id instead.
added earns to vector
added dublin to vector
added flight to vector
added dual to vector
added carrier to vector
added player to vector
added attributes to vector
added dribbling to vector
added dre to vector
added dorian to vector
added domain to vector
added treatments to vector
added dog to vector
added id to vector
document_type_name not recognized; using random instead.
document_subset_id not recognized; using id instead.
added documents to vector
added document to vector
added structure to vector
added code to vector
added documents to vector
added document to vector
added status to vector
added code to vector
added dock to vector
dnumber not recognized; using random instead.
dname not recognized; using random instead.
added team to vector
added half to vector
added div to vector
added win to vector
added team to vector
added half to vector
added div to vector
added id to vector
added dinning to vector
added differential to vector
added died to vector
device_id not recognized; using id instead.
added detentions to vector
added reference to vector
added detention to vector
added type to vector
added detention to vector
added type to vector
added description to vector
added detention to vector
added detention to vector
added summary to vector
added destroy to vector
added designed to vector
added deputy to vector
added departments to vector
added department to vector
added store to vector
added id to vector
added department to vector
added stores to vector
added department to vector
added store to vector
added chain to vector
added id to vector
added dependent to vector
added flight to vector
added stop to vector
added departure to vector
added time to vector
added student to vector
added enrolment to vector
added degree to vector
added program to vector
added id to vector
added definition to vector
added defenders to vector
added decisions to vector
added december to vector
added debit to vector
debate_id not recognized; using id instead.
added debate to vector
added date to vector
added day to vector
added day to vector
added number to vector
added days to vector
added day to vector
added name to vector
added david to vector
added detention to vector
datetime not recognized; using random instead.
added detention to vector
added start to vector
added detention to vector
added detention to vector
added end to vector
date_opened not recognized; using random instead.
added properties to vector
added date to vector
added on to vector
added market to vector
date_claim_settled not recognized; using random instead.
date_claim_made not recognized; using random instead.
datasetid not recognized; using id instead.
added daniel to vector
added damien to vector
damianfort not recognized; using random instead.
added dameon to vector
added daily to vector
added cvo to vector
added cutter to vector
customers_and_services_id not recognized; using id instead.
added customers to vector
added customer to vector
added number to vector
added customers to vector
added customer to vector
added middle to vector
added initial to vector
customer_interaction_id not recognized; using id instead.
customer_event_id not recognized; using id instead.
cust_id not recognized; using id instead.
added current to vector
added currency to vector
added cuba to vector
crs_code not recognized; using random instead.
added player to vector
added attributes to vector
added crossing to vector
added crew to vector
added crescent to vector
added creative to vector
covin not recognized; using random instead.
added student to vector
added course to vector
added registrations to vector
added course to vector
added schedule to vector
added id to vector
added coupons to vector
added customers to vector
added coupon to vector
added id to vector
countrycode not recognized; using random instead.
added players to vector
added country to vector
added code to vector
countries' not recognized; using random instead.
added costing to vector
corresonding not recognized; using random instead.
added circulation to vector
added history to vector
added copy to vector
added number to vector
added contracts to vector
added votes to vector
added contestant to vector
added number to vector
added construction to vector
added conrad to vector
added connection to vector
conference_id not recognized; using id instead.
conductorname not recognized; using random instead.
conductor_id not recognized; using id instead.
concert_id not recognized; using id instead.
added computing to vector
added comptrollers to vector
composername not recognized; using random instead.
added component to vector
added completed to vector
added food to vector
added service to vector
added compartment to vector
added commanded to vector
added coming to vector
added combinations to vector
added columbus to vector
added products to vector
added color to vector
added code to vector
collection_subset_id not recognized; using id instead.
added collectible to vector
coach_id not recognized; using id instead.
clubid not recognized; using id instead.
club_name not recognized; using random instead.
added closure to vector
added close to vector
added clemson to vector
added clean to vector
added classified to vector
added fare to vector
added basis to vector
added class to vector
added type to vector
class_code not recognized; using random instead.
claim_stage_id not recognized; using id instead.
city_channel_id not recognized; using id instead.
added cite to vector
circuitid not recognized; using id instead.
cinema_id not recognized; using id instead.
church_id not recognized; using id instead.
added chrissy to vector
added chris to vector
added chosen to vector
added choose to vector
added phone to vector
added chip to vector
added model to vector
added chico to vector
added chennai to vector
added chelsea to vector
added checkin to vector
added charles to vector
added characteristics to vector
added characteristic to vector
added type to vector
added code to vector
added product to vector
added characteristics to vector
added characteristic to vector
added id to vector
added character to vector
added char to vector
added chang to vector
added chandler to vector
added champlin to vector
added participants to vector
added position to vector
added chains to vector
added chain to vector
added census to vector
added professionals to vector
added cell to vector
added number to vector
added film to vector
added category to vector
added category to vector
added id to vector
added catalog to vector
added structure to vector
added catalog to vector
added id to vector
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added catalog to vector
added entry to vector
added id to vector
added cash to vector
added cases to vector
added carolina to vector
added carol to vector
added customers to vector
added cards to vector
added card to vector
added number to vector
added financial to vector
added transactions to vector
added card to vector
added id to vector
added customer to vector
added card to vector
added credit to vector
added cannot to vector
added canceled to vector
added canadian to vector
added calgary to vector
added ca to vector
added tracks to vector
added bytes to vector
added busiest to vector
added buses to vector
added burns to vector
added mill to vector
added built to vector
added year to vector
budget_type_code not recognized; using random instead.
added brown to vector
added bronze to vector
added bromley to vector
added broadcasted to vector
added dogs to vector
added breed to vector
added code to vector
added brands to vector
added brander to vector
added apartment to vector
added bookings to vector
added booking to vector
added status to vector
added code to vector
added apartment to vector
added bookings to vector
added booking to vector
added start to vector
added date to vector
added apartment to vector
added bookings to vector
added booking to vector
added end to vector
added date to vector
added fare to vector
added basis to vector
added booking to vector
added class to vector
added culture to vector
added company to vector
added book to vector
added club to vector
added id to vector
added blues to vector
added blanche to vector
added billy to vector
added billing to vector
added trip to vector
added bike to vector
added id to vector
biale not recognized; using random instead.
added pitching to vector
added postseason to vector
added bfp to vector
added berge to vector
benifit not recognized; using random instead.
added belgium to vector
added become to vector
added beatrix to vector
bdate not recognized; using random instead.
added player to vector
added bats to vector
added baseball to vector
added barton to vector
added barker to vector
added bar to vector
added pitching to vector
added postseason to vector
baopp not recognized; using random instead.
added bands to vector
added bandmate to vector
added bal to vector
added bachelors to vector
added bachelor to vector
added az to vector
added avatar to vector
added view to vector
added unit to vector
added status to vector
added available to vector
added yes to vector
added or to vector
added no to vector
added auto to vector
added authorized to vector
added authorization to vector
authorid not recognized; using id instead.
added documents to vector
added author to vector
added name to vector
authid not recognized; using id instead.
added augustana to vector
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added attribute to vector
added id to vector
attraction_type_code not recognized; using random instead.
added attends to vector
added attained to vector
added ato to vector
added assistant to vector
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added status to vector
added code to vector
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added start to vector
added date to vector
added assignment to vector
assets_billion not recognized; using random instead.
added aspect to vector
artwork_id not recognized; using id instead.
added artwork to vector
added flight to vector
added stop to vector
added arrival to vector
added time to vector
added arrears to vector
added armed to vector
added arizona to vector
added argentina to vector
added mill to vector
added architect to vector
added id to vector
added architect to vector
added view to vector
added unit to vector
added status to vector
added apartment to vector
added booking to vector
added id to vector
added april to vector
added approved to vector
added applications to vector
added anonymous to vector
added annual to vector
aniyah not recognized; using random instead.
added anguilla to vector
added angola to vector
added andy to vector
amount_settled not recognized; using random instead.
added customer to vector
added payments to vector
added amount to vector
added payment to vector
added customers to vector
added amount to vector
added outstanding to vector
added americano to vector
added america to vector
added has to vector
added amenity to vector
added amenity to vector
added id to vector
added amc to vector
added alyson to vector
added alton to vector
added almeida to vector
added allow to vector
added alison to vector
added airlines to vector
added airline to vector
added id to vector
added albania to vector
added alaska to vector
airportcode not recognized; using random instead.
added airport to vector
added airport to vector
added name to vector
added flight to vector
added airline to vector
added code to vector
added flight to vector
added aircraft to vector
added code to vector
added sequence to vector
added equipment to vector
added sequence to vector
added aircraft to vector
added code to vector
agent_id not recognized; using id instead.
added users to vector
added age to vector
added category to vector
added code to vector
added affiliations to vector
added author to vector
added list to vector
added affiliation to vector
added id to vector
added affect to vector
added aerosmith to vector
added advising to vector
added reference to vector
added address to vector
added types to vector
added address to vector
added type to vector
added description to vector
added customer to vector
added addresses to vector
added address to vector
added type to vector
added actually to vector
actual_delivery_date not recognized; using random instead.
added actress to vector
added achievements to vector
added achievement to vector
added type to vector
added code to vector
added accounts to vector
added account to vector
added name to vector
added able to vector
added abilene to vector
added abbreviations to vector
added abandoned to vector
added a4 to vector
added 99 to vector
added 95 to vector
added 94103 to vector
94002 not recognized; using random instead.
93000 not recognized; using random instead.
added 93 to vector
900000 not recognized; using random instead.
90000 not recognized; using random instead.
added 9000 to vector
8741 not recognized; using random instead.
8000000 not recognized; using random instead.
added 7th to vector
added 720 to vector
70174 not recognized; using random instead.
added 70000 to vector
added 7000 to vector
added 660 to vector
added 636 to vector
added 5600 to vector
added 5200 to vector
added 520 to vector
4500000 not recognized; using random instead.
added 45 to vector
42000 not recognized; using random instead.
added 400 to vector
added 37 to vector
3452 not recognized; using random instead.
added 338 to vector
added 337 to vector
added 33 to vector
added 31 to vector
added 268 to vector
added 255 to vector
added 254 to vector
added 2500 to vector
242518965 not recognized; using random instead.
added 220 to vector
added 22 to vector
2192 not recognized; using random instead.
2018-03-17 not recognized; using random instead.
added 2018 to vector
2010-01-01 not recognized; using random instead.
2009-01-01 not recognized; using random instead.
2000-01-01 not recognized; using random instead.
added 1987 to vector
added 1985 to vector
1976-01-01 not recognized; using random instead.
1975-01-01 not recognized; using random instead.
added 1974 to vector
added 1970 to vector
added 1960 to vector
added 1959 to vector
added 1958 to vector
added 1956 to vector
added 1955 to vector
added 1949 to vector
added 1948 to vector
added 1945 to vector
added 1939 to vector
added 1935 to vector
added 1930 to vector
added 1928 to vector
added 1907 to vector
added 1900 to vector
added 190 to vector
added 1890 to vector
added 1885 to vector
added 1880 to vector
180cm not recognized; using random instead.
added 1800 to vector
added 180 to vector
added 171 to vector
added 163 to vector
added 162 to vector
160000 not recognized; using random instead.
added 160 to vector
140000 not recognized; using random instead.
added 14 to vector
12:00:00 not recognized; using random instead.
added 121 to vector
120000 not recognized; using random instead.
added 112 to vector
added 110 to vector
added 109 to vector
added 108 to vector
added 107 to vector
added 105 to vector
added 103 to vector
added 102 to vector
added 1004 to vector
added 10018 to vector
1000000 not recognized; using random instead.
09:00:00 not recognized; using random instead.
09700166582 not recognized; using random instead.
08/30/2015 not recognized; using random instead.
07:13:53 not recognized; using random instead.
added -50 to vector
$180 not recognized; using random instead.
$160 not recognized; using random instead.
“mike” not recognized; using random instead.
“dar” not recognized; using random instead.
’t’ not recognized; using random instead.
‘swift’ not recognized; using random instead.
‘superstar’ not recognized; using random instead.
‘ee’ not recognized; using random instead.
‘edu’ not recognized; using random instead.
‘donor’ not recognized; using random instead.
‘brown’ not recognized; using random instead.
‘a’ not recognized; using random instead.
{value}-field not recognized; using random instead.
added fielding to vector
added zr to vector
years_played not recognized; using random instead.
years_opened not recognized; using random instead.
years_of_experiencing not recognized; using random instead.
years_as_tallest not recognized; using random instead.
added yearly to vector
added hall to vector
added of to vector
added fame to vector
yearid not recognized; using id instead.
yeared not recognized; using random instead.
year_working not recognized; using random instead.
year_profits_billion not recognized; using random instead.
year_opened not recognized; using random instead.
year_of_work not recognized; using random instead.
year_of_founded not recognized; using random instead.
year_entered_competition not recognized; using random instead.
year_awarded not recognized; using random instead.
ycard not recognized; using random instead.
wthat not recognized; using random instead.
added team to vector
added ws to vector
added win to vector
would_take_again not recognized; using random instead.
added worst to vector
works_on not recognized; using random instead.
added program to vector
added course to vector
added workload to vector
working_year_starts not recognized; using random instead.
working_horses not recognized; using random instead.
work: not recognized; using random instead.
added woodroffe to vector
wmodel not recognized; using random instead.
wins_count not recognized; using random instead.
added winnings to vector
winning_team not recognized; using random instead.
winning_pilot not recognized; using random instead.
winning_driver not recognized; using random instead.
winning_aircraft not recognized; using random instead.
added matches to vector
added winner to vector
added seed to vector
added matches to vector
added winner to vector
added rank to vector
added points to vector
added matches to vector
added winner to vector
added rank to vector
added matches to vector
added winner to vector
added name to vector
added matches to vector
added winner to vector
added ioc to vector
added matches to vector
added winner to vector
added id to vector
added matches to vector
added winner to vector
added ht to vector
added matches to vector
added winner to vector
added hand to vector
added matches to vector
added winner to vector
added entry to vector
added matches to vector
added winner to vector
added age to vector
added aircraft to vector
added wing to vector
added span to vector
added weekly to vector
added weather to vector
added wind to vector
added speed to vector
added mph to vector
added weather to vector
added wind to vector
added dir to vector
added degrees to vector
added paintings to vector
added width to vector
added mm to vector
added catalog to vector
added contents to vector
added width to vector
added widely to vector
added aircraft to vector
added wide to vector
added body to vector
wholoves not recognized; using random instead.
wholikes not recognized; using random instead.
whoisloved not recognized; using random instead.
whoisliked not recognized; using random instead.
whic`h not recognized; using random instead.
added whcih to vector
added whare to vector
whah not recognized; using random instead.
added well-paid to vector
added weighted to vector
weeks_on_top not recognized; using random instead.
weekly_weather not recognized; using random instead.
added wednesday to vector
added websites to vector
added website to vector
web_client_accelerator not recognized; using random instead.
added weather to vector
added team to vector
added wc to vector
added win to vector
added wat to vector
added warehouses to vector
added warehouse to vector
wardsplaced not recognized; using random instead.
wardskilled not recognized; using random instead.
wardsbought not recognized; using random instead.
added customer to vector
added voucher to vector
added credit to vector
voting_record not recognized; using random instead.
added hall to vector
added of to vector
added fame to vector
votedby not recognized; using random instead.
vote_percent not recognized; using random instead.
added votes to vector
added vote to vector
added id to vector
added volvos to vector
voluptatem not recognized; using random instead.
volume_issue not recognized; using random instead.
volume_id not recognized; using id instead.
added player to vector
added attributes to vector
added volleys to vector
added performance to vector
added score to vector
added voice to vector
added sound to vector
added quality to vector
added vocables to vector
visits_restaurant not recognized; using random instead.
visitor_id not recognized; using id instead.
added visiting to vector
added engineer to vector
added visits to vector
added visit to vector
added start to vector
visit_id not recognized; using id instead.
added engineer to vector
added visits to vector
added visit to vector
added end to vector
visit_details not recognized; using random instead.
visit_date not recognized; using random instead.
visionscore not recognized; using random instead.
added player to vector
added attributes to vector
added vision to vector
visiblity not recognized; using random instead.
viewers_m not recognized; using random instead.
view_unit_status not recognized; using random instead.
view_product_availability not recognized; using random instead.
added vietti to vector
video_games not recognized; using random instead.
added victories to vector
vice_president_vote not recognized; using random instead.
vice_manager_name not recognized; using random instead.
added versions to vector
version_number not recognized; using random instead.
venuename not recognized; using random instead.
added renting to vector
added history to vector
added vehicles to vector
added id to vector
vehicle_flight_number not recognized; using random instead.
vehicle_driver not recognized; using random instead.
added vehicles to vector
added vehicle to vector
added details to vector
vault_points not recognized; using random instead.
added product to vector
added categories to vector
added vat to vector
added rating to vector
added machine to vector
added value to vector
added points to vector
valid_answers not recognized; using random instead.
valid_answer_text not recognized; using random instead.
added usernames to vector
user_searches not recognized; using random instead.
user_property_history not recognized; using random instead.
user_profiles not recognized; using random instead.
added users to vector
added user to vector
added name to vector
added users to vector
added user to vector
added login to vector
added ref to vector
added user to vector
added categories to vector
added user to vector
added category to vector
added description to vector
added users to vector
added user to vector
added address to vector
added id to vector
added screen to vector
added mode to vector
added used to vector
added kb to vector
added usage to vector
us_senate not recognized; using random instead.
added ups to vector
unversities not recognized; using random instead.
unsure_rate not recognized; using random instead.
university_name not recognized; using random instead.
units_sold_millions not recognized; using random instead.
added reference to vector
added product to vector
added categories to vector
added unit to vector
added of to vector
added measure to vector
uniqname not recognized; using random instead.
added uniformed to vector
added undergraduate to vector
added undergoes to vector
added uncertain to vector
added unavailable to vector
added unaffected to vector
added organizations to vector
added uk to vector
added vat to vector
added number to vector
added typically to vector
added products to vector
added typical to vector
added selling to vector
added price to vector
added products to vector
added typical to vector
added buying to vector
added price to vector
type_of_thing_code not recognized; using random instead.
type_of_restaurant not recognized; using random instead.
type_of_question_code not recognized; using random instead.
type_of_powertrain not recognized; using random instead.
added tv to vector
added show to vector
added tv to vector
added show to vector
added name to vector
tv_show not recognized; using random instead.
tv_channel not recognized; using random instead.
turretkills not recognized; using random instead.
added tune to vector
added tuesday to vector
added try to vector
truedmgtochamp not recognized; using random instead.
truedmgtaken not recognized; using random instead.
truedmgdealt not recognized; using random instead.
added trucks to vector
added truck to vector
added licence to vector
added number to vector
added trucks to vector
added truck to vector
added details to vector
added triple to vector
added kills to vector
added trinket to vector
added tries to vector
added treats to vector
treatment_types not recognized; using random instead.
added treatment to vector
added types to vector
added treatment to vector
added type to vector
added description to vector
added treatments to vector
added treatment to vector
added id to vector
treasurer_vote not recognized; using random instead.
added river to vector
added traverse to vector
added ground to vector
added service to vector
added transport to vector
added type to vector
added transmitter to vector
transit_passengers not recognized; using random instead.
added student to vector
added record to vector
added transfer to vector
added source to vector
added transcripts to vector
added transcript to vector
added details to vector
added transcripts to vector
added transcript to vector
added date to vector
transcript_contents not recognized; using random instead.
transactions_lots not recognized; using random instead.
added reference to vector
added transaction to vector
added types to vector
added transaction to vector
added type to vector
added description to vector
trained_in not recognized; using random instead.
train_station not recognized; using random instead.
added train to vector
added train to vector
added number to vector
train_num not recognized; using random instead.
trade_name not recognized; using random instead.
tracklists not recognized; using random instead.
added fielding to vector
added postseason to vector
added tp to vector
added towns to vector
harrykills not recognized; using random instead.
added matches to vector
added tourney to vector
added name to vector
added matches to vector
added tourney to vector
added level to vector
added matches to vector
added tourney to vector
added id to vector
added matches to vector
added tourney to vector
added date to vector
added tournaments to vector
tourist_details not recognized; using random instead.
tourist_attractions not recognized; using random instead.
tourist_attraction_features not recognized; using random instead.
tough_tests not recognized; using random instead.
tough_grader not recognized; using random instead.
totunitshealed not recognized; using random instead.
totminionskilled not recognized; using random instead.
totheal not recognized; using random instead.
totdmgtochamp not recognized; using random instead.
totdmgtaken not recognized; using random instead.
totdmgdealt not recognized; using random instead.
totcctimedealt not recognized; using random instead.
totalenrollment_ay not recognized; using random instead.
total_wl not recognized; using random instead.
added product to vector
added suppliers to vector
added total to vector
added value to vector
added purchased to vector
total_spent not recognized; using random instead.
total_production not recognized; using random instead.
total_pounds not recognized; using random instead.
total_points not recognized; using random instead.
added renting to vector
added history to vector
added total to vector
added hours to vector
total_horses not recognized; using random instead.
added student to vector
added total to vector
added gpa to vector
total_duration_min not recognized; using random instead.
total_disk_area not recognized; using random instead.
total_cattle not recognized; using random instead.
added budget to vector
added total to vector
added budget to vector
added percent to vector
added invested to vector
added budget to vector
added total to vector
added budget to vector
added percent to vector
added budgeted to vector
total_attendance not recognized; using random instead.
added product to vector
added suppliers to vector
added total to vector
added amount to vector
added purchased to vector
total_amount not recognized; using random instead.
added student to vector
added total to vector
added credits to vector
added topic to vector
top_speed not recognized; using random instead.
took_office not recognized; using random instead.
added toll to vector
added tip to vector
added tip to vector
added id to vector
added tip to vector
added timothy to vector
timmothy not recognized; using random instead.
timed_status_of_things not recognized; using random instead.
timed_locations_of_things not recognized; using random instead.
timecc not recognized; using random instead.
added time to vector
added zone to vector
added time to vector
added zone to vector
added name to vector
time_zone not recognized; using random instead.
time_slot not recognized; using random instead.
time_of_purchase not recognized; using random instead.
time_of_day not recognized; using random instead.
time_interval not recognized; using random instead.
added flight to vector
added time to vector
added elapsed to vector
added tied to vector
ticket_price not recognized; using random instead.
added thursday to vector
added player to vector
added throws to vector
third_party_companies not recognized; using random instead.
theor not recognized; using random instead.
theme_parks not recognized; using random instead.
theme_park_id not recognized; using id instead.
theme_park_details not recognized; using random instead.
added textbook to vector
added assessment to vector
added notes to vector
added text to vector
added of to vector
added notes to vector
added student to vector
added tests to vector
added taken to vector
added test to vector
added result to vector
added student to vector
added record to vector
added test to vector
added id to vector
added management to vector
added temporary to vector
added acting to vector
added temporary to vector
added temporarily to vector
template_type_description not recognized; using random instead.
template_details not recognized; using random instead.
added team to vector
added team to vector
added short to vector
added name to vector
added team to vector
added team to vector
added long to vector
added name to vector
team_leader not recognized; using random instead.
added postseason to vector
added team to vector
added id to vector
added winner to vector
added team to vector
added team to vector
added id to vector
added retro to vector
added postseason to vector
added team to vector
added id to vector
added loser to vector
added team to vector
added team to vector
added id to vector
lahman45 not recognized; using random instead.
added team to vector
added team to vector
added id to vector
added br to vector
team_half not recognized; using random instead.
team_franchise not recognized; using random instead.
team_driver not recognized; using random instead.
team_attributes not recognized; using random instead.
added teachers to vector
added teacher to vector
added details to vector
added tasks to vector
added task to vector
added id to vector
added tasks to vector
added task to vector
added details to vector
added trust to vector
added target to vector
added user to vector
added id to vector
added takes to vector
added tahn to vector
added tags to vector
systmes not recognized; using random instead.
swimmer_id not recognized; using id instead.
sweaz not recognized; using random instead.
surfacearea not recognized; using random instead.
supportrepid not recognized; using id instead.
added supporting to vector
added customers to vector
added support to vector
added rep to vector
added id to vector
support_rate not recognized; using random instead.
added suppliers to vector
added supplier to vector
added phone to vector
added suppliers to vector
added supplier to vector
added name to vector
added assets to vector
added supplier to vector
added company to vector
added id to vector
supplier_addresses not recognized; using random instead.
super_ssn not recognized; using random instead.
added sunday to vector
added summed to vector
added summaries to vector
added suffered to vector
added successful to vector
added succeeded to vector
added trip to vector
added subscription to vector
added type to vector
added subjects to vector
added subject to vector
added name to vector
subject_code not recognized; using random instead.
sub_tittle not recognized; using random instead.
added style to vector
student{value}s not recognized; using random instead.
students_in_detention not recognized; using random instead.
students_addresses not recognized; using random instead.
student_tests_taken not recognized; using random instead.
student_record not recognized; using random instead.
student_loans not recognized; using random instead.
added student to vector
added loans to vector
added student to vector
added loan to vector
added id to vector
student_events not recognized; using random instead.
student_enrolment_courses not recognized; using random instead.
student_enrolment not recognized; using random instead.
student_course_enrolment not recognized; using random instead.
student_course_attendance not recognized; using random instead.
student_course_assignments not recognized; using random instead.
added dorm to vector
added student to vector
added capacity to vector
student_assessments not recognized; using random instead.
student_answers not recognized; using random instead.
student_addresses not recognized; using random instead.
added students to vector
added addresses to vector
added student to vector
added address to vector
added id to vector
student_address not recognized; using random instead.
stu_transfer not recognized; using random instead.
stu_phone not recognized; using random instead.
stu_lname not recognized; using random instead.
stu_init not recognized; using random instead.
stu_hrs not recognized; using random instead.
stu_gpa not recognized; using random instead.
stu_fname not recognized; using random instead.
stu_dob not recognized; using random instead.
stu_class not recognized; using random instead.
added structures to vector
added player to vector
added attributes to vector
added strength to vector
street_name not recognized; using random instead.
street_markets not recognized; using random instead.
added stream to vector
store_product not recognized; using random instead.
added department to vector
added stores to vector
added store to vector
added email to vector
store_district not recognized; using random instead.
added department to vector
added stores to vector
added store to vector
added address to vector
added restriction to vector
added stopovers to vector
added flight to vector
added stop to vector
added stop to vector
added time to vector
added flight to vector
added stop to vector
added stop to vector
added number to vector
added flight to vector
added stop to vector
added stop to vector
added days to vector
added flight to vector
added stop to vector
added stop to vector
added airport to vector
staystart not recognized; using random instead.
added stays to vector
stayid not recognized; using id instead.
stayend not recognized; using random instead.
status_of_thing_code not recognized; using random instead.
station_name not recognized; using random instead.
station_company not recognized; using random instead.
statement_details not recognized; using random instead.
added stated to vector
state_province not recognized; using random instead.
added customers to vector
added state to vector
added county to vector
added province to vector
state_county not recognized; using random instead.
added all to vector
added star to vector
added starting to vector
added pos to vector
start_time not recognized; using random instead.
added trip to vector
added start to vector
added station to vector
added name to vector
added trip to vector
added start to vector
added station to vector
added id to vector
added time to vector
added slot to vector
added start to vector
added minute to vector
added time to vector
added slot to vector
added start to vector
added hour to vector
start_from not recognized; using random instead.
added meetings to vector
added start to vector
added date to vector
added time to vector
staring_date not recognized; using random instead.
added ref to vector
added hotel to vector
added star to vector
added ratings to vector
added star to vector
added rating to vector
added description to vector
added player to vector
added attributes to vector
added standing to vector
added tackle to vector
added player to vector
added attributes to vector
added stamina to vector
stageposition not recognized; using random instead.
added performance to vector
added score to vector
added stage to vector
added presence to vector
staff_roles not recognized; using random instead.
added reference to vector
added staff to vector
added roles to vector
added staff to vector
added role to vector
added description to vector
added staff to vector
added staff to vector
added last to vector
added name to vector
staff_in_processes not recognized; using random instead.
staff_in_meetings not recognized; using random instead.
added staff to vector
added staff to vector
added gender to vector
added staff to vector
added staff to vector
added first to vector
added name to vector
staff_department_assignments not recognized; using random instead.
added staff to vector
added staff to vector
added address to vector
added id to vector
teamid not recognized; using id instead.
matchid not recognized; using id instead.
added routes to vector
added source to vector
added airport to vector
added id to vector
added routes to vector
added source to vector
added airport to vector
added flights to vector
added source to vector
added player to vector
added attributes to vector
added sprint to vector
added speed to vector
added spouse to vector
added sporty to vector
sportsinfo not recognized; using random instead.
sportname not recognized; using random instead.
sponsor_name not recognized; using random instead.
spokesman_district not recognized; using random instead.
speed_knots not recognized; using random instead.
added speech to vector
added specifically to vector
added film to vector
added special to vector
added features to vector
speach_title not recognized; using random instead.
added spans to vector
added home to vector
added game to vector
added span to vector
added last to vector
added home to vector
added game to vector
added span to vector
added first to vector
added sources to vector
sourceairport not recognized; using random instead.
added trust to vector
added source to vector
added user to vector
added id to vector
added cmi to vector
added cross to vector
added references to vector
added source to vector
added system to vector
added code to vector
added performance to vector
added score to vector
added songs to vector
added id to vector
song_release_year not recognized; using random instead.
song_id not recognized; using id instead.
software_platform not recognized; using random instead.
added team to vector
added soa to vector
sname not recognized; using random instead.
added smoking to vector
small_silver not recognized; using random instead.
added slots to vector
added player to vector
added attributes to vector
added sliding to vector
added tackle to vector
sleephabits not recognized; using random instead.
skills_required_to_fix not recognized; using random instead.
added skills to vector
added skill to vector
added description to vector
added skills to vector
added skill to vector
added code to vector
added sizes to vector
added size to vector
added description to vector
added situations to vector
added sit to vector
singles_wl not recognized; using random instead.
singer_in_concert not recognized; using random instead.
added sing to vector
added simon to vector
added clients to vector
added sic to vector
added code to vector
added si to vector
added properties to vector
added shp to vector
added feature to vector
added 3 to vector
added properties to vector
added shp to vector
added feature to vector
added 2 to vector
added properties to vector
added shp to vector
added feature to vector
added 1 to vector
added showing to vector
show_times_per_day not recognized; using random instead.
show_id not recognized; using id instead.
added player to vector
added attributes to vector
added shot to vector
added power to vector
added player to vector
added attributes to vector
added short to vector
added passing to vector
shops' not recognized; using random instead.
shop_name not recognized; using random instead.
shop_details not recognized; using random instead.
added customer to vector
added orders to vector
added shipping to vector
added method to vector
added code to vector
added reference to vector
added shipping to vector
added agents to vector
added shipping to vector
added agent to vector
added name to vector
added reference to vector
added shipping to vector
added agents to vector
added shipping to vector
added agent to vector
added description to vector
shipmentid not recognized; using id instead.
shipes not recognized; using random instead.
added ship to vector
added ship to vector
added type to vector
shiips not recognized; using random instead.
sheep_and_goats not recognized; using random instead.
added shareholding to vector
added transactions to vector
added share to vector
added count to vector
added shanghai to vector
settlement_amount not recognized; using random instead.
added set to vector
services_and_channels_details not recognized; using random instead.
service_type_description not recognized; using random instead.
added services to vector
added service to vector
added descriptio to vector
added party to vector
added services to vector
added service to vector
added tv to vector
added channel to vector
added series to vector
added name to vector
added protein to vector
added sequence to vector
added length to vector
added protein to vector
added sequence to vector
added identity to vector
added to to vector
added human to vector
added protein to vector
added documents to vector
added sent to vector
added date to vector
added sender to vector
added semesters to vector
added semester to vector
added start to vector
added date to vector
added semesters to vector
added semester to vector
added end to vector
added date to vector
added semesters to vector
added semester to vector
added description to vector
semeseter not recognized; using random instead.
added sells to vector
added seen to vector
added document to vector
added sections to vector
added section to vector
added title to vector
added document to vector
added sections to vector
added section to vector
added sequence to vector
section_number not recognized; using random instead.
added sections to vector
added section to vector
added description to vector
added document to vector
added sections to vector
added section to vector
added code to vector
secretary_vote not recognized; using random instead.
added seat to vector
added matches to vector
seasonid not recognized; using id instead.
added user to vector
added searches to vector
added search to vector
added string to vector
added user to vector
added searches to vector
added search to vector
added seq to vector
added user to vector
added searches to vector
added search to vector
added sculptures to vector
sculptureid not recognized; using id instead.
sculptorid not recognized; using id instead.
added scoring to vector
scientists{value} not recognized; using random instead.
school_year not recognized; using random instead.
school_performance not recognized; using random instead.
school_name not recognized; using random instead.
school_details not recognized; using random instead.
school_colors not recognized; using random instead.
school_code not recognized; using random instead.
school_bus not recognized; using random instead.
added scheme to vector
added say to vector
added saves to vector
added restriction to vector
added saturday to vector
added stay to vector
added required to vector
added saturday to vector
added santo to vector
added sales to vector
added sales to vector
added transaction to vector
added id to vector
sales_in_billion not recognized; using random instead.
added sales to vector
added sales to vector
added details to vector
saleprice not recognized; using random instead.
sale_amount not recognized; using random instead.
added sale to vector
added sailors to vector
added safety to vector
added safari to vector
s_id not recognized; using id instead.
added ryley to vector
added ryan to vector
royal_family_id not recognized; using id instead.
royal_family_details not recognized; using random instead.
royal_family not recognized; using random instead.
added delivery to vector
added routes to vector
added route to vector
added name to vector
added rounds to vector
added fare to vector
added round to vector
added trip to vector
added required to vector
added fare to vector
added round to vector
added trip to vector
added cost to vector
round_id not recognized; using id instead.
roomtype not recognized; using random instead.
roomnumber not recognized; using random instead.
roomname not recognized; using random instead.
roomid not recognized; using id instead.
added ref to vector
added room to vector
added types to vector
added room to vector
added type to vector
added description to vector
added rooms to vector
added room to vector
added size to vector
added roof to vector
rom_mib not recognized; using random instead.
added rom to vector
roller_coaster_id not recognized; using id instead.
roller_coaster not recognized; using random instead.
role_name not recognized; using random instead.
added robbin to vector
added roads to vector
rnag_mhz not recognized; using random instead.
added river to vector
added river to vector
added name to vector
added river to vector
rings_points not recognized; using random instead.
added right-handed to vector
added right to vector
added performance to vector
added score to vector
added rhythm to vector
added tempo to vector
review_id not recognized; using id instead.
added business to vector
added review to vector
added count to vector
added products to vector
added booked to vector
added returned to vector
added yes to vector
added or to vector
added no to vector
added products to vector
added booked to vector
added returned to vector
added late to vector
added yes to vector
added or to vector
added no to vector
added bookings to vector
added returned to vector
added damaged to vector
added yes to vector
added or to vector
added no to vector
added rental to vector
added return to vector
added date to vector
added player to vector
added retro to vector
added id to vector
added retailing to vector
resultid not recognized; using id instead.
restypename not recognized; using random instead.
restypedescription not recognized; using random instead.
added restriction to vector
restaurant_type not recognized; using random instead.
restaurant_id not recognized; using id instead.
added documents to vector
added response to vector
added received to vector
added date to vector
added respected to vector
resname not recognized; using random instead.
residents_services not recognized; using random instead.
residents_per_officer not recognized; using random instead.
added reside to vector
added reserves to vector
added researchers to vector
research_staff not recognized; using random instead.
research_point not recognized; using random instead.
research_outcomes not recognized; using random instead.
reputation_point not recognized; using random instead.
added represented to vector
representatives' not recognized; using random instead.
representative_name not recognized; using random instead.
reportsto not recognized; using random instead.
added employees to vector
added reports to vector
added to to vector
added reports to vector
added problems to vector
added reported to vector
added by to vector
added staff to vector
added id to vector
added film to vector
added replacement to vector
added cost to vector
added replacement to vector
added replace to vector
added student to vector
added record to vector
added repeat to vector
added term to vector
repair_assignment not recognized; using random instead.
added rep to vector
renting_history not recognized; using random instead.
added film to vector
added rental to vector
added rate to vector
added film to vector
added rental to vector
added duration to vector
added rental to vector
added rental to vector
added date to vector
rent_arrears not recognized; using random instead.
added remarks to vector
added song to vector
releasedate not recognized; using random instead.
added relation to vector
related_document_object_id not recognized; using id instead.
related_collection_id not recognized; using id instead.
added rejected to vector
regular_orders not recognized; using random instead.
regular_order_products not recognized; using random instead.
regoin not recognized; using random instead.
added registries to vector
register_year not recognized; using random instead.
registed not recognized; using random instead.
regional_population not recognized; using random instead.
region_code not recognized; using random instead.
added reggae to vector
added refund to vector
added publication to vector
added reference to vector
added num to vector
ref_user_categories not recognized; using random instead.
ref_transaction_types not recognized; using random instead.
ref_template_types not recognized; using random instead.
ref_staff_roles not recognized; using random instead.
ref_shipping_agents not recognized; using random instead.
ref_service_types not recognized; using random instead.
ref_room_types not recognized; using random instead.
ref_progress_status not recognized; using random instead.
ref_product_categories not recognized; using random instead.
ref_payment_methods not recognized; using random instead.
ref_locations not recognized; using random instead.
ref_incident_type not recognized; using random instead.
ref_hotel_star_ratings not recognized; using random instead.
ref_feature_types not recognized; using random instead.
ref_event_types not recognized; using random instead.
ref_document_status not recognized; using random instead.
ref_colors not recognized; using random instead.
ref_characteristic_types not recognized; using random instead.
ref_calendar not recognized; using random instead.
ref_budget_codes not recognized; using random instead.
ref_attraction_types not recognized; using random instead.
ref_assignment_status not recognized; using random instead.
ref_age_categories not recognized; using random instead.
ref_achievement_type not recognized; using random instead.
recoreded not recognized; using random instead.
added fault to vector
added log to vector
added recorded to vector
added by to vector
added staff to vector
added id to vector
record_id not recognized; using id instead.
record_company not recognized; using random instead.
added behavior to vector
added incident to vector
added recommendations to vector
added recluse to vector
added recipient to vector
added receipts to vector
receiptnumber not recognized; using random instead.
added documents to vector
added receipt to vector
added number to vector
added documents to vector
added receipt to vector
added date to vector
added receieved to vector
readers_in_million not recognized; using random instead.
added player to vector
added attributes to vector
added reactions to vector
rating_in_percent not recognized; using random instead.
added rankings to vector
added ranking to vector
added points to vector
added rankings to vector
added ranking to vector
added date to vector
rank_position not recognized; using random instead.
rank_in_series not recognized; using random instead.
rank_in_round not recognized; using random instead.
added aircraft to vector
added range to vector
added miles to vector
ram_mib not recognized; using random instead.
added rained to vector
added rain to vector
railway_manage not recognized; using random instead.
radio_mhz not recognized; using random instead.
racing_series not recognized; using random instead.
added rachel to vector
race_name not recognized; using random instead.
race_id not recognized; using id instead.
added team to vector
added ra to vector
added matches to vector
queueid not recognized; using id instead.
questions_in_exams not recognized; using random instead.
added questions to vector
question_text not recognized; using random instead.
added machine to vector
added quality to vector
added rank to vector
added qualifying to vector
qualifyid not recognized; using id instead.
added candidate to vector
added assessments to vector
added qualification to vector
added quadra to vector
added kills to vector
added circuits to vector
added url to vector
added circuits to vector
added altitude to vector
added circuits to vector
added longitude to vector
added put to vector
added meetings to vector
added purpose to vector
added of to vector
added meeting to vector
purchaseprice not recognized; using random instead.
added cyclists to vector
added own to vector
added bikes to vector
added purchase to vector
added year to vector
added purchases to vector
added purchase to vector
added transaction to vector
added id to vector
added purchases to vector
added purchase to vector
added details to vector
added punk to vector
added publish to vector
publication_keyword not recognized; using random instead.
publication_id not recognized; using id instead.
publication_date not recognized; using random instead.
added provisional to vector
added provides to vector
added providers to vector
added provider to vector
added protein to vector
added protein to vector
added name to vector
added aircraft to vector
added propulsion to vector
added proportion to vector
property_photos not recognized; using random instead.
property_features not recognized; using random instead.
added properties to vector
added property to vector
added description to vector
added properties to vector
added property to vector
added address to vector
added id to vector
added prominent to vector
project_staff not recognized; using random instead.
project_outcomes not recognized; using random instead.
added ref to vector
added progress to vector
added status to vector
added progress to vector
added status to vector
added description to vector
added assignment to vector
added progress to vector
added progress to vector
added report to vector
added date to vector
progrects not recognized; using random instead.
programs' not recognized; using random instead.
program_requirement not recognized; using random instead.
program_course not recognized; using random instead.
profits_in_billion not recognized; using random instead.
prof_office not recognized; using random instead.
prof_high_degree not recognized; using random instead.
prof_extension not recognized; using random instead.
added prof to vector
produdction not recognized; using random instead.
products_in_events not recognized; using random instead.
products_for_hire not recognized; using random instead.
products_booked not recognized; using random instead.
added product to vector
added categories to vector
added product to vector
added type to vector
added description to vector
added invoice to vector
added line to vector
added items to vector
added product to vector
added title to vector
product_suppliers not recognized; using random instead.
added catalog to vector
added contents to vector
added product to vector
added stock to vector
added number to vector
product_in_event_id not recognized; using id instead.
product_characteristics not recognized; using random instead.
added product to vector
added characteristics to vector
added product to vector
added characteristic to vector
added value to vector
added reference to vector
added product to vector
added categories to vector
added product to vector
added category to vector
added description to vector
product_categories not recognized; using random instead.
added producer to vector
added process to vector
added status to vector
added process to vector
added status to vector
added description to vector
process_status not recognized; using random instead.
process_outcomes not recognized; using random instead.
added process to vector
added outcomes to vector
added process to vector
added outcome to vector
added description to vector
added business to vector
added processes to vector
added process to vector
added name to vector
added business to vector
added processes to vector
added process to vector
added description to vector
added problem to vector
added status to vector
added codes to vector
added problem to vector
added status to vector
added description to vector
problem_status_codes not recognized; using random instead.
added problem to vector
added log to vector
added problem to vector
added log to vector
added id to vector
problem_log not recognized; using random instead.
added problems to vector
added problem to vector
added description to vector
added problem to vector
added category to vector
added codes to vector
added problem to vector
added category to vector
added description to vector
problem_category_codes not recognized; using random instead.
added printed to vector
added print to vector
principal_activities not recognized; using random instead.
primaryaffiliation not recognized; using random instead.
added pricy to vector
added priced to vector
added hotels to vector
added price to vector
added range to vector
added properties to vector
added price to vector
added min to vector
added properties to vector
added price to vector
added max to vector
added catalog to vector
added contents to vector
added price to vector
added in to vector
added pounds to vector
added catalog to vector
added contents to vector
added price to vector
added in to vector
added euros to vector
added catalog to vector
added contents to vector
added price to vector
added in to vector
added dollars to vector
price_in_dollar not recognized; using random instead.
added financial to vector
added transactions to vector
added previous to vector
added transaction to vector
added id to vector
added catalog to vector
added contents to vector
added previous to vector
added entry to vector
added id to vector
added previous to vector
added aircraft to vector
added pressurized to vector
added press to vector
president_vote not recognized; using random instead.
added presented to vector
added prescribes to vector
added prerequisite to vector
added prerequisite to vector
added id to vector
prereq not recognized; using random instead.
prepnurse not recognized; using random instead.
added fare to vector
added basis to vector
added premium to vector
added premises to vector
added premises to vector
added type to vector
added premises to vector
added premise to vector
added details to vector
added prefix to vector
added artist to vector
added preferred to vector
added genre to vector
added player to vector
added attributes to vector
added preferred to vector
added foot to vector
added preferences to vector
added prefer to vector
added student to vector
added predicted to vector
added graduation to vector
added semester to vector
added weather to vector
added precipitation to vector
added inches to vector
added course to vector
added prerequisite to vector
added pre to vector
added course to vector
added id to vector
added practiced to vector
added ppos to vector
added team to vector
added ppf to vector
added powertrain to vector
added power to vector
added posted to vector
added possibly to vector
positionorder not recognized; using random instead.
added player to vector
added attributes to vector
added positioning to vector
added populated to vector
added participants to vector
added popularity to vector
added populace to vector
popuation not recognized; using random instead.
pop_quiz not recognized; using random instead.
pommel_horse_points not recognized; using random instead.
added pollock to vector
poll_source not recognized; using random instead.
added countries to vector
added politics to vector
added score to vector
police_officers not recognized; using random instead.
police_force not recognized; using random instead.
pole_position not recognized; using random instead.
poker_player_id not recognized; using id instead.
poker_player not recognized; using random instead.
pnumber not recognized; using random instead.
added pno to vector
added manager to vector
plyr not recognized; using random instead.
added mgr to vector
plocation not recognized; using random instead.
added plaza to vector
plays_games not recognized; using random instead.
playlisttrack not recognized; using random instead.
playlist_tracks not recognized; using random instead.
added playlist to vector
added tracks to vector
added playlist to vector
added id to vector
player_college not recognized; using random instead.
player_coach not recognized; using random instead.
player_award_vote not recognized; using random instead.
player_award not recognized; using random instead.
player_attributes not recognized; using random instead.
added matches to vector
platformid not recognized; using id instead.
platform_name not recognized; using random instead.
planned_destruction_date not recognized; using random instead.
planetid not recognized; using id instead.
pixel_aspect_ratio_par not recognized; using random instead.
added pitstops to vector
pitching_postseason not recognized; using random instead.
added pitching to vector
pinksbought not recognized; using random instead.
pilotskills not recognized; using random instead.
added piloted to vector
pilot_record not recognized; using random instead.
added pigs to vector
added pieces to vector
added piece to vector
physicians{value} not recognized; using random instead.
physicaldmgdealt not recognized; using random instead.
physdmgtochamp not recognized; using random instead.
physdmgtaken not recognized; using random instead.
added property to vector
added photos to vector
added photo to vector
added title to vector
added property to vector
added photos to vector
added photo to vector
added seq to vector
photo_id not recognized; using id instead.
added property to vector
added photos to vector
added photo to vector
added filename to vector
added property to vector
added photos to vector
added photo to vector
added description to vector
phons not recognized; using random instead.
phone_market not recognized; using random instead.
added phantom to vector
pettype not recognized; using random instead.
added hotels to vector
added pets to vector
added allowed to vector
added yn to vector
added peter to vector
added pets to vector
added pet to vector
added age to vector
personfriend not recognized; using random instead.
added people to vector
added addresses to vector
added person to vector
added address to vector
added id to vector
perpetrator_id not recognized; using id instead.
added students to vector
added permanent to vector
added address to vector
added id to vector
performers_in_bookings not recognized; using random instead.
added performers to vector
performance_score not recognized; using random instead.
added perform to vector
people’s not recognized; using random instead.
added peoples to vector
people_addresses not recognized; using random instead.
peopel not recognized; using random instead.
pentakills not recognized; using random instead.
added pennsylvania to vector
added player to vector
added attributes to vector
added penalties to vector
added pcp to vector
added payments to vector
added payment to vector
added type to vector
added code to vector
added reference to vector
added payment to vector
added methods to vector
added payment to vector
added method to vector
added description to vector
added payments to vector
added payment to vector
added details to vector
pay_per_view_ppv not recognized; using random instead.
added aircraft to vector
added pay to vector
added load to vector
added pay to vector
pattern_recognition not recognized; using random instead.
party_theme not recognized; using random instead.
party_services not recognized; using random instead.
added parties to vector
added party to vector
added phone to vector
party_name not recognized; using random instead.
party_host not recognized; using random instead.
party_forms not recognized; using random instead.
party_events not recognized; using random instead.
added parties to vector
added party to vector
added email to vector
party_details not recognized; using random instead.
party_addresses not recognized; using random instead.
added user to vector
added profiles to vector
added partition to vector
added id to vector
parties_in_events not recognized; using random instead.
added participation to vector
participates_in not recognized; using random instead.
added participates to vector
participants_in_events not recognized; using random instead.
participant_type_code not recognized; using random instead.
participant_details not recognized; using random instead.
added parts to vector
added part to vector
added name to vector
part_faults not recognized; using random instead.
added parkway to vector
added properties to vector
added parking to vector
added lots to vector
parking_fines not recognized; using random instead.
added park to vector
added park to vector
added name to vector
added park to vector
added park to vector
added alias to vector
parent_service_type_code not recognized; using random instead.
added organizations to vector
added parent to vector
added organization to vector
added id to vector
added functional to vector
added areas to vector
added parent to vector
added functional to vector
added area to vector
added code to vector
added catalog to vector
added contents to vector
added parent to vector
added entry to vector
added id to vector
added document to vector
added structures to vector
added parent to vector
added document to vector
added structure to vector
added code to vector
parent_document_object_id not recognized; using id instead.
parent_collection_id not recognized; using id instead.
added parent to vector
parallel_bars_points not recognized; using random instead.
paragraph_text not recognized; using random instead.
paragraph_id not recognized; using id instead.
papers' not recognized; using random instead.
paperkeyphrase not recognized; using random instead.
paperdataset not recognized; using random instead.
added pairs to vector
added pair to vector
added paintings to vector
paintingid not recognized; using id instead.
painterid not recognized; using id instead.
added product to vector
added pages to vector
added per to vector
added minute to vector
added color to vector
packagenumber not recognized; using random instead.
package_version not recognized; using random instead.
package_option not recognized; using random instead.
added oxen to vector
ownjunglekills not recognized; using random instead.
added owning to vector
owner’s not recognized; using random instead.
added properties to vector
added owner to vector
added user to vector
added id to vector
owned_since not recognized; using random instead.
added overpayments to vector
added countries to vector
added overall to vector
added score to vector
added player to vector
added attributes to vector
added overall to vector
added rating to vector
outsanding not recognized; using random instead.
added project to vector
added outcomes to vector
added outcome to vector
added details to vector
added research to vector
added outcomes to vector
added outcome to vector
added description to vector
added others to vector
added engineer to vector
added visits to vector
added other to vector
added visit to vector
added details to vector
added users to vector
added other to vector
added user to vector
added details to vector
added shipments to vector
added other to vector
added shipment to vector
added details to vector
added delivery to vector
added routes to vector
added other to vector
added route to vector
added details to vector
added rooms to vector
added other to vector
added room to vector
added details to vector
other_property_features not recognized; using random instead.
added assignment to vector
added progress to vector
added other to vector
added progress to vector
added details to vector
added products to vector
added other to vector
added product to vector
added details to vector
added problems to vector
added other to vector
added problem to vector
added details to vector
added student to vector
added course to vector
added assignments to vector
added other to vector
added placement to vector
added details to vector
added parts to vector
added other to vector
added part to vector
added details to vector
added order to vector
added items to vector
added other to vector
added order to vector
added item to vector
added details to vector
added problem to vector
added log to vector
added other to vector
added log to vector
added details to vector
added hotels to vector
added other to vector
added hotel to vector
added details to vector
added courses to vector
added scheduled to vector
added other to vector
added course to vector
added schedule to vector
added details to vector
added courses to vector
added offered to vector
added other to vector
added course to vector
added offering to vector
added details to vector
added maintenance to vector
added contracts to vector
added other to vector
added contract to vector
added details to vector
added third to vector
added party to vector
added companies to vector
added other to vector
added company to vector
added details to vector
added characteristics to vector
added other to vector
added characteristic to vector
added details to vector
added customers to vector
added cards to vector
added other to vector
added card to vector
added details to vector
other_available_features not recognized; using random instead.
added assets to vector
added other to vector
added asset to vector
added details to vector
added properties to vector
added oth to vector
added feature to vector
added 3 to vector
added properties to vector
added oth to vector
added feature to vector
added 2 to vector
added properties to vector
added oth to vector
added feature to vector
added 1 to vector
added os to vector
added originate to vector
added film to vector
added original to vector
added language to vector
added id to vector
added songs to vector
added original to vector
added artist to vector
original_airdate not recognized; using random instead.
organized_by not recognized; using random instead.
added organizations to vector
added organization to vector
added name to vector
added organizations to vector
added organization to vector
added details to vector
organization_contact_individuals not recognized; using random instead.
organisation_types not recognized; using random instead.
added organisation to vector
added types to vector
added organisation to vector
added type to vector
added description to vector
added organisations to vector
added organisation to vector
added details to vector
added ore to vector
added ordinal to vector
added ordering to vector
orderes not recognized; using random instead.
order_year not recognized; using random instead.
added customer to vector
added orders to vector
added order to vector
added shipping to vector
added charges to vector
added customer to vector
added orders to vector
added order to vector
added placed to vector
added order to vector
added items to vector
added order to vector
added item to vector
added status to vector
added order to vector
added items to vector
added order to vector
added item to vector
added details to vector
order_deliveries not recognized; using random instead.
added customer to vector
added orders to vector
added order to vector
added delivered to vector
added orbit to vector
oppose_rate not recognized; using random instead.
operating_system not recognized; using random instead.
added operates to vector
operate_company not recognized; using random instead.
added opera to vector
openning_year not recognized; using random instead.
added home to vector
added game to vector
added openings to vector
opening_hours not recognized; using random instead.
onscholarship not recognized; using random instead.
added fare to vector
added one to vector
added direction to vector
added cost to vector
oncallstart not recognized; using random instead.
oncallend not recognized; using random instead.
on_call not recognized; using random instead.
official_ratings_(millions) not recognized; using random instead.
official_name not recognized; using random instead.
official_languages not recognized; using random instead.
office_locations not recognized; using random instead.
offering_instructor_id not recognized; using id instead.
offering_instructor not recognized; using random instead.
added offering to vector
added oder to vector
added occured to vector
added occupations to vector
added observed to vector
objectnumber not recognized; using random instead.
added object to vector
ny_phil not recognized; using random instead.
added nursing to vector
numciting not recognized; using random instead.
numcitedby not recognized; using random instead.
numcc not recognized; using random instead.
added numbered to vector
number_team not recognized; using random instead.
number_products not recognized; using random instead.
number_of_stories not recognized; using random instead.
number_of_product_category not recognized; using random instead.
number_of_platforms not recognized; using random instead.
number_of_matches not recognized; using random instead.
number_of_hosts not recognized; using random instead.
number_of_championships not recognized; using random instead.
number_in_season not recognized; using random instead.
number_deaths not recognized; using random instead.
number_city_affected not recognized; using random instead.
number_cities not recognized; using random instead.
num_semesters not recognized; using random instead.
num_reviews not recognized; using random instead.
num_of_ticket not recognized; using random instead.
num_of_stock not recognized; using random instead.
num_of_shaff_in_charge not recognized; using random instead.
added tv to vector
added series to vector
added num to vector
added of to vector
added seasons to vector
num_of_pieces not recognized; using random instead.
num_of_factories not recognized; using random instead.
num_of_employees not recognized; using random instead.
num_of_component not recognized; using random instead.
num_enrolled not recognized; using random instead.
num_employees not recognized; using random instead.
added nov to vector
added noth to vector
added assessment to vector
added notes to vector
added notes to vector
added id to vector
added norway to vector
added normally to vector
added nominations to vector
no_skip not recognized; using random instead.
added customer to vector
added number to vector
added of to vector
added loans to vector
added bank to vector
added no to vector
added of to vector
added customers to vector
added restriction to vector
added no to vector
added discounts to vector
added nicholas to vector
next_show_name not recognized; using random instead.
added business to vector
added processes to vector
added next to vector
added process to vector
added id to vector
added catalog to vector
added contents to vector
added next to vector
added entry to vector
added id to vector
next_claim_stage_id not recognized; using id instead.
news_report not recognized; using random instead.
neutralminionskilled not recognized; using random instead.
added station to vector
added network to vector
added name to vector
net_worth_millions not recognized; using random instead.
added neighbourhood to vector
added neighbourhood to vector
added name to vector
added neighbourhood to vector
added neighborhood to vector
added needs to vector
added hall to vector
added of to vector
added fame to vector
added needed to vector
added note to vector
added hall to vector
added of to vector
added fame to vector
added needed to vector
neames not recognized; using random instead.
added player to vector
added name to vector
added last to vector
added player to vector
added name to vector
added given to vector
added college to vector
added name to vector
added full to vector
added player to vector
added name to vector
added first to vector
naems not recognized; using random instead.
added team to vector
added franchise to vector
added na to vector
added assoc to vector
added n to vector
musictype not recognized; using random instead.
museum_details not recognized; using random instead.
added multiracial to vector
msot not recognized; using random instead.
added mph to vector
movietheaters not recognized; using random instead.
moviest not recognized; using random instead.
added moved to vector
added office to vector
added locations to vector
added move to vector
added in to vector
added year to vector
added mountain to vector
added mountain to vector
added name to vector
added mountain to vector
added mountain to vector
added altitude to vector
added genre to vector
added most to vector
added popular to vector
added in to vector
added mortgage to vector
added mortage to vector
added months to vector
added student to vector
added addresses to vector
added monthly to vector
added rental to vector
month_profits_billion not recognized; using random instead.
added month to vector
added month to vector
added name to vector
money_requested not recognized; using random instead.
money_rank not recognized; using random instead.
added monday to vector
added moment to vector
moeny not recognized; using random instead.
added modes to vector
modelid not recognized; using id instead.
model_year not recognized; using random instead.
model_name not recognized; using random instead.
model_list not recognized; using random instead.
added mm to vector
mission_id not recognized; using id instead.
added miramichi to vector
added airport to vector
added service to vector
added minutes to vector
added distant to vector
added minumum to vector
minor_in not recognized; using random instead.
added minit to vector
added restriction to vector
added minimum to vector
added stay to vector
added airport to vector
added minimum to vector
added connect to vector
added time to vector
minimu not recognized; using random instead.
added weather to vector
added min to vector
added visibility to vector
added miles to vector
added weather to vector
added min to vector
added temperature to vector
added f to vector
added weather to vector
added min to vector
added sea to vector
added level to vector
added pressure to vector
added inches to vector
min_salary not recognized; using random instead.
added weather to vector
added min to vector
added humidity to vector
added weather to vector
added min to vector
added dew to vector
added point to vector
added f to vector
added program to vector
added requirement to vector
added min to vector
added credit to vector
added airport to vector
added service to vector
added miles to vector
added distant to vector
added mib to vector
mgr_start_date not recognized; using random instead.
mgr_ssn not recognized; using random instead.
added meyer to vector
added swimmer to vector
added meter to vector
added 700 to vector
added swimmer to vector
added meter to vector
added 600 to vector
added swimmer to vector
added meter to vector
added 500 to vector
added swimmer to vector
added meter to vector
added 400 to vector
added swimmer to vector
added meter to vector
added 300 to vector
added swimmer to vector
added meter to vector
added 200 to vector
added swimmer to vector
added meter to vector
added 100 to vector
added mergenthaler to vector
added menu to vector
added mentioned to vector
added mendelian to vector
memory_in_g not recognized; using random instead.
membership_register_branch not recognized; using random instead.
membership_card not recognized; using random instead.
added branch to vector
added membership to vector
added amount to vector
member_of_club not recognized; using random instead.
member_of not recognized; using random instead.
member_name not recognized; using random instead.
member_in_charge_id not recognized; using id instead.
member_attendance not recognized; using random instead.
added meetings to vector
added meeting to vector
added type to vector
added meetings to vector
added meeting to vector
added outcome to vector
mediumon not recognized; using random instead.
added medicine to vector
added enzyme to vector
added interaction to vector
added medicine to vector
added id to vector
medicine_enzyme_interaction not recognized; using random instead.
mediatype not recognized; using random instead.
media_types not recognized; using random instead.
added tracks to vector
added media to vector
added type to vector
added id to vector
medcines not recognized; using random instead.
added measurements to vector
added measured to vector
measuerment not recognized; using random instead.
added weather to vector
added mean to vector
added wind to vector
added speed to vector
added mph to vector
added weather to vector
added mean to vector
added visibility to vector
added miles to vector
added weather to vector
added mean to vector
added temperature to vector
added f to vector
added weather to vector
added mean to vector
added sea to vector
added level to vector
added pressure to vector
added inches to vector
added weather to vector
added mean to vector
added humidity to vector
added weather to vector
added mean to vector
added dew to vector
added point to vector
added f to vector
added food to vector
added service to vector
added meal to vector
added number to vector
added food to vector
added service to vector
added meal to vector
added description to vector
added mcewen to vector
added may to vector
maxoccupancy not recognized; using random instead.
added restriction to vector
added maximum to vector
added stay to vector
max_wind_speed_mph not recognized; using random instead.
added weather to vector
added max to vector
added visibility to vector
added miles to vector
added weather to vector
added max to vector
added temperature to vector
added f to vector
max_team_number not recognized; using random instead.
max_speed not recognized; using random instead.
added weather to vector
added max to vector
added sea to vector
added level to vector
added pressure to vector
added inches to vector
max_salary not recognized; using random instead.
added product to vector
added max to vector
added page to vector
added size to vector
added weather to vector
added max to vector
added humidity to vector
added weather to vector
added max to vector
added gust to vector
added speed to vector
added mph to vector
max_gross_weight not recognized; using random instead.
max_disk_loading not recognized; using random instead.
added weather to vector
added max to vector
added dew to vector
added point to vector
added f to vector
added camera to vector
added lens to vector
added max to vector
added aperture to vector
added bike to vector
added material to vector
match_season not recognized; using random instead.
match_result not recognized; using random instead.
added matches to vector
added match to vector
added num to vector
added mascots to vector
added marks to vector
added player to vector
added attributes to vector
added marking to vector
marketing_regions not recognized; using random instead.
marketing_region_name not recognized; using random instead.
marketing_region_descriptrion not recognized; using random instead.
market_value_in_billion not recognized; using random instead.
market_value_billion not recognized; using random instead.
market_value not recognized; using random instead.
added browser to vector
added market to vector
added share to vector
market_rate not recognized; using random instead.
market_district not recognized; using random instead.
market_details not recognized; using random instead.
added screen to vector
added mode to vector
added map to vector
added manufacturing to vector
added manufactures to vector
manufacturers' not recognized; using random instead.
added manufacture to vector
added manner to vector
added store to vector
added manager to vector
added staff to vector
added id to vector
manager_half not recognized; using random instead.
manager_award_vote not recognized; using random instead.
manager_award not recognized; using random instead.
added management to vector
male_id not recognized; using id instead.
making_year not recognized; using random instead.
added making to vector
added majority to vector
added majored to vector
major_record_format not recognized; using random instead.
major_ranking not recognized; using random instead.
major_name not recognized; using random instead.
major_code not recognized; using random instead.
maintenance_engineers not recognized; using random instead.
maintenance_contracts not recognized; using random instead.
added maintenance to vector
added contracts to vector
added maintenance to vector
added contract to vector
added company to vector
added id to vector
added maintain to vector
added mainly to vector
main_services not recognized; using random instead.
main_industry not recognized; using random instead.
added dual to vector
added carrier to vector
added main to vector
added airline to vector
added campaigns to vector
added start to vector
added date to vector
added campaigns to vector
added name to vector
added campaigns to vector
added end to vector
added date to vector
mailshot_customers not recognized; using random instead.
added customers to vector
added customer to vector
added date to vector
mailshot_campaigns not recognized; using random instead.
added documents to vector
added mailed to vector
added mailing to vector
added date to vector
added documents to vector
added mailed to vector
added mailed to vector
added to to vector
added address to vector
added id to vector
added mailed to vector
added mail to vector
magicdmgtochamp not recognized; using random instead.
magicdmgtaken not recognized; using random instead.
magicdmgdealt not recognized; using random instead.
made_by not recognized; using random instead.
machine_series not recognized; using random instead.
added mac to vector
added ma to vector
lyric_fm_mhz not recognized; using random instead.
added luca to vector
highlow not recognized; using random instead.
added lowest to vector
added point to vector
added lowest to vector
added elevation to vector
added weekly to vector
added weather to vector
added low to vector
added temperature to vector
added dual to vector
added carrier to vector
added low to vector
added flight to vector
added number to vector
low_estimate not recognized; using random instead.
added loves to vector
added love to vector
added louisiana to vector
added lots to vector
added lot to vector
added details to vector
added ship to vector
added lost to vector
added in to vector
added battle to vector
added postseason to vector
added losses to vector
added matches to vector
added loser to vector
added seed to vector
added matches to vector
added loser to vector
added rank to vector
added points to vector
added matches to vector
added loser to vector
added rank to vector
added matches to vector
added loser to vector
added name to vector
added matches to vector
added loser to vector
added ioc to vector
added matches to vector
added loser to vector
added id to vector
added matches to vector
added loser to vector
added ht to vector
added matches to vector
added loser to vector
added hand to vector
added matches to vector
added loser to vector
added entry to vector
added matches to vector
added loser to vector
added age to vector
added longest to vector
added time to vector
added spent to vector
added living to vector
added longest-serving to vector
added longest-lasting to vector
added player to vector
added attributes to vector
added long to vector
added shots to vector
added player to vector
added attributes to vector
added long to vector
added passing to vector
long_lectures not recognized; using random instead.
added problem to vector
added log to vector
added log to vector
added entry to vector
added fix to vector
added problem to vector
added log to vector
added log to vector
added entry to vector
added description to vector
added problem to vector
added log to vector
added log to vector
added entry to vector
added date to vector
location_of_office not recognized; using random instead.
location_description not recognized; using random instead.
added delivery to vector
added route to vector
added locations to vector
added location to vector
added address to vector
added id to vector
localname not recognized; using random instead.
added station to vector
added local to vector
added authority to vector
added loan to vector
added loan to vector
added type to vector
loan_id not recognized; using id instead.
added races to vector
added time to vector
lives_in not recognized; using random instead.
added literacy to vector
added links to vector
added addresses to vector
added line to vector
added 3 to vector
added area to vector
added locality to vector
added addresses to vector
added line to vector
added 2 to vector
added number to vector
added street to vector
added likes to vector
added liked to vector
added id to vector
added like to vector
added lifespans to vector
added lifespan to vector
lifeexpectancy not recognized; using random instead.
lieutenant_governor not recognized; using random instead.
added license to vector
added team to vector
added lg to vector
added win to vector
added lexicographically to vector
added lexicographic to vector
added letters to vector
added grade to vector
added conversion to vector
added letter to vector
added grade to vector
added lessons to vector
added lesson to vector
added time to vector
added lessons to vector
added lesson to vector
added status to vector
added code to vector
added lessons to vector
added lesson to vector
added id to vector
added lessons to vector
added lesson to vector
added date to vector
added lent to vector
lengthes not recognized; using random instead.
added bridge to vector
added length to vector
added meters to vector
added bridge to vector
added length to vector
added feet to vector
added lended to vector
legendarykills not recognized; using random instead.
added flight to vector
added leg to vector
added leg to vector
added number to vector
added flight to vector
added leg to vector
added leg to vector
added flight to vector
left_office not recognized; using random instead.
added lecturers to vector
added leagues to vector
added postseason to vector
added league to vector
added id to vector
added winner to vector
added postseason to vector
added league to vector
added id to vector
added loser to vector
leader_name not recognized; using random instead.
launched_year not recognized; using random instead.
launch_year not recognized; using random instead.
launch_date not recognized; using random instead.
added battle to vector
added latin to vector
added commander to vector
added late to vector
added lasts to vector
added lastly to vector
last_year not recognized; using random instead.
added largest to vector
added multi to vector
added kill to vector
added largest to vector
added killing to vector
added spree to vector
largestcrit not recognized; using random instead.
added lake to vector
added lake to vector
added name to vector
added labour to vector
added kuhr to vector
added koby to vector
added kim to vector
added killing to vector
added sprees to vector
keyphrasename not recognized; using random instead.
keyphrase not recognized; using random instead.
kerluke not recognized; using random instead.
added countries to vector
added justice to vector
added score to vector
added june to vector
added player to vector
added attributes to vector
added jumping to vector
added jul to vector
journalname not recognized; using random instead.
added joseph to vector
join_year not recognized; using random instead.
added staff to vector
added department to vector
added assignments to vector
added job to vector
added title to vector
added code to vector
job_history not recognized; using random instead.
added jan to vector
added j to vector
added ith to vector
added order to vector
added items to vector
added item to vector
added status to vector
added code to vector
added order to vector
added items to vector
added item to vector
added order to vector
added quantity to vector
added order to vector
added items to vector
added item to vector
added id to vector
added order to vector
added items to vector
added item to vector
added delivered to vector
item6 not recognized; using random instead.
item5 not recognized; using random instead.
item4 not recognized; using random instead.
item3 not recognized; using random instead.
item2 not recognized; using random instead.
item1 not recognized; using random instead.
added italian to vector
issue_date not recognized; using random instead.
isofficial not recognized; using random instead.
isava not recognized; using random instead.
added users to vector
added is to vector
added seller to vector
added business to vector
added is to vector
added open to vector
is_new not recognized; using random instead.
is_main_in_charge not recognized; using random instead.
is_full_time not recognized; using random instead.
is_free not recognized; using random instead.
is_first_director not recognized; using random instead.
added users to vector
added is to vector
added buyer to vector
added involve to vector
invoicelineid not recognized; using id instead.
invoiceline not recognized; using random instead.
invoicedate not recognized; using random instead.
added invoices to vector
added invoice to vector
added status to vector
added code to vector
added invoices to vector
added invoice to vector
added status to vector
invoice_lines not recognized; using random instead.
invoice_line_items not recognized; using random instead.
invoice_items not recognized; using random instead.
invoice_item_id not recognized; using id instead.
investor_details not recognized; using random instead.
added student to vector
added internship to vector
added internet to vector
international_passengers not recognized; using random instead.
added product to vector
added interface to vector
added interchange to vector
added player to vector
added attributes to vector
added interceptions to vector
interanation not recognized; using random instead.
added medicine to vector
added enzyme to vector
added interaction to vector
added interaction to vector
added type to vector
integration_platform_id not recognized; using id instead.
integration_platform_details not recognized; using random instead.
integration_platform not recognized; using random instead.
insuranceid not recognized; using id instead.
instructure not recognized; using random instead.
institution_name not recognized; using random instead.
added instances to vector
added station to vector
added installation to vector
added date to vector
added inst to vector
added inspirational to vector
added inside to vector
injury_accident not recognized; using random instead.
added initial to vector
added individuals to vector
inidividual not recognized; using random instead.
added phone to vector
added inhibitors to vector
added inheritance to vector
added hall to vector
added of to vector
added fame to vector
added inducted to vector
indstries not recognized; using random instead.
added individuals to vector
added individual to vector
added middle to vector
added name to vector
added individuals to vector
added individual to vector
added last to vector
added name to vector
added individuals to vector
added individual to vector
added first to vector
added name to vector
added individuals to vector
added individual to vector
added email to vector
added individuals to vector
added individual to vector
added address to vector
indepyear not recognized; using random instead.
added independent to vector
added incur to vector
added incorporated to vector
added reference to vector
added incident to vector
added type to vector
added incident to vector
added type to vector
added description to vector
added behavior to vector
added incident to vector
added incident to vector
added summary to vector
in_office not recognized; using random instead.
added images to vector
added image to vector
added url to vector
added images to vector
added image to vector
added name to vector
added images to vector
added image to vector
added alt to vector
added text to vector
added image to vector
ihsaa_football_class not recognized; using random instead.
ihsaa_class not recognized; using random instead.
added ihsaa to vector
added ignore to vector
if_full_time not recognized; using random instead.
if_first_show not recognized; using random instead.
if_affirmative_win not recognized; using random instead.
if_active not recognized; using random instead.
added ideas to vector
idauthor not recognized; using random instead.
added works to vector
id2 not recognized; using random instead.
ice_cream not recognized; using random instead.
added i to vector
added hunger to vector
humity not recognized; using random instead.
htat not recognized; using random instead.
added properties to vector
added hse to vector
added feature to vector
added 3 to vector
added properties to vector
added hse to vector
added feature to vector
added 2 to vector
added properties to vector
added hse to vector
added feature to vector
added 1 to vector
added hs to vector
added team to vector
added hra to vector
how_to_get_there not recognized; using random instead.
added houston to vector
added houses to vector
house_number not recognized; using random instead.
hoursperweek not recognized; using random instead.
hours_played not recognized; using random instead.
added time to vector
added zone to vector
added hours to vector
added from to vector
added gmt to vector
added hotels to vector
added hotel to vector
added id to vector
hosue not recognized; using random instead.
hosting_city not recognized; using random instead.
host_city_id not recognized; using id instead.
host_city not recognized; using random instead.
horizontal_bar_points not recognized; using random instead.
home_team not recognized; using random instead.
home_games not recognized; using random instead.
home_game not recognized; using random instead.
home_conference not recognized; using random instead.
home_city not recognized; using random instead.
added home to vector
added town to vector
added holds to vector
added histories to vector
added hispanics to vector
hiredate not recognized; using random instead.
added him to vector
added hilarious to vector
highway_fuel_economy_rate not recognized; using random instead.
hight_definition_tv not recognized; using random instead.
highschooler not recognized; using random instead.
highest_position not recognized; using random instead.
added highest to vector
added point to vector
added highest to vector
added elevation to vector
added weekly to vector
added weather to vector
added high to vector
added temperature to vector
added dual to vector
added carrier to vector
added high to vector
added flight to vector
added number to vector
high_estimate not recognized; using random instead.
added here to vector
helpfulness_score not recognized; using random instead.
added paintings to vector
added height to vector
added mm to vector
height_feet not recognized; using random instead.
heavy_reading not recognized; using random instead.
heavy_papers not recognized; using random instead.
heavy_assignments not recognized; using random instead.
added heaviest to vector
added heavier to vector
added cyclist to vector
added heat to vector
added countries to vector
added health to vector
added score to vector
head{value}ha' not recognized; using random instead.
headquartered_city not recognized; using random instead.
added headquarted to vector
added headphone to vector
headofstate not recognized; using random instead.
added player to vector
added attributes to vector
added heading to vector
added accuracy to vector
added headed to vector
haven{value}thompson' not recognized; using random instead.
added hat to vector
has_projects not recognized; using random instead.
has_pet not recognized; using random instead.
has_lab not recognized; using random instead.
has_final_project not recognized; using random instead.
has_final_exam not recognized; using random instead.
has_exams not recognized; using random instead.
has_discussion not recognized; using random instead.
has_clearance not recognized; using random instead.
has_amenity not recognized; using random instead.
has_allergy not recognized; using random instead.
banturn not recognized; using random instead.
added harold to vector
added harford to vector
hardware_model_name not recognized; using random instead.
hardware_colours not recognized; using random instead.
happy_hour_member not recognized; using random instead.
happy_hour not recognized; using random instead.
added hanzi to vector
hanyu_pinyin not recognized; using random instead.
added hangar to vector
hall_of_fame not recognized; using random instead.
added hail to vector
added team to vector
added ha to vector
gymnast_id not recognized; using id instead.
added guruvayur to vector
added guests to vector
added guest to vector
added last to vector
added name to vector
added guests to vector
added guest to vector
added first to vector
added name to vector
gtype not recognized; using random instead.
added gsi to vector
added grow to vector
group_projects not recognized; using random instead.
ground_service not recognized; using random instead.
added ground to vector
added service to vector
added ground to vector
added fare to vector
gross_worldwide not recognized; using random instead.
gross_in_dollar not recognized; using random instead.
gross/total not recognized; using random instead.
added results to vector
added rank to vector
added fielding to vector
added outfield to vector
added grf to vector
graphics_mode not recognized; using random instead.
added grants to vector
added grant to vector
added start to vector
added date to vector
added grants to vector
added grant to vector
added end to vector
added date to vector
added grants to vector
added grant to vector
added amount to vector
graduation_college not recognized; using random instead.
added graduates to vector
added graduated to vector
gradient_from not recognized; using random instead.
gradeconversion not recognized; using random instead.
added gpas to vector
added all to vector
added star to vector
added gp to vector
governmentform not recognized; using random instead.
government_website not recognized; using random instead.
added gotten to vector
added gorgoroth to vector
added customers to vector
added good to vector
added or to vector
added bad to vector
added customer to vector
good_lecture not recognized; using random instead.
good_feedback not recognized; using random instead.
goldspent not recognized; using random instead.
added goldner to vector
goldearned not recognized; using random instead.
added goes to vector
goergia not recognized; using random instead.
added goals to vector
added goal to vector
gnpold not recognized; using random instead.
gname not recognized; using random instead.
added fielding to vector
added outfield to vector
added glf to vector
added glasgow to vector
added player to vector
added attributes to vector
added gk to vector
added reflexes to vector
added player to vector
added attributes to vector
added gk to vector
added positioning to vector
added player to vector
added attributes to vector
added gk to vector
added kicking to vector
added player to vector
added attributes to vector
added gk to vector
added handling to vector
added player to vector
added attributes to vector
added gk to vector
added diving to vector
added gives to vector
added courses to vector
added scheduled to vector
added given to vector
added by to vector
added staff to vector
added id to vector
added team to vector
ghome not recognized; using random instead.
geovannyton not recognized; using random instead.
added geographic to vector
added song to vector
added genre to vector
added is to vector
added tracks to vector
added genre to vector
added id to vector
added generated to vector
added fielding to vector
added outfield to vector
added gcf to vector
added gasoline to vector
gas_station not recognized; using random instead.
added properties to vector
added garage to vector
added yn to vector
gamesplayed not recognized; using random instead.
game_player not recognized; using random instead.
added all to vector
added star to vector
added game to vector
added num to vector
added gallery to vector
added appearances to vector
added g to vector
added ss to vector
added appearances to vector
added g to vector
added rf to vector
added appearances to vector
added g to vector
added pr to vector
added appearances to vector
added g to vector
added ph to vector
added appearances to vector
added g to vector
added p to vector
added appearances to vector
added g to vector
added of to vector
added genre to vector
added genre to vector
added name to vector
added appearances to vector
added g to vector
added lf to vector
added appearances to vector
added g to vector
added dh to vector
added appearances to vector
added g to vector
added defense to vector
added appearances to vector
added g to vector
added cf to vector
added appearances to vector
added g to vector
added c to vector
added appearances to vector
added g to vector
added batting to vector
added appearances to vector
added g to vector
added all to vector
added appearances to vector
added g to vector
added 3b to vector
added appearances to vector
added g to vector
added 2b to vector
added appearances to vector
added g to vector
added 1b to vector
added furthest to vector
furniture_manufacte not recognized; using random instead.
funiture not recognized; using random instead.
functional_areas not recognized; using random instead.
added functional to vector
added areas to vector
added functional to vector
added area to vector
added description to vector
added function to vector
fullname not recognized; using random instead.
added business to vector
added full to vector
added address to vector
fuel_propulsion not recognized; using random instead.
fte_ay not recognized; using random instead.
from_year not recognized; using random instead.
added friendship to vector
friendly-type not recognized; using random instead.
added friend to vector
added friend to vector
added id to vector
added friday to vector
freight_metric_tonnes not recognized; using random instead.
added player to vector
added attributes to vector
added free to vector
added kick to vector
added accuracy to vector
added free to vector
added team to vector
added franchise to vector
added franchise to vector
added name to vector
added franchise to vector
added team to vector
added fp to vector
added foundation to vector
added found to vector
forname not recognized; using random instead.
added forms to vector
added form to vector
added type to vector
added code to vector
added party to vector
added forms to vector
added form to vector
added status to vector
added code to vector
added forms to vector
added form to vector
added number to vector
added forms to vector
added form to vector
added name to vector
added forms to vector
added form to vector
added description to vector
added ford to vector
added foot to vector
food_type not recognized; using random instead.
food_service not recognized; using random instead.
added follows to vector
added follower to vector
added foggy to vector
added fog to vector
added camera to vector
added lens to vector
added focal to vector
added length to vector
added mm to vector
added focal to vector
floor_exercise_points not recognized; using random instead.
added flight to vector
added flight to vector
added number to vector
flightnumber not recognized; using random instead.
flightno not recognized; using random instead.
flight_stop not recognized; using random instead.
added flight to vector
added flight to vector
added number to vector
flight_leg not recognized; using random instead.
flight_fare not recognized; using random instead.
added flight to vector
added flight to vector
added days to vector
fleet_series not recognized; using random instead.
added properties to vector
added fld to vector
added feature to vector
added 3 to vector
added properties to vector
added fld to vector
added feature to vector
added 2 to vector
added properties to vector
added fld to vector
added feature to vector
added 1 to vector
added flavor to vector
added flagship to vector
added fixing to vector
firstharry not recognized; using random instead.
towerkills not recognized; using random instead.
dragonkills not recognized; using random instead.
baronkills not recognized; using random instead.
inhibkills not recognized; using random instead.
first_year not recognized; using random instead.
first_notification_of_loss not recognized; using random instead.
first_elected not recognized; using random instead.
added player to vector
added attributes to vector
added finishing to vector
added finish to vector
added fines to vector
added fine to vector
added finances to vector
added finally to vector
final_table_made not recognized; using random instead.
added player to vector
added final to vector
added game to vector
film_text not recognized; using random instead.
film_market_estimation not recognized; using random instead.
film_category not recognized; using random instead.
film_actor not recognized; using random instead.
added filename to vector
added files to vector
added file to vector
added size to vector
added fifth to vector
fielding_postseason not recognized; using random instead.
fielding_outfield not recognized; using random instead.
added fielding to vector
added field to vector
added fiats to vector
few_tests not recognized; using random instead.
festival_name not recognized; using random instead.
festival_detail not recognized; using random instead.
female_id not recognized; using id instead.
added fedex to vector
added property to vector
added features to vector
added feature to vector
added value to vector
added reference to vector
added feature to vector
added types to vector
added feature to vector
added type to vector
added name to vector
feature_details not recognized; using random instead.
fda_approved not recognized; using random instead.
added faulty to vector
added part to vector
added faults to vector
added fault to vector
added short to vector
added name to vector
fault_log_parts not recognized; using random instead.
added fault to vector
added log to vector
added fault to vector
added log to vector
added entry to vector
fault_log not recognized; using random instead.
fastestlaptime not recognized; using random instead.
fastestlapspeed not recognized; using random instead.
fastestlap not recognized; using random instead.
fastest_qualifying not recognized; using random instead.
fastest_lap not recognized; using random instead.
farm_competition not recognized; using random instead.
fare_basis not recognized; using random instead.
added fare to vector
added fare to vector
added airline to vector
added fare to vector
added far to vector
famous_title not recognized; using random instead.
famous_release_date not recognized; using random instead.
faculty_participates_in not recognized; using random instead.
added apartment to vector
added facilities to vector
added facility to vector
added code to vector
added follows to vector
added follower to vector
added id to vector
added follows to vector
added user to vector
added id to vector
extra_credit not recognized; using random instead.
added experience to vector
added exit to vector
added existence to vector
exhibition_record not recognized; using random instead.
added exceeds to vector
added exceeding to vector
added exams to vector
examinationroom not recognized; using random instead.
exam_name not recognized; using random instead.
exam_date not recognized; using random instead.
added everything to vector
events_number not recognized; using random instead.
added reference to vector
added event to vector
added types to vector
added event to vector
added type to vector
added description to vector
event_name not recognized; using random instead.
event_details not recognized; using random instead.
added student to vector
added events to vector
added event to vector
added date to vector
event_attendance not recognized; using random instead.
estimation_id not recognized; using id instead.
added established to vector
erp_kw not recognized; using random instead.
added equity to vector
equipment_sequence not recognized; using random instead.
added equipment to vector
added equals to vector
eqal not recognized; using random instead.
added medicine to vector
added enzyme to vector
added interaction to vector
added enzyme to vector
added id to vector
entrepreneur_id not recognized; using id instead.
added entrant to vector
added entire to vector
added student to vector
added entered to vector
added as to vector
enrolled_in not recognized; using random instead.
enroll_grade not recognized; using random instead.
added college to vector
added enrollment to vector
added songs to vector
added english to vector
added translation to vector
added england to vector
added aircraft to vector
added engines to vector
engineer_visits not recognized; using random instead.
added engineer to vector
added visits to vector
added engineer to vector
added visit to vector
added id to vector
engineer_skills not recognized; using random instead.
added engine to vector
enforced_requirement not recognized; using random instead.
enemyjunglekills not recognized; using random instead.
added endowment to vector
added endowment to vector
added id to vector
added trip to vector
added end to vector
added station to vector
added name to vector
added trip to vector
added end to vector
added station to vector
added id to vector
added time to vector
added slot to vector
added end to vector
added minute to vector
added time to vector
added slot to vector
added end to vector
added hour to vector
added meetings to vector
added end to vector
added date to vector
added time to vector
added research to vector
added staff to vector
added employer to vector
added organisation to vector
added id to vector
added employees to vector
added employee to vector
added phone to vector
added employees to vector
added employee to vector
added address to vector
added id to vector
added employed to vector
emp_lname not recognized; using random instead.
emp_jobcode not recognized; using random instead.
emp_initial not recognized; using random instead.
emp_hiredate not recognized; using random instead.
emp_fname not recognized; using random instead.
emp_dob not recognized; using random instead.
email_adress not recognized; using random instead.
elnaugh not recognized; using random instead.
elimination_move not recognized; using random instead.
elimination_id not recognized; using id instead.
eliminated_by not recognized; using random instead.
added electoral to vector
added register to vector
added electoral to vector
added register to vector
added id to vector
electoral_register not recognized; using random instead.
added electoral to vector
election_cycle not recognized; using random instead.
added eggs to vector
added egg to vector
added eg to vector
added agree to vector
added objectives to vector
added ed to vector
added fare to vector
added basis to vector
added economy to vector
added countries to vector
added economics to vector
added score to vector
easiness_score not recognized; using random instead.
earpads not recognized; using random instead.
added student to vector
added record to vector
added earn to vector
added credit to vector
added early to vector
added due to vector
added dual to vector
added carrier to vector
added dual to vector
added airline to vector
added routes to vector
added destination to vector
added airport to vector
added id to vector
added routes to vector
added destination to vector
added airport to vector
added flights to vector
added destination to vector
added drums to vector
added driving to vector
added drives to vector
driverstandingsid not recognized; using id instead.
driverstandings not recognized; using random instead.
driverref not recognized; using random instead.
driver_name not recognized; using random instead.
added order to vector
added deliveries to vector
added driver to vector
added employee to vector
added id to vector
driver-matched_db not recognized; using random instead.
added driven to vector
added drive to vector
added drink to vector
added matches to vector
added draw to vector
added size to vector
added draw to vector
drama_workshop_groups not recognized; using random instead.
championid not recognized; using id instead.
added drafts to vector
draft_pick_number not recognized; using random instead.
added document to vector
added drafts to vector
added draft to vector
added details to vector
draft_copies not recognized; using random instead.
draft_class not recognized; using random instead.
added product to vector
added dpi to vector
dphone not recognized; using random instead.
download_rank not recognized; using random instead.
doubles_wl not recognized; using random instead.
added double to vector
added kills to vector
added dorm to vector
added dorm to vector
added name to vector
dorm_amenity not recognized; using random instead.
added endowment to vector
added donator to vector
added name to vector
added donated to vector
added domingo to vector
domestic_passengers not recognized; using random instead.
domain_publication not recognized; using random instead.
domain_keyword not recognized; using random instead.
domain_journal not recognized; using random instead.
domain_conference not recognized; using random instead.
domain_author not recognized; using random instead.
documents_with_expenses not recognized; using random instead.
documents_to_be_destroyed not recognized; using random instead.
documents_processes not recognized; using random instead.
documents_mailed not recognized; using random instead.
documents_in_collections not recognized; using random instead.
document_types not recognized; using random instead.
document_subsets not recognized; using random instead.
document_subset_name not recognized; using random instead.
document_subset_members not recognized; using random instead.
document_subset_details not recognized; using random instead.
document_structures not recognized; using random instead.
added document to vector
added structures to vector
added document to vector
added structure to vector
added description to vector
added reference to vector
added document to vector
added status to vector
added document to vector
added status to vector
added description to vector
document_sections_images not recognized; using random instead.
document_sections not recognized; using random instead.
document_objects not recognized; using random instead.
document_locations not recognized; using random instead.
document_functional_areas not recognized; using random instead.
document_drafts not recognized; using random instead.
document_details not recognized; using random instead.
document_date not recognized; using random instead.
added doctors to vector
added status to vector
added docks to vector
added available to vector
added station to vector
added dock to vector
added count to vector
added constructors to vector
added name to vector
dnum not recognized; using random instead.
dmgtoturrets not recognized; using random instead.
dmgtoobj not recognized; using random instead.
dmgselfmit not recognized; using random instead.
dlocation not recognized; using random instead.
added diverse to vector
added protein to vector
added divergence to vector
added from to vector
added human to vector
added lineage to vector
district_name not recognized; using random instead.
added regular to vector
added orders to vector
distributer not recognized; using random instead.
added id to vector
added distinctive to vector
added ship to vector
added disposition to vector
added of to vector
added ship to vector
added disposed to vector
dish_name not recognized; using random instead.
added fare to vector
added basis to vector
added discounted to vector
added renting to vector
added history to vector
added discount to vector
added id to vector
discount_coupons not recognized; using random instead.
added discount to vector
discipline_enrollments not recognized; using random instead.
added discipline to vector
director_admin not recognized; using random instead.
added airport to vector
added service to vector
added direction to vector
direct_distance not recognized; using random instead.
added product to vector
added dimensions to vector
digital_terrestrial_channel not recognized; using random instead.
diffrent not recognized; using random instead.
differnt not recognized; using random instead.
didnt' not recognized; using random instead.
added developers to vector
destruction_authorised_by_employee_id not recognized; using id instead.
destroyed_by_employee_id not recognized; using id instead.
destairport not recognized; using random instead.
added desk to vector
added describe to vector
descennding not recognized; using random instead.
added derricks to vector
added invoice to vector
added line to vector
added items to vector
added derived to vector
added vat to vector
added payable to vector
added invoice to vector
added line to vector
added items to vector
added derived to vector
added total to vector
added cost to vector
added invoice to vector
added line to vector
added items to vector
added derived to vector
added product to vector
added cost to vector
added department to vector
added store to vector
added chain to vector
added department to vector
added store to vector
added chain to vector
added name to vector
dept_locations not recognized; using random instead.
dept_extension not recognized; using random instead.
dept_address not recognized; using random instead.
added dept to vector
added dependents to vector
dependent_name not recognized; using random instead.
added departures to vector
added flight to vector
added stop to vector
added departure to vector
added flight to vector
added number to vector
added flight to vector
added departure to vector
added date to vector
added flight to vector
added stop to vector
added departure to vector
added airline to vector
departments' not recognized; using random instead.
departmentid not recognized; using id instead.
department_stores not recognized; using random instead.
department_store_chain not recognized; using random instead.
added departments to vector
added department to vector
added description to vector
deparments not recognized; using random instead.
added deparment to vector
density_km not recognized; using random instead.
added state to vector
added density to vector
denesik not recognized; using random instead.
added order to vector
added deliveries to vector
added delivery to vector
added status to vector
added code to vector
delivery_routes not recognized; using random instead.
delivery_route_locations not recognized; using random instead.
added order to vector
added deliveries to vector
added delivery to vector
added date to vector
added deliveries to vector
added degree to vector
added programs to vector
added degree to vector
added summary to vector
added name to vector
added degree to vector
added programs to vector
added degree to vector
added summary to vector
added description to vector
degree_programs not recognized; using random instead.
added defiance to vector
added player to vector
added attributes to vector
added defensive to vector
added work to vector
added rate to vector
defenceteamwidthclass not recognized; using random instead.
defenceteamwidth not recognized; using random instead.
defencepressureclass not recognized; using random instead.
defencepressure not recognized; using random instead.
defencedefenderlineclass not recognized; using random instead.
defenceaggressionclass not recognized; using random instead.
defenceaggression not recognized; using random instead.
decriptions not recognized; using random instead.
decoration_theme not recognized; using random instead.
added decoration to vector
added student to vector
added declare to vector
added major to vector
added player to vector
added debut to vector
debate_people not recognized; using random instead.
deathyear not recognized; using random instead.
added player to vector
added death to vector
added year to vector
added player to vector
added death to vector
added state to vector
added player to vector
added death to vector
added month to vector
added player to vector
added death to vector
added day to vector
added player to vector
added death to vector
added country to vector
added player to vector
added death to vector
added city to vector
added dean to vector
added deals to vector
days_held not recognized; using random instead.
added days to vector
added days to vector
added code to vector
day_or_boarding not recognized; using random instead.
added weekly to vector
added weather to vector
added day to vector
added of to vector
added week to vector
added daugavpils to vector
dateundergoes not recognized; using random instead.
added customer to vector
added payments to vector
added payment to vector
added user to vector
added property to vector
added history to vector
datestamp not recognized; using random instead.
dates_active not recognized; using random instead.
dateorder not recognized; using random instead.
dateexped not recognized; using random instead.
added customers to vector
added cards to vector
added date to vector
added valid to vector
added to to vector
added customers to vector
added cards to vector
added date to vector
added valid to vector
added from to vector
added student to vector
added tests to vector
added taken to vector
added date to vector
added test to vector
added taken to vector
added product to vector
added suppliers to vector
added date to vector
added supplied to vector
added to to vector
added product to vector
added suppliers to vector
added date to vector
added supplied to vector
added from to vector
date_stored not recognized; using random instead.
added properties to vector
added date to vector
added sold to vector
added residents to vector
added services to vector
added date to vector
added requested to vector
added users to vector
added date to vector
added registered to vector
added residents to vector
added services to vector
added date to vector
added provided to vector
added products to vector
added date to vector
added product to vector
added first to vector
added available to vector
added products to vector
added date to vector
added product to vector
added discontinued to vector
added problems to vector
added date to vector
added problem to vector
added reported to vector
added problems to vector
added date to vector
added problem to vector
added closed to vector
date_payment_made not recognized; using random instead.
added properties to vector
added date to vector
added off to vector
added market to vector
added treatments to vector
added date to vector
added of to vector
added treatment to vector
added transcripts to vector
added date to vector
added of to vector
added transcript to vector
added transactions to vector
added date to vector
added of to vector
added transaction to vector
date_of_settlement not recognized; using random instead.
added students to vector
added date to vector
added of to vector
added registration to vector
added catalogs to vector
added date to vector
added of to vector
added publication to vector
added assessment to vector
added notes to vector
added date to vector
added of to vector
added notes to vector
added student to vector
added loans to vector
added date to vector
added of to vector
added loan to vector
added catalogs to vector
added date to vector
added of to vector
added latest to vector
added revision to vector
added students to vector
added date to vector
added of to vector
added latest to vector
added logon to vector
added student to vector
added course to vector
added enrolment to vector
added date to vector
added of to vector
added enrolment to vector
added student to vector
added course to vector
added enrolment to vector
added date to vector
added of to vector
added completion to vector
date_of_claim not recognized; using random instead.
date_of_ceremony not recognized; using random instead.
added student to vector
added course to vector
added attendance to vector
added date to vector
added of to vector
added attendance to vector
date_of_answer not recognized; using random instead.
added residents to vector
added date to vector
added moved to vector
added out to vector
added students to vector
added date to vector
added left to vector
added university to vector
added staff to vector
added date to vector
added left to vector
added staff to vector
added students to vector
added date to vector
added left to vector
added customers to vector
added date to vector
added last to vector
added hire to vector
added staff to vector
added date to vector
added joined to vector
added staff to vector
date_join not recognized; using random instead.
added discount to vector
added coupons to vector
added date to vector
added issued to vector
added behavior to vector
added incident to vector
added date to vector
added incident to vector
added start to vector
added behavior to vector
added incident to vector
added date to vector
added incident to vector
added end to vector
date_in_locaton_to not recognized; using random instead.
date_in_location_from not recognized; using random instead.
added party to vector
added forms to vector
added date to vector
added fully to vector
added completed to vector
added organizations to vector
added date to vector
added formed to vector
added students to vector
added date to vector
added first to vector
added rental to vector
added students to vector
added date to vector
added first to vector
added registered to vector
date_effective_to not recognized; using random instead.
date_effective_from not recognized; using random instead.
added dogs to vector
added date to vector
added departed to vector
date_day not recognized; using random instead.
added organization to vector
added contact to vector
added individuals to vector
added date to vector
added contact to vector
added to to vector
added organization to vector
added contact to vector
added individuals to vector
added date to vector
added contact to vector
added from to vector
added party to vector
added forms to vector
added date to vector
added completion to vector
added started to vector
added complaints to vector
added date to vector
added complaint to vector
added raised to vector
added complaints to vector
added date to vector
added complaint to vector
added closed to vector
date_closed not recognized; using random instead.
added staff to vector
added department to vector
added assignments to vector
added date to vector
added assigned to vector
added to to vector
added staff to vector
added department to vector
added assignments to vector
added date to vector
added assigned to vector
added from to vector
added dogs to vector
added date to vector
added arrived to vector
date_and_time not recognized; using random instead.
date_and_date not recognized; using random instead.
added dogs to vector
added date to vector
added adopted to vector
added achievements to vector
added date to vector
added achievement to vector
added accounts to vector
added date to vector
added account to vector
added opened to vector
datasetname not recognized; using random instead.
added dataset to vector
added dat to vector
damange not recognized; using random instead.
damage_millions_usd not recognized; using random instead.
added products to vector
added for to vector
added hire to vector
added daily to vector
added hire to vector
added cost to vector
added daan to vector
added cylinder to vector
cyclists_own_bikes not recognized; using random instead.
added cyclists to vector
added own to vector
added bikes to vector
added cyclist to vector
added id to vector
added cyclist to vector
added cycles to vector
customers’ not recognized; using random instead.
customers_policies not recognized; using random instead.
customers_cards not recognized; using random instead.
customers_and_services_details not recognized; using random instead.
customers_and_services not recognized; using random instead.
added customers to vector
added customer to vector
added type to vector
added code to vector
added customers to vector
added customer to vector
added status to vector
added code to vector
customer_rating not recognized; using random instead.
customer_policy_id not recognized; using id instead.
customer_policies not recognized; using random instead.
customer_payments not recognized; using random instead.
customer_payment_methods not recognized; using random instead.
added customers to vector
added customer to vector
added password to vector
customer_order not recognized; using random instead.
customer_master_index not recognized; using random instead.
added customers to vector
added customer to vector
added login to vector
customer_interactions not recognized; using random instead.
customer_events not recognized; using random instead.
customer_event_notes not recognized; using random instead.
customer_event_note_id not recognized; using id instead.
customer_contact_channels not recognized; using random instead.
added customers to vector
added customer to vector
added code to vector
added customers to vector
added customer to vector
added address to vector
added id to vector
customer_address_history not recognized; using random instead.
custoemrs not recognized; using random instead.
added customer to vector
added customer to vector
added name to vector
added player to vector
added attributes to vector
added curve to vector
added students to vector
added current to vector
added address to vector
added id to vector
currency_code not recognized; using random instead.
added cumulative to vector
added cummings to vector
culture_company not recognized; using random instead.
added ct to vector
csu_fees not recognized; using random instead.
added aircraft to vector
added cruising to vector
added speed to vector
crs_description not recognized; using random instead.
crs_credit not recognized; using random instead.
crime_rate not recognized; using random instead.
added customer to vector
added credit to vector
added score to vector
added tweets to vector
added create to vector
added date to vector
created_date not recognized; using random instead.
created_by_staff_id not recognized; using id instead.
added customer to vector
added create to vector
added date to vector
added covers to vector
added covered to vector
courses_scheduled not recognized; using random instead.
courses_offered not recognized; using random instead.
course_tags_count not recognized; using random instead.
course_prerequisite not recognized; using random instead.
added courses to vector
added offered to vector
added course to vector
added offering to vector
added name to vector
course_offering not recognized; using random instead.
course_authors_and_tutors not recognized; using random instead.
course_arrange not recognized; using random instead.
added discount to vector
added coupons to vector
added coupon to vector
added amount to vector
added coupon to vector
added addresses to vector
added county to vector
added state to vector
added province to vector
county_public_safety not recognized; using random instead.
county_name not recognized; using random instead.
countryname not recognized; using random instead.
countrylanguage not recognized; using random instead.
countryid not recognized; using id instead.
countryabbrev not recognized; using random instead.
added countires to vector
counties_represented not recognized; using random instead.
countains not recognized; using random instead.
added bookings to vector
added count to vector
added hired to vector
added cound to vector
council_tax not recognized; using random instead.
added could to vector
added costly to vector
cost_per_25_miles not recognized; using random instead.
added treatments to vector
added cost to vector
added of to vector
added treatment to vector
coresponding not recognized; using random instead.
added copyright to vector
added coordinates to vector
added contributed to vector
added maintenance to vector
added contracts to vector
added contract to vector
added start to vector
added date to vector
added maintenance to vector
added contracts to vector
added contract to vector
added end to vector
added date to vector
added continues to vector
continuation_of not recognized; using random instead.
contid not recognized; using id instead.
added contestants to vector
added contestant to vector
added name to vector
added contained to vector
added contacts to vector
added engineer to vector
added visits to vector
added contact to vector
added staff to vector
added id to vector
added contacts to vector
added contact to vector
added phone to vector
added customer to vector
added contact to vector
added channels to vector
added contact to vector
added number to vector
added contacts to vector
added contact to vector
added id to vector
added consume to vector
constructorstandingsid not recognized; using id instead.
constructorstandings not recognized; using random instead.
constructorresultsid not recognized; using id instead.
constructorresults not recognized; using random instead.
constructorref not recognized; using random instead.
added consisting to vector
added considered to vector
consider_rate not recognized; using random instead.
added flight to vector
added connections to vector
conferrred not recognized; using random instead.
conference_participation not recognized; using random instead.
conference_name not recognized; using random instead.
added concluded to vector
concert_name not recognized; using random instead.
completed_year not recognized; using random instead.
added complaints to vector
added complaint to vector
added type to vector
added code to vector
added complaints to vector
added complaint to vector
added status to vector
added code to vector
added complaints to vector
added complaint to vector
added outcome to vector
added code to vector
added complaints to vector
added complaint to vector
added id to vector
added complains to vector
added complained to vector
competition_type not recognized; using random instead.
competition_result not recognized; using random instead.
competition_record not recognized; using random instead.
added accelerator to vector
added compatible to vector
added browser to vector
added compatible to vector
added since to vector
added year to vector
compartment_class not recognized; using random instead.
added third to vector
added party to vector
added companies to vector
added company to vector
added type to vector
added third to vector
added party to vector
added companies to vector
added company to vector
added address to vector
added protein to vector
added common to vector
added name to vector
commmon not recognized; using random instead.
added commissions to vector
commission_pct not recognized; using random instead.
added assignment to vector
added progress to vector
added comments to vector
added on to vector
added progress to vector
added student to vector
added course to vector
added assignments to vector
added comments to vector
added on to vector
added assignment to vector
added comments to vector
added comment to vector
added instructor to vector
added comment to vector
added text to vector
comment_instructor not recognized; using random instead.
added commanders to vector
combined_fuel_economy_rate not recognized; using random instead.
added combined to vector
added comapny to vector
added columns to vector
added colours to vector
added colored to vector
added reference to vector
added colors to vector
added color to vector
added description to vector
college_location not recognized; using random instead.
added collections to vector
collection_subsets not recognized; using random instead.
collection_subset_name not recognized; using random instead.
collection_subset_members not recognized; using random instead.
collection_name not recognized; using random instead.
collection_description not recognized; using random instead.
added collected to vector
collecrtion_subset_details not recognized; using random instead.
added routes to vector
added code to vector
added share to vector
code_description not recognized; using random instead.
code2 not recognized; using random instead.
added cobham to vector
cobam not recognized; using random instead.
added coached to vector
coach_name not recognized; using random instead.
added customer to vector
added master to vector
added index to vector
added cmi to vector
added details to vector
cmi_cross_references not recognized; using random instead.
clubname not recognized; using random instead.
clublocation not recognized; using random instead.
clubdesc not recognized; using random instead.
club_rank not recognized; using random instead.
club_leader not recognized; using random instead.
club_id_2 not recognized; using random instead.
club_id_1 not recognized; using random instead.
added weather to vector
added cloud to vector
added cover to vector
added problems to vector
added closure to vector
added authorised to vector
added by to vector
added staff to vector
added id to vector
added climbing to vector
climbined not recognized; using random instead.
climber_id not recognized; using id instead.
added clients to vector
added client to vector
added details to vector
cleavant not recognized; using random instead.
clear_grading not recognized; using random instead.
clean_jerk not recognized; using random instead.
added classification to vector
class_time not recognized; using random instead.
class_senator_vote not recognized; using random instead.
class_section not recognized; using random instead.
class_room not recognized; using random instead.
class_president_vote not recognized; using random instead.
class_of_service not recognized; using random instead.
added classes to vector
added class to vector
added id to vector
added classes to vector
added class to vector
added details to vector
added class to vector
added of to vector
added service to vector
added class to vector
added description to vector
class_address not recognized; using random instead.
class_aa not recognized; using random instead.
class_a not recognized; using random instead.
clarity_score not recognized; using random instead.
claims_processing_stages not recognized; using random instead.
claims_processing not recognized; using random instead.
claims_documents not recognized; using random instead.
claim_type_code not recognized; using random instead.
claim_status_name not recognized; using random instead.
claim_status_description not recognized; using random instead.
claim_status_code not recognized; using random instead.
claim_processing_id not recognized; using id instead.
claim_outcome_code not recognized; using random instead.
claim_headers not recognized; using random instead.
claim_header_id not recognized; using id instead.
city{value} not recognized; using random instead.
city_town not recognized; using random instead.
city_population not recognized; using random instead.
city_fuel_economy_rate not recognized; using random instead.
city_customer not recognized; using random instead.
city_channel_tv_show not recognized; using random instead.
city_channel_radio not recognized; using random instead.
city_channel not recognized; using random instead.
city_area not recognized; using random instead.
city_agent not recognized; using random instead.
added direct to vector
added distance to vector
city2 not recognized; using random instead.
added code to vector
added direct to vector
added distance to vector
city1 not recognized; using random instead.
added code to vector
added citizens to vector
citingpaperid not recognized; using id instead.
added cite to vector
added citing to vector
cities' not recognized; using random instead.
citedpaperid not recognized; using id instead.
added citation to vector
added cited to vector
added paper to vector
added id to vector
added cite to vector
added cited to vector
citation_point not recognized; using random instead.
added publication to vector
added citation to vector
added num to vector
circulation_history not recognized; using random instead.
circuitref not recognized; using random instead.
added choice to vector
added chiltern to vector
added checkout to vector
added chassis to vector
added charges to vector
added parts to vector
added chargeable to vector
added yn to vector
added parts to vector
added chargeable to vector
added amount to vector
added chargeable to vector
added charges to vector
added charge to vector
added type to vector
added charges to vector
added charge to vector
added id to vector
added charges to vector
added charge to vector
added amount to vector
charactersitic not recognized; using random instead.
added reference to vector
added characteristic to vector
added types to vector
added characteristic to vector
added type to vector
added description to vector
added characteristics to vector
added characteristic to vector
added name to vector
added characteristics to vector
added characteristic to vector
added data to vector
added type to vector
characteristic: not recognized; using random instead.
char_cells not recognized; using random instead.
channel_details not recognized; using random instead.
added customer to vector
added contact to vector
added channels to vector
added channel to vector
added code to vector
chancecreationshootingclass not recognized; using random instead.
chancecreationshooting not recognized; using random instead.
chancecreationpositioningclass not recognized; using random instead.
chancecreationpassingclass not recognized; using random instead.
chancecreationpassing not recognized; using random instead.
chancecreationcrossingclass not recognized; using random instead.
chancecreationcrossing not recognized; using random instead.
added champs to vector
champlvl not recognized; using random instead.
added championship to vector
added chairs to vector
chair_name not recognized; using random instead.
certificationexpires not recognized; using random instead.
certificationdate not recognized; using random instead.
added certain to vector
added centered to vector
census_ranking not recognized; using random instead.
added cells to vector
added customers to vector
added cell to vector
added mobile to vector
added phone to vector
added number to vector
cell_mobile_phone not recognized; using random instead.
added cd to vector
added causes to vector
added death to vector
added caused to vector
added by to vector
added ship to vector
added id to vector
added category to vector
added category to vector
added name to vector
added catalogue to vector
catalog_structure not recognized; using random instead.
added catalogs to vector
added catalog to vector
added publisher to vector
added catalogs to vector
added catalog to vector
added name to vector
added catalog to vector
added structure to vector
added catalog to vector
added level to vector
added name to vector
added catalog to vector
added contents to vector
added catalog to vector
added entry to vector
added name to vector
catalog_contents_additional_attributes not recognized; using random instead.
catalog_contents not recognized; using random instead.
added cast to vector
case_burden not recognized; using random instead.
cartoones not recognized; using random instead.
cartools not recognized; using random instead.
carsw not recognized; using random instead.
cars_data not recognized; using random instead.
added carry to vector
added carribean to vector
added carr to vector
added caribbean to vector
cares_for_students not recognized; using random instead.
added customers to vector
added cards to vector
added card to vector
added type to vector
added code to vector
car_owner not recognized; using random instead.
car_names not recognized; using random instead.
car_makers not recognized; using random instead.
car_# not recognized; using random instead.
captain_id not recognized; using id instead.
added capitals to vector
capacity_percentage not recognized; using random instead.
added capable to vector
added candidates to vector
added candidate to vector
added details to vector
candidate_assessments not recognized; using random instead.
campusfee not recognized; using random instead.
added cameras to vector
added photos to vector
added camera to vector
added lens to vector
added id to vector
camera_lens not recognized; using random instead.
added came to vector
added airlines to vector
added call to vector
added sign to vector
calendar_date not recognized; using random instead.
added properties to vector
added buyer to vector
added offered to vector
added price to vector
added business to vector
added rates to vector
added business to vector
added rates to vector
added id to vector
business_rates not recognized; using random instead.
business_processes not recognized; using random instead.
added burdens to vector
added burden to vector
added bulls to vector
added battle to vector
added bulgarian to vector
added commander to vector
buildupplayspeedclass not recognized; using random instead.
buildupplayspeed not recognized; using random instead.
buildupplaypositioningclass not recognized; using random instead.
buildupplaypassingclass not recognized; using random instead.
buildupplaypassing not recognized; using random instead.
buildupplaydribblingclass not recognized; using random instead.
buildupplaydribbling not recognized; using random instead.
added apartment to vector
added buildings to vector
added building to vector
added short to vector
added name to vector
added apartment to vector
added buildings to vector
added building to vector
added phone to vector
added apartment to vector
added buildings to vector
added building to vector
added manager to vector
added apartment to vector
added buildings to vector
added building to vector
added full to vector
added name to vector
added apartment to vector
added buildings to vector
added building to vector
added description to vector
added apartment to vector
added buildings to vector
added building to vector
added address to vector
build_year not recognized; using random instead.
budget_type_description not recognized; using random instead.
budget_million not recognized; using random instead.
budget_invested_percent not recognized; using random instead.
budget_in_billions not recognized; using random instead.
browswers not recognized; using random instead.
added accelerator to vector
added compatible to vector
added browser to vector
added browser to vector
added id to vector
brown’s not recognized; using random instead.
broadcast_share not recognized; using random instead.
added british to vector
added britanny to vector
added breeds to vector
added breeds to vector
added breed to vector
added name to vector
brazil’s not recognized; using random instead.
added brandon to vector
added team to vector
added bpf to vector
boys_or_girls not recognized; using random instead.
added boxes to vector
added head to vector
added born to vector
added state to vector
border_info not recognized; using random instead.
added border to vector
added info to vector
added border to vector
books_order not recognized; using random instead.
bookings_services not recognized; using random instead.
added party to vector
added services to vector
added booking to vector
added made to vector
added date to vector
added products to vector
added booked to vector
added booked to vector
added count to vector
added products to vector
added booked to vector
added booked to vector
added amount to vector
book_title not recognized; using random instead.
book_series not recognized; using random instead.
book_club not recognized; using random instead.
body_builder_id not recognized; using id instead.
body_builder not recognized; using random instead.
added boats to vector
added bank to vector
bname not recognized; using random instead.
added bluetooth to vector
added blackville to vector
birthyear not recognized; using random instead.
added births to vector
added birthdates to vector
added birthdate to vector
added player to vector
added birth to vector
added state to vector
birth_place not recognized; using random instead.
added player to vector
added birth to vector
added month to vector
added player to vector
added birth to vector
added day to vector
added player to vector
added birth to vector
added country to vector
added students to vector
added bio to vector
added data to vector
added billion to vector
billingstate not recognized; using random instead.
billingpostalcode not recognized; using random instead.
billingcountry not recognized; using random instead.
billingcity not recognized; using random instead.
billingaddress not recognized; using random instead.
added invoices to vector
added billing to vector
added state to vector
added invoices to vector
added billing to vector
added postal to vector
added code to vector
added invoices to vector
added billing to vector
added country to vector
added invoices to vector
added billing to vector
added city to vector
added invoices to vector
added billing to vector
added address to vector
added meetings to vector
added billable to vector
added yn to vector
added status to vector
added bikes to vector
added available to vector
big_silver not recognized; using random instead.
added big to vector
added bianka to vector
added matches to vector
added best to vector
added of to vector
best_finish not recognized; using random instead.
added benjamin to vector
benefits_overpayments not recognized; using random instead.
added benefits to vector
added being to vector
added behaviour to vector
added monitoring to vector
added behaviour to vector
added monitoring to vector
added id to vector
added behaviour to vector
added monitoring to vector
added behaviour to vector
added monitoring to vector
added details to vector
behaviour_monitoring not recognized; using random instead.
behavior_incident not recognized; using random instead.
added begins to vector
added beginning to vector
added time to vector
added interval to vector
added begin to vector
added time to vector
bedtype not recognized; using random instead.
added apartments to vector
added bedroom to vector
added count to vector
added becoming to vector
added beach to vector
added player to vector
bbref not recognized; using random instead.
added id to vector
added team to vector
added bba to vector
batting_postseason not recognized; using random instead.
added batting to vector
added batters to vector
added batter to vector
added apartments to vector
added bathroom to vector
added count to vector
basketball_match not recognized; using random instead.
added fare to vector
added basis to vector
added basis to vector
added days to vector
added aircraft to vector
added basic to vector
added type to vector
baseprice not recognized; using random instead.
teamid not recognized; using id instead.
added participants to vector
matchid not recognized; using id instead.
added bangladeshi to vector
bandmateid not recognized; using id instead.
added hall to vector
added of to vector
added fame to vector
added ballots to vector
added player to vector
added attributes to vector
added ball to vector
added control to vector
added ball to vector
baketball not recognized; using random instead.
away_team not recognized; using random instead.
added away to vector
added averaged to vector
average_attendance not recognized; using random instead.
availablility not recognized; using random instead.
available_policies not recognized; using random instead.
added authorship to vector
authorname not recognized; using random instead.
added authorities to vector
authorder not recognized; using random instead.
author_tutor_atb not recognized; using random instead.
author_or_editor not recognized; using random instead.
author_list not recognized; using random instead.
author_book not recognized; using random instead.
added aug to vector
added audiences to vector
atttending not recognized; using random instead.
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added attribute to vector
added value to vector
added attribute to vector
added definitions to vector
added attribute to vector
added name to vector
attribute_definitions not recognized; using random instead.
added attribute to vector
added definitions to vector
added attribute to vector
added data to vector
added type to vector
attractions' not recognized; using random instead.
attraction_type_description not recognized; using random instead.
attorney_general not recognized; using random instead.
attendnance not recognized; using random instead.
added player to vector
added attributes to vector
added attacking to vector
added work to vector
added rate to vector
asy not recognized; using random instead.
added associate to vector
added assists to vector
assistingnurse not recognized; using random instead.
added assignments to vector
added ref to vector
added assignment to vector
added status to vector
added assignment to vector
added status to vector
added description to vector
assignment_progress not recognized; using random instead.
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added due to vector
added date to vector
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added completed to vector
added date to vector
assignedto not recognized; using random instead.
added problem to vector
added log to vector
added assigned to vector
added to to vector
added staff to vector
added id to vector
assets_in_events not recognized; using random instead.
assets_in_billion not recognized; using random instead.
asset_parts not recognized; using random instead.
added assets to vector
added asset to vector
added model to vector
added assets to vector
added asset to vector
added make to vector
added assets to vector
added asset to vector
added disposed to vector
added date to vector
added assets to vector
added asset to vector
added details to vector
added assets to vector
added asset to vector
added acquired to vector
added date to vector
assessment_notes not recognized; using random instead.
added candidate to vector
added assessments to vector
added assessment to vector
added date to vector
added candidate to vector
added assessments to vector
asessment not recognized; using random instead.
added outcome to vector
added code to vector
ascendingly not recognized; using random instead.
added ascended to vector
added arrive to vector
added flight to vector
added stop to vector
added arrival to vector
added flight to vector
added number to vector
added flight to vector
added arrival to vector
added date to vector
added flight to vector
added stop to vector
added arrival to vector
added airline to vector
added arrange to vector
added around to vector
aritist not recognized; using random instead.
area_size not recognized; using random instead.
area_km_2 not recognized; using random instead.
area_km not recognized; using random instead.
area_code_state not recognized; using random instead.
added area to vector
added code to vector
added state to vector
added area to vector
added code to vector
archtect not recognized; using random instead.
architects' not recognized; using random instead.
added apartments to vector
added apartment to vector
added type to vector
added code to vector
added apartments to vector
added apartment to vector
added number to vector
added properties to vector
added apt to vector
added feature to vector
added 3 to vector
added properties to vector
added apt to vector
added feature to vector
added 2 to vector
added properties to vector
added apt to vector
added feature to vector
added 1 to vector
added apr to vector
appt_type not recognized; using random instead.
added apps to vector
appointmentid not recognized; using id instead.
added restriction to vector
added application to vector
added apple to vector
appearring not recognized; using random instead.
added appearances to vector
added appear to vector
added airports to vector
added airport to vector
added id to vector
added aperture to vector
apartment_facilities not recognized; using random instead.
apartment_buildings not recognized; using random instead.
apartment_bookings not recognized; using random instead.
added anywhere to vector
annual_interchanges not recognized; using random instead.
annual_fuel_cost not recognized; using random instead.
annual_entry_exit not recognized; using random instead.
added android to vector
analytical_layer_type_code not recognized; using random instead.
analytical_layer not recognized; using random instead.
analytical_id not recognized; using id instead.
analogue_terrestrial_channel not recognized; using random instead.
amount_piad not recognized; using random instead.
added bookings to vector
added amount to vector
added payable to vector
added payments to vector
added amount to vector
added paid to vector
added in to vector
added full to vector
added yn to vector
added payments to vector
added amount to vector
added paid to vector
added transactions to vector
added amount to vector
added of to vector
added transaction to vector
added bookings to vector
added amount to vector
added of to vector
added refund to vector
added student to vector
added loans to vector
added amount to vector
added of to vector
added loan to vector
added bookings to vector
added amount to vector
added of to vector
added discount to vector
added payments to vector
added amount to vector
added due to vector
added amongst to vector
amisulpride not recognized; using random instead.
added amersham to vector
added amerindian to vector
added dorm to vector
added amenity to vector
added amenity to vector
added name to vector
added always to vector
added races to vector
added url to vector
added already to vector
alphaetical not recognized; using random instead.
allow_audit not recognized; using random instead.
allery not recognized; using random instead.
allergytype not recognized; using random instead.
allergy_type not recognized; using random instead.
allegry not recognized; using random instead.
all_star not recognized; using random instead.
all_road not recognized; using random instead.
all_neutral not recognized; using random instead.
all_home not recognized; using random instead.
all_games_percent not recognized; using random instead.
all_games not recognized; using random instead.
all_documents not recognized; using random instead.
added all-female to vector
added aliens to vector
added tracks to vector
added album to vector
added id to vector
added al to vector
airports' not recognized; using random instead.
airportname not recognized; using random instead.
airport_service not recognized; using random instead.
added airport to vector
added airport to vector
added location to vector
airport_aircraft not recognized; using random instead.
added airplanes to vector
added airline to vector
added airline to vector
added name to vector
added flight to vector
added airline to vector
added flight to vector
airilne not recognized; using random instead.
added aired to vector
aircraft_movements not recognized; using random instead.
added aircraft to vector
added aircraft to vector
added description to vector
aircon not recognized; using random instead.
air_date not recognized; using random instead.
aiports not recognized; using random instead.
aicrafts not recognized; using random instead.
added agreements to vector
added properties to vector
added agreed to vector
added selling to vector
added price to vector
added player to vector
added attributes to vector
added agility to vector
added player to vector
added attributes to vector
added aggression to vector
added agents to vector
added agencies to vector
added agency to vector
added details to vector
added ref to vector
added age to vector
added categories to vector
added age to vector
added category to vector
added description to vector
affiliated_with not recognized; using random instead.
added affecting to vector
affected_region not recognized; using random instead.
advisory_requirement not recognized; using random instead.
added restriction to vector
added advance to vector
added purchase to vector
added student to vector
added admit to vector
added term to vector
address_road not recognized; using random instead.
added customers to vector
added address to vector
added line to vector
added 2 to vector
added addresses to vector
added address to vector
added content to vector
added address to vector
address2 not recognized; using random instead.
added program to vector
added requirement to vector
added additional to vector
req not recognized; using random instead.
added addison to vector
added ad to vector
actual_orders not recognized; using random instead.
actual_order_products not recognized; using random instead.
added actual to vector
added orders to vector
added actual to vector
added order to vector
added date to vector
actual_destruction_date not recognized; using random instead.
added acts to vector
added activity to vector
added activity to vector
added name to vector
activitor not recognized; using random instead.
added customer to vector
added contact to vector
added channels to vector
added active to vector
added to to vector
added date to vector
added customer to vector
added contact to vector
added channels to vector
added active to vector
added from to vector
added date to vector
added activator to vector
added acquired to vector
added achievements to vector
added reference to vector
added achievement to vector
added type to vector
added achievement to vector
added type to vector
added description to vector
added achievements to vector
added achievement to vector
added id to vector
added achievements to vector
added achievement to vector
added details to vector
added achieved to vector
accreditation_type not recognized; using random instead.
accreditation_level not recognized; using random instead.
accoutning not recognized; using random instead.
accountnumber not recognized; using random instead.
account_details not recognized; using random instead.
accoung not recognized; using random instead.
added according to vector
accomdate not recognized; using random instead.
added accident to vector
added protein to vector
added accession to vector
added number to vector
added accesses to vector
added documents to vector
added access to vector
added count to vector
added accelerator to vector
added compatible to vector
added browser to vector
added accelerator to vector
added id to vector
accelerator_compatible_browser not recognized; using random instead.
added player to vector
added attributes to vector
added acceleration to vector
added customer to vector
added account to vector
added type to vector
acc_road not recognized; using random instead.
acc_regular_season not recognized; using random instead.
acc_percent not recognized; using random instead.
acc_home not recognized; using random instead.
added customer to vector
added account to vector
added balance to vector
added publication to vector
added abstract to vector
added dogs to vector
added abandoned to vector
added yes to vector
added or to vector
added no to vector
added aac to vector
added review to vector
added a to vector
added id to vector
added \ to vector
added > to vector
added ; to vector
added 737-800s to vector
added 64 to vector
added 60000 to vector
added 56 to vector
added 4500 to vector
4000000 not recognized; using random instead.
added 34 to vector
added 3300 to vector
added 315 to vector
3000？ not recognized; using random instead.
2fm_mhz not recognized; using random instead.
added 24 to vector
2002-06-21 not recognized; using random instead.
2000000 not recognized; using random instead.
added 197 to vector
190cm not recognized; using random instead.
18_49_rating_share not recognized; using random instead.
added 1840 to vector
added 1830 to vector
added 13000 to vector
added 13 to vector
added 1031 to vector
100% not recognized; using random instead.
added / to vector
%_change_2007 not recognized; using random instead.
$60 not recognized; using random instead.
$200 not recognized; using random instead.
$150 not recognized; using random instead.
$120 not recognized; using random instead.
added unk to vector
sequence_start not recognized; using random instead.
sequence_end not recognized; using random instead.
copy_word not recognized; using random instead.
copy_schema not recognized; using random instead.
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added * to vector
useracct not recognized; using random instead.
added name to vector
added concerts to vector
added id to vector
added constructor to vector
added standings to vector
added constructor to vector
added id to vector
added customer to vector
added orders to vector
added customer to vector
added id to vector
added addresses to vector
added country to vector
added student to vector
added addresses to vector
added other to vector
added details to vector
added head to vector
added age to vector
added addresses to vector
added city to vector
added order to vector
added items to vector
added product to vector
added id to vector
added customer to vector
added address to vector
added history to vector
added address to vector
added id to vector
added races to vector
added circuit to vector
added id to vector
added students to vector
added in to vector
added detention to vector
added student to vector
added id to vector
added item to vector
added title to vector
added weather to vector
added date to vector
added rankings to vector
added player to vector
added id to vector
added order to vector
added items to vector
added order to vector
added id to vector
added student to vector
added enrolment to vector
added courses to vector
added course to vector
added id to vector
added participates to vector
added in to vector
added student to vector
added id to vector
added customers to vector
added state to vector
added complaints to vector
added staff to vector
added id to vector
added contacts to vector
added last to vector
added name to vector
added contacts to vector
added first to vector
added name to vector
added customers to vector
added review to vector
added rank to vector
added results to vector
added result to vector
added id to vector
added contacts to vector
added gender to vector
added team to vector
added half to vector
added team to vector
added id to vector
added team to vector
added half to vector
added league to vector
added id to vector
added documents to vector
added document to vector
added id to vector
added seasons to vector
added url to vector
added sex to vector
added products to vector
added product to vector
added name to vector
teamstats not recognized; using random instead.
firsttower not recognized; using random instead.
added artists to vector
added last to vector
added name to vector
added artists to vector
added first to vector
added name to vector
added mill to vector
added type to vector
added participants to vector
added role to vector
people_id not recognized; using id instead.
added store to vector
added last to vector
added update to vector
added student to vector
added city to vector
added code to vector
added addresses to vector
added customers to vector
added address to vector
added team to vector
added student to vector
added products to vector
member_id not recognized; using id instead.
added status to vector
added time to vector
added party to vector
added services to vector
added service to vector
added id to vector
added review to vector
added rating to vector
added catalog to vector
added contents to vector
added height to vector
added player to vector
added weight to vector
added store to vector
added store to vector
added id to vector
added staff to vector
added other to vector
added property to vector
added features to vector
added property to vector
added id to vector
added order to vector
added items to vector
added order to vector
added item to vector
added id to vector
added major to vector
added student to vector
added events to vector
added event to vector
added id to vector
added customers to vector
added email to vector
added address to vector
added advisor to vector
school_id not recognized; using id instead.
added lessons to vector
added price to vector
added party to vector
added forms to vector
added party to vector
added id to vector
added customers to vector
added customer to vector
added name to vector
added users to vector
added role to vector
added code to vector
added battle to vector
added result to vector
added products to vector
added product to vector
added price to vector
added city to vector
added population to vector
added customers to vector
added phone to vector
added people to vector
added teachers to vector
added middle to vector
added name to vector
added g to vector
added documents to vector
added document to vector
added type to vector
added code to vector
added customers to vector
added customer to vector
added phone to vector
added river to vector
added country to vector
added name to vector
added country to vector
added country to vector
added id to vector
club_id not recognized; using id instead.
channel_id not recognized; using id instead.
branch_id not recognized; using id instead.
added addresses to vector
added zip to vector
added postcode to vector
added qualifying to vector
added q2 to vector
added customers to vector
added payment to vector
added method to vector
added code to vector
order_items not recognized; using random instead.
added student to vector
added lastname to vector
added inventory to vector
added film to vector
added id to vector
added employees to vector
added employee to vector
added id to vector
added user to vector
added profiles to vector
added email to vector
added film to vector
added text to vector
added description to vector
added degree to vector
added programs to vector
added department to vector
added id to vector
added department to vector
added guests to vector
added date to vector
added of to vector
added birth to vector
added city to vector
added city to vector
added id to vector
added certificate to vector
added airline to vector
added id to vector
added paper to vector
added venue to vector
added addresses to vector
added state to vector
added province to vector
added county to vector
added drivers to vector
added code to vector
region_id not recognized; using id instead.
raceid not recognized; using id instead.
added student to vector
added program to vector
added id to vector
added made to vector
added by to vector
added pid to vector
added customers to vector
added phone to vector
added number to vector
added party to vector
added student to vector
firstname not recognized; using random instead.
driver_id not recognized; using id instead.
added address to vector
added district to vector
added directed to vector
added by to vector
added did to vector
added customer to vector
added address to vector
added history to vector
added date to vector
added to to vector
added customer to vector
added address to vector
added history to vector
added date to vector
added from to vector
added office to vector
added locations to vector
added company to vector
added id to vector
added program to vector
added requirement to vector
added category to vector
added catalog to vector
added contents to vector
added capacity to vector
added section to vector
added building to vector
added author to vector
added users to vector
added user to vector
added id to vector
added detention to vector
added teacher to vector
added id to vector
added students to vector
added qualifying to vector
added q3 to vector
added status to vector
added station to vector
added id to vector
added mountain to vector
added state to vector
added name to vector
added students to vector
added ssn to vector
shop_id not recognized; using id instead.
added station to vector
added services to vector
added services to vector
added service to vector
added type to vector
added code to vector
added student to vector
added record to vector
added semester to vector
added concerts to vector
added season to vector
added matches to vector
added score to vector
added salary to vector
added salary to vector
added region to vector
added products to vector
added product to vector
added description to vector
policy_id not recognized; using id instead.
added written to vector
added by to vector
msid not recognized; using id instead.
added member to vector
added manager to vector
added invoice to vector
added line to vector
added items to vector
added invoice to vector
added number to vector
added high to vector
added schooler to vector
added grade to vector
added employee to vector
added customers to vector
added company to vector
added program to vector
added college to vector
added electoral to vector
added register to vector
added cmi to vector
added cross to vector
added reference to vector
added id to vector
added class to vector
added checkin to vector
added cid to vector
added lake to vector
added area to vector
added w to vector
added school to vector
added room to vector
added invoice to vector
added lines to vector
added quantity to vector
added tasks to vector
added project to vector
added id to vector
added physician to vector
added users to vector
added password to vector
paperid not recognized; using id instead.
added customer to vector
added orders to vector
added order to vector
added status to vector
added code to vector
added order to vector
added items to vector
added order to vector
added quantity to vector
added customer to vector
added orders to vector
added order to vector
added date to vector
added concerts to vector
added orchestra to vector
open_year not recognized; using random instead.
added aircraft to vector
added manufacturer to vector
added customers to vector
added login to vector
added name to vector
location_id not recognized; using id instead.
added l to vector
added invoices to vector
institution_id not recognized; using id instead.
added other to vector
added property to vector
added features to vector
added feature to vector
added id to vector
added enrollment to vector
added employees to vector
driverid not recognized; using id instead.
added documents to vector
customer_orders not recognized; using random instead.
added customers to vector
added customer to vector
added email to vector
added customer to vector
added course to vector
added customers to vector
added county to vector
claim_id not recognized; using id instead.
added city to vector
added city to vector
added name to vector
added campus to vector
added tip to vector
added business to vector
added id to vector
added institution to vector
added building to vector
added id to vector
added party to vector
added services to vector
added booking to vector
added id to vector
added player to vector
added birth to vector
added year to vector
added author to vector
added list to vector
added author to vector
added id to vector
added addresses to vector
added address to vector
added details to vector
added weather to vector
added zip to vector
added code to vector
added results to vector
added race to vector
added id to vector
added customers to vector
added town to vector
added city to vector
tourist_attraction_id not recognized; using id instead.
added invoices to vector
added total to vector
songid not recognized; using id instead.
added team to vector
added so to vector
added team to vector
added sf to vector
added student to vector
added enrolment to vector
added semester to vector
added id to vector
added team to vector
added sb to vector
added r to vector
added properties to vector
added property to vector
added type to vector
added code to vector
added products to vector
added product to vector
added type to vector
added code to vector
added patient to vector
added organization to vector
added contact to vector
added individuals to vector
added organization to vector
added id to vector
added mill to vector
added notes to vector
added model to vector
laptimes not recognized; using random instead.
added milliseconds to vector
market_id not recognized; using id instead.
added order to vector
added deliveries to vector
added location to vector
added code to vector
added catalog to vector
added contents to vector
added length to vector
added invoice to vector
added lines to vector
added invoice to vector
added id to vector
added team to vector
added hr to vector
added team to vector
added hbp to vector
added h to vector
added pitching to vector
added postseason to vector
added gs to vector
added founded to vector
added matches to vector
added duration to vector
added driver to vector
added documents to vector
added document to vector
added name to vector
added document to vector
added types to vector
added document to vector
added description to vector
added dno to vector
added movie to vector
added director to vector
added student to vector
added department to vector
added name to vector
added customers to vector
added customer to vector
added details to vector
added customers to vector
added customer to vector
added address to vector
added team to vector
added cs to vector
constructorid not recognized; using id instead.
added club to vector
added state to vector
added capital to vector
added tv to vector
added series to vector
added budget to vector
book_id not recognized; using id instead.
added player to vector
added birth to vector
added city to vector
added team to vector
added bb to vector
added team to vector
added attendance to vector
added fault to vector
added log to vector
added asset to vector
added id to vector
added albums to vector
added artist to vector
added id to vector
added artist to vector
added aircraft to vector
added affiliation to vector
added reference to vector
added address to vector
added types to vector
added address to vector
added type to vector
added code to vector
added financial to vector
added transactions to vector
added account to vector
added id to vector
added hall to vector
added of to vector
added fame to vector
added votes to vector
added lessons to vector
added vehicle to vector
added id to vector
added user to vector
added profiles to vector
added uid to vector
added financial to vector
added transactions to vector
added transaction to vector
added id to vector
added route to vector
added train to vector
added id to vector
added playlist to vector
added tracks to vector
added track to vector
added id to vector
added timed to vector
added locations to vector
added of to vector
added things to vector
added thing to vector
added id to vector
added theme to vector
added department to vector
added stores to vector
added store to vector
added name to vector
added pitching to vector
added stint to vector
start_year not recognized; using random instead.
added trip to vector
added start to vector
added date to vector
added research to vector
added staff to vector
added staff to vector
added details to vector
added game to vector
added stadium to vector
added id to vector
added stadium to vector
singer_id not recognized; using id instead.
added shipment to vector
added items to vector
added shipment to vector
added id to vector
ship_id not recognized; using id instead.
added pitching to vector
added postseason to vector
added sh to vector
added lives to vector
added in to vector
added room to vector
added number to vector
added roles to vector
added role to vector
added description to vector
firstblood not recognized; using random instead.
added review to vector
added rid to vector
added customer to vector
added event to vector
added notes to vector
added resident to vector
added id to vector
added rankings to vector
added ranking to vector
question_id not recognized; using id instead.
programid not recognized; using id instead.
added product to vector
added product to vector
added customers to vector
added postal to vector
added code to vector
performance_id not recognized; using id instead.
added payment to vector
added payment to vector
added id to vector
added addresses to vector
added other to vector
added address to vector
added details to vector
added orders to vector
added circuits to vector
added country to vector
added staff to vector
added nickname to vector
added movie to vector
added tip to vector
added month to vector
manager_id not recognized; using id instead.
added addresses to vector
added line to vector
added 2 to vector
added addresses to vector
added line to vector
added 1 to vector
added number to vector
added building to vector
added addresses to vector
added line to vector
added 1 to vector
added songs to vector
added language to vector
added tags to vector
added kid to vector
added keyword to vector
added keyword to vector
added invoices to vector
added invoice to vector
added date to vector
added institution to vector
added pitching to vector
added postseason to vector
added ibb to vector
added organization to vector
added homepage to vector
added genre to vector
added genre to vector
added injury to vector
added accident to vector
added game to vector
added id to vector
added pitching to vector
added postseason to vector
added g to vector
added idp to vector
added flight to vector
added stop to vector
added flight to vector
added id to vector
added customers to vector
added fax to vector
added faculty to vector
facid not recognized; using id instead.
added trip to vector
added end to vector
added date to vector
employeeid not recognized; using id instead.
district_id not recognized; using id instead.
added detention to vector
added detention to vector
added type to vector
added code to vector
dept_code not recognized; using random instead.
added departments to vector
added customer to vector
added event to vector
added notes to vector
added date to vector
added moved to vector
added in to vector
added customers to vector
added date to vector
added became to vector
added customer to vector
added customer to vector
added addresses to vector
added date to vector
added address to vector
added to to vector
added customer to vector
added addresses to vector
added date to vector
added address to vector
added from to vector
customer_addresses not recognized; using random instead.
added rent to vector
added arrears to vector
added council to vector
added tax to vector
added id to vector
added organization to vector
added continent to vector
competition_id not recognized; using id instead.
added college to vector
added college to vector
added id to vector
added meetings to vector
added client to vector
added id to vector
added channel to vector
added teachers to vector
added cell to vector
added mobile to vector
added number to vector
added book to vector
added employees to vector
added birth to vector
added date to vector
added player to vector
added award to vector
added vote to vector
added award to vector
added id to vector
added view to vector
added unit to vector
added status to vector
added apartment to vector
added id to vector
added loan to vector
added amount to vector
added routes to vector
added airline to vector
aircraft_id not recognized; using id instead.
added staff to vector
added active to vector
added accounts to vector
added pitching to vector
added postseason to vector
added wp to vector
workshop_group_id not recognized; using id instead.
added qualifying to vector
added constructor to vector
added id to vector
university_id not recognized; using id instead.
added invoice to vector
added lines to vector
added unit to vector
added price to vector
added team to vector
added triple to vector
added transcript to vector
added contents to vector
added transcript to vector
added id to vector
added train to vector
trackid not recognized; using id instead.
added time to vector
added zone to vector
added time to vector
added zone to vector
added code to vector
added tweets to vector
added text to vector
added teachers to vector
added team to vector
added sv to vector
added product to vector
added suppliers to vector
added supplier to vector
added id to vector
added department to vector
added stores to vector
added store to vector
added phone to vector
added store to vector
added station to vector
added state to vector
added state to vector
added code to vector
sqlite_sequence not recognized; using random instead.
added song to vector
added skills to vector
added required to vector
added to to vector
added fix to vector
added skill to vector
added id to vector
added reserves to vector
added sailor to vector
added id to vector
added shop to vector
added team to vector
added sho to vector
added ship to vector
settlement_id not recognized; using id instead.
added dual to vector
added carrier to vector
added service to vector
added name to vector
added things to vector
added service to vector
added details to vector
added artists to vector
added name to vector
added document to vector
added sections to vector
added images to vector
added section to vector
added id to vector
added takes to vector
added classes to vector
added section to vector
added id to vector
added sales to vector
added properties to vector
added room to vector
added count to vector
added roles to vector
added review to vector
resid not recognized; using id instead.
added film to vector
added release to vector
added year to vector
added actual to vector
added orders to vector
added regular to vector
added order to vector
added id to vector
region_name not recognized; using random instead.
ref_document_types not recognized; using random instead.
railway_id not recognized; using id instead.
added properties to vector
added projects to vector
added program to vector
added products to vector
added product to vector
added details to vector
added products to vector
added product to vector
added category to vector
added code to vector
added staff to vector
added in to vector
added processes to vector
added process to vector
added id to vector
positiontext not recognized; using random instead.
added available to vector
added policies to vector
added policy to vector
added type to vector
added code to vector
added player to vector
added player to vector
added name to vector
added planet to vector
pilot_id not recognized; using id instead.
added pilot to vector
added performance to vector
added payments to vector
added customers to vector
added payment to vector
added method to vector
added participants to vector
added performance to vector
added score to vector
added participant to vector
added id to vector
added part to vector
added faults to vector
added part to vector
added id to vector
added skills to vector
added required to vector
added to to vector
added fix to vector
added part to vector
added fault to vector
added id to vector
added team to vector
added park to vector
added products to vector
added parent to vector
added product to vector
added id to vector
added paper to vector
added paper to vector
added id to vector
added owner to vector
added research to vector
added outcomes to vector
added outcome to vector
added code to vector
added students to vector
added other to vector
added student to vector
added details to vector
added staff to vector
added other to vector
added staff to vector
added details to vector
other_item_details not recognized; using random instead.
added train to vector
added origin to vector
added projects to vector
added organisation to vector
added id to vector
added customer to vector
added orders to vector
added order to vector
added details to vector
added student to vector
added record to vector
added offering to vector
added id to vector
museum_id not recognized; using id instead.
added photos to vector
added mountain to vector
added id to vector
added mountain to vector
added movie to vector
added mid to vector
firstinhib not recognized; using random instead.
marketing_region_code not recognized; using random instead.
added make to vector
added locations to vector
added delivery to vector
added route to vector
added locations to vector
added location to vector
added name to vector
added tip to vector
added likes to vector
level_of_membership not recognized; using random instead.
added level to vector
added song to vector
added languages to vector
added language to vector
added language to vector
added id to vector
added journal to vector
job_id not recognized; using id instead.
added publication to vector
added jid to vector
added isbn to vector
added team to vector
ipouts not recognized; using random instead.
added transactions to vector
added investor to vector
added id to vector
added works to vector
added interval to vector
added comment to vector
added instructor to vector
added instructor to vector
added id to vector
added instructor to vector
added airlines to vector
added icao to vector
ic_id not recognized; using id instead.
added airlines to vector
added iata to vector
added review to vector
added item to vector
added id to vector
added hours to vector
added hometown to vector
added professionals to vector
added home to vector
added phone to vector
added headquarters to vector
added matches to vector
gameid not recognized; using id instead.
added flight to vector
added film to vector
added fault to vector
added log to vector
added parts to vector
added fault to vector
added log to vector
added entry to vector
added id to vector
exam_id not recognized; using id instead.
added weather to vector
added events to vector
added student to vector
added events to vector
added event to vector
added type to vector
added code to vector
added event to vector
added team to vector
added era to vector
added team to vector
added er to vector
added engineer to vector
added visits to vector
added engineer to vector
added id to vector
added employees to vector
added employee to vector
added name to vector
emp_num not recognized; using random instead.
added e to vector
added circulation to vector
added history to vector
added draft to vector
added number to vector
added team to vector
added dp to vector
added team to vector
added double to vector
added lives to vector
added in to vector
added dorm to vector
added id to vector
added reference to vector
added document to vector
added types to vector
added document to vector
added type to vector
added description to vector
document_object_id not recognized; using id instead.
added document to vector
added sections to vector
added document to vector
added code to vector
added direct to vector
added distance to vector
added distance to vector
director_id not recognized; using id instead.
directed_by not recognized; using random instead.
added students to vector
added in to vector
added detention to vector
added detention to vector
added id to vector
added departments to vector
added department to vector
added name to vector
added checkin to vector
added day to vector
added orders to vector
added date to vector
added order to vector
added placed to vector
customerid not recognized; using id instead.
added customers to vector
added customer to vector
added last to vector
added name to vector
added customers to vector
added customer to vector
added first to vector
added name to vector
customer_email_address not recognized; using random instead.
added checking to vector
added customer to vector
added id to vector
added course to vector
added credits to vector
added courses to vector
added gsi to vector
added course to vector
added offering to vector
added id to vector
added courses to vector
added course to vector
added name to vector
added courses to vector
added course to vector
added description to vector
county_id not recognized; using id instead.
added countries to vector
added competition to vector
added third to vector
added party to vector
added companies to vector
added company to vector
added name to vector
added photos to vector
added color to vector
collection_id not recognized; using id instead.
cname not recognized; using random instead.
added client to vector
added classroom to vector
added team to vector
added cg to vector
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added catalog to vector
added level to vector
added number to vector
added candidate to vector
added assessments to vector
added candidate to vector
added id to vector
added branch to vector
blockfloor not recognized; using random instead.
blockcode not recognized; using random instead.
added business to vector
added bid to vector
added checking to vector
added balance to vector
artistid not recognized; using id instead.
added song to vector
added artist to vector
added name to vector
amount_claimed not recognized; using random instead.
albumid not recognized; using id instead.
added airports to vector
added flight to vector
added airport to vector
added id to vector
added ground to vector
added service to vector
added airport to vector
added code to vector
added airport to vector
added clients to vector
added agency to vector
added id to vector
added customers to vector
added address to vector
added line to vector
added 1 to vector
added order to vector
added deliveries to vector
added actual to vector
added order to vector
added id to vector
added film to vector
added actor to vector
added actor to vector
added id to vector
added actor to vector
added faculty to vector
added participates to vector
added in to vector
added activity to vector
added id to vector
added team to vector
added ab to vector
years_working not recognized; using random instead.
year_join not recognized; using random instead.
written_by not recognized; using random instead.
added writes to vector
added writer to vector
wrestler_id not recognized; using id instead.
worktitle not recognized; using random instead.
workshop_id not recognized; using id instead.
work_type not recognized; using random instead.
added written to vector
added by to vector
added wid to vector
weekly_rank not recognized; using random instead.
added player to vector
added award to vector
added vote to vector
added votes to vector
added first to vector
added volume to vector
venueid not recognized; using id instead.
added properties to vector
added vendor to vector
added requested to vector
added price to vector
added vehicles to vector
valid_answer_id not recognized; using id instead.
added users to vector
added users to vector
added user to vector
added category to vector
added code to vector
added university to vector
unitprice not recognized; using random instead.
added user to vector
added id to vector
tv_show_id not recognized; using id instead.
tv_series not recognized; using random instead.
added trust to vector
added trust to vector
added order to vector
added deliveries to vector
added truck to vector
added id to vector
added treatments to vector
added treatment to vector
added type to vector
added code to vector
added transcripts to vector
added transactions to vector
added transaction to vector
added type to vector
added code to vector
added financial to vector
added transactions to vector
added transaction to vector
added type to vector
added financial to vector
added transactions to vector
added transaction to vector
added date to vector
added financial to vector
added transactions to vector
added transaction to vector
added comment to vector
added financial to vector
added transactions to vector
added transaction to vector
added amount to vector
added track to vector
tourist_id not recognized; using id instead.
total_passengers not recognized; using random instead.
added student to vector
added total to vector
added credit to vector
added ship to vector
added tonnage to vector
added flight to vector
added to to vector
added airport to vector
added tv to vector
added series to vector
added title to vector
added aka to vector
added time to vector
added slot to vector
added time to vector
added slot to vector
added id to vector
added player to vector
added award to vector
added tie to vector
template_type_code not recognized; using random instead.
template_id not recognized; using id instead.
added repair to vector
added assignment to vector
added technician to vector
added id to vector
firstbaron not recognized; using random instead.
team_name not recognized; using random instead.
added team to vector
added attributes to vector
added team to vector
added fifa to vector
added api to vector
added id to vector
added team to vector
added attributes to vector
added team to vector
added api to vector
added id to vector
submission_id not recognized; using id instead.
added courses to vector
added subject to vector
added id to vector
added student to vector
added enrolment to vector
added courses to vector
added student to vector
added enrolment to vector
added id to vector
added students to vector
added student to vector
added details to vector
student_course_registrations not recognized; using random instead.
added transcript to vector
added contents to vector
added student to vector
added course to vector
added id to vector
student_answer_text not recognized; using random instead.
student_answer_id not recognized; using id instead.
stu_num not recognized; using random instead.
street_address not recognized; using random instead.
added professionals to vector
added street to vector
storm_id not recognized; using id instead.
store_email_address not recognized; using random instead.
added stock to vector
added stay to vector
statusid not recognized; using id instead.
added view to vector
added unit to vector
added status to vector
added status to vector
added date to vector
status_code not recognized; using random instead.
statement_id not recognized; using id instead.
starting_year not recognized; using random instead.
added hotels to vector
added star to vector
added rating to vector
added code to vector
added staff to vector
added in to vector
added processes to vector
added staff to vector
added role to vector
added code to vector
added staff to vector
added staff to vector
added name to vector
added sponsor to vector
spokesman_id not recognized; using id instead.
added songs to vector
added song to vector
added song to vector
added name to vector
added works to vector
added soloists to vector
soloistroles not recognized; using random instead.
soloistname not recognized; using random instead.
soloistinstrument not recognized; using random instead.
added dogs to vector
added size to vector
added code to vector
added singer to vector
added documents to vector
added shipping to vector
added agent to vector
added code to vector
added shipments to vector
added shipments to vector
added shipment to vector
added tracking to vector
added number to vector
shipment_items not recognized; using random instead.
added shipments to vector
added shipment to vector
added date to vector
added shipment to vector
share_in_percent not recognized; using random instead.
added share to vector
added settlements to vector
added semesters to vector
added semesters to vector
added semester to vector
added name to vector
added phone to vector
added screen to vector
added mode to vector
satisfactory_yn not recognized; using random instead.
sales_billion not recognized; using random instead.
added delivery to vector
added route to vector
added locations to vector
added route to vector
added id to vector
added rooms to vector
added rooms to vector
added room to vector
added type to vector
added code to vector
restypeid not recognized; using id instead.
added restriction to vector
added restriction to vector
added code to vector
added restaurant to vector
representative_id not recognized; using id instead.
repair_id not recognized; using id instead.
added rental to vector
added rental to vector
added id to vector
release_date not recognized; using random instead.
added student to vector
added tests to vector
added taken to vector
added registration to vector
added id to vector
added student to vector
added course to vector
added registrations to vector
added registration to vector
added date to vector
ref_property_types not recognized; using random instead.
ref_detention_type not recognized; using random instead.
ref_address_types not recognized; using random instead.
added record to vector
added batting to vector
added postseason to vector
added rbi to vector
rank_of_the_year not recognized; using random instead.
added range to vector
added railway to vector
radio_id not recognized; using id instead.
added race to vector
added publisher to vector
added publication to vector
added reference to vector
added property to vector
added types to vector
added property to vector
added type to vector
added description to vector
added properties to vector
added property to vector
added name to vector
added other to vector
added property to vector
added features to vector
added property to vector
added feature to vector
added description to vector
added properties to vector
added property to vector
added address to vector
added prominence to vector
added projects to vector
added project to vector
added details to vector
added project to vector
added ref to vector
added progress to vector
added status to vector
added progress to vector
added status to vector
added code to vector
profits_billion not recognized; using random instead.
added treatments to vector
added professional to vector
added id to vector
prof_num not recognized; using random instead.
added products to vector
added production to vector
added type to vector
added code to vector
production_code not recognized; using random instead.
added products to vector
added product to vector
added size to vector
added invoice to vector
added line to vector
added items to vector
added product to vector
added quantity to vector
added products to vector
added product to vector
added color to vector
mailshot not recognized; using random instead.
added campaigns to vector
added product to vector
added category to vector
added documents to vector
added processes to vector
added process to vector
added status to vector
added code to vector
added documents to vector
added processes to vector
added process to vector
added outcome to vector
added code to vector
added procedures to vector
added problem to vector
added status to vector
added codes to vector
added problem to vector
added status to vector
added code to vector
added problems to vector
added problem to vector
added id to vector
added problem to vector
added log to vector
added problem to vector
added category to vector
added code to vector
primary_conference not recognized; using random instead.
press_id not recognized; using id instead.
added customer to vector
added addresses to vector
added premise to vector
added id to vector
postalcode not recognized; using random instead.
added fielding to vector
added postseason to vector
added pos to vector
added player to vector
added award to vector
added vote to vector
added points to vector
added won to vector
added player to vector
added award to vector
added vote to vector
added points to vector
added max to vector
added fielding to vector
added postseason to vector
added po to vector
pname not recognized; using random instead.
playlistid not recognized; using id instead.
added player to vector
added player to vector
added fifa to vector
added api to vector
added id to vector
added player to vector
added player to vector
added api to vector
added id to vector
platform_id not recognized; using id instead.
planned_delivery_date not recognized; using random instead.
added hangar to vector
added plane to vector
added name to vector
added pilot to vector
added skills to vector
added pilot to vector
added name to vector
added photos to vector
phone_id not recognized; using id instead.
petid not recognized; using id instead.
added students to vector
added personal to vector
added name to vector
added people to vector
added addresses to vector
added person to vector
added id to vector
performer_id not recognized; using id instead.
added fielding to vector
added postseason to vector
added pb to vector
added payment to vector
added payment to vector
added date to vector
added parties to vector
added park to vector
added park to vector
added id to vector
added paper to vector
added dogs to vector
added owner to vector
added id to vector
overall_ranking not recognized; using random instead.
added financial to vector
added transactions to vector
added other to vector
added transaction to vector
added details to vector
added properties to vector
added other to vector
added property to vector
added details to vector
other_product_service_details not recognized; using random instead.
other_order_details not recognized; using random instead.
added part to vector
added faults to vector
added other to vector
added fault to vector
added details to vector
added customers to vector
added other to vector
added customer to vector
added details to vector
added accounts to vector
added other to vector
added account to vector
added details to vector
original_air_date not recognized; using random instead.
added organizations to vector
added organisations to vector
added organisation to vector
added type to vector
added customer to vector
added orders to vector
added order to vector
added status to vector
added order to vector
added items to vector
added order to vector
added item to vector
added status to vector
added code to vector
added order to vector
orchestra_id not recognized; using id instead.
opening_year not recognized; using random instead.
open_date not recognized; using random instead.
added organization to vector
added oid to vector
official_native_language not recognized; using random instead.
added nurse to vector
num_of_staff not recognized; using random instead.
num_of_shops not recognized; using random instead.
added writer to vector
added num to vector
added of to vector
added episodes to vector
num_of_audience not recognized; using random instead.
added death to vector
added note to vector
added no to vector
musical_id not recognized; using id instead.
music_festival not recognized; using random instead.
added culture to vector
added company to vector
added movie to vector
added id to vector
added works to vector
added movement to vector
added month to vector
added month to vector
added number to vector
added discount to vector
added membership to vector
added credit to vector
added staff to vector
added in to vector
added meetings to vector
added meeting to vector
added id to vector
added sculptures to vector
added medium to vector
added medication to vector
mediatypeid not recognized; using id instead.
added food to vector
added service to vector
added meal to vector
added code to vector
added matches to vector
match_id not recognized; using id instead.
added match to vector
added cmi to vector
added cross to vector
added references to vector
added master to vector
added customer to vector
added id to vector
added market to vector
manufacturer_id not recognized; using id instead.
manager_name not recognized; using random instead.
added maker to vector
major_id not recognized; using id instead.
added assets to vector
added maintenance to vector
added contract to vector
added id to vector
added customers to vector
added id to vector
machine_id not recognized; using id instead.
added transactions to vector
added lots to vector
added lot to vector
added id to vector
added city to vector
added longitude to vector
added customers to vector
added login to vector
added password to vector
added addresses to vector
added line to vector
added 3 to vector
added league to vector
added city to vector
added latitude to vector
added station to vector
added latitude to vector
added driver to vector
added standings to vector
added race to vector
added id to vector
added lap to vector
added label to vector
added death to vector
added killed to vector
keyphraseid not recognized; using id instead.
journalist_id not recognized; using id instead.
journalid not recognized; using id instead.
journal_id not recognized; using id instead.
added staff to vector
added job to vector
added title to vector
added item to vector
is_online not recognized; using random instead.
is_male not recognized; using random instead.
invoiceid not recognized; using id instead.
added invoices to vector
added invoice to vector
added details to vector
added rental to vector
added inventory to vector
added id to vector
instid not recognized; using id instead.
added manager to vector
added half to vector
inseason not recognized; using random instead.
added fielding to vector
added postseason to vector
added inn to vector
added outs to vector
added death to vector
added injured to vector
teambans not recognized; using random instead.
matchid not recognized; using id instead.
added industry to vector
added organization to vector
added contact to vector
added individuals to vector
added individual to vector
added id to vector
incorporated_in not recognized; using random instead.
added behavior to vector
added incident to vector
added incident to vector
added type to vector
added code to vector
added students to vector
added in to vector
added detention to vector
added incident to vector
added id to vector
added document to vector
added sections to vector
added images to vector
added image to vector
added id to vector
idorder not recognized; using random instead.
idclient not recognized; using random instead.
host_id not recognized; using id instead.
added host to vector
added employees to vector
added hire to vector
added date to vector
hh_id not recognized; using id instead.
headphone_id not recognized; using id instead.
head_id not recognized; using id instead.
added head to vector
added team to vector
added half to vector
added half to vector
added apartment to vector
added bookings to vector
added guest to vector
added id to vector
group_equity_shareholding not recognized; using random instead.
added grape to vector
added grants to vector
added grant to vector
added id to vector
added gold to vector
added classification to vector
added gid to vector
added pitching to vector
added postseason to vector
added gf to vector
genreid not recognized; using id instead.
gender_mfu not recognized; using random instead.
added customers to vector
added gender to vector
added guests to vector
added gender to vector
added code to vector
added game to vector
furniture_id not recognized; using id instead.
added document to vector
added functional to vector
added areas to vector
added functional to vector
added area to vector
added code to vector
added flight to vector
added from to vector
added airport to vector
added from to vector
added person to vector
added friend to vector
added friend to vector
added team to vector
added franchise to vector
added franchise to vector
added id to vector
added party to vector
added forms to vector
added form to vector
added id to vector
fnol_id not recognized; using id instead.
added flights to vector
firstdragon not recognized; using random instead.
financial_transactions not recognized; using random instead.
finance_id not recognized; using id instead.
festival_id not recognized; using id instead.
added features to vector
added other to vector
added available to vector
added features to vector
added feature to vector
added type to vector
added code to vector
added other to vector
added available to vector
added features to vector
added feature to vector
added name to vector
added other to vector
added available to vector
added features to vector
added feature to vector
added description to vector
added fault to vector
added log to vector
added parts to vector
added fault to vector
added status to vector
added part to vector
added faults to vector
added fault to vector
added description to vector
farm_id not recognized; using id instead.
added flight to vector
added fare to vector
added fare to vector
added id to vector
added fare to vector
added basis to vector
added fare to vector
added basis to vector
added code to vector
added students to vector
added family to vector
added name to vector
added song to vector
added song to vector
added id to vector
exhibition_id not recognized; using id instead.
eventtype not recognized; using random instead.
essn not recognized; using random instead.
equipment_id not recognized; using id instead.
added endowment to vector
added time to vector
added interval to vector
added end to vector
added time to vector
election_id not recognized; using id instead.
added election to vector
added certificate to vector
added employee to vector
added id to vector
effective_date not recognized; using random instead.
editor_id not recognized; using id instead.
added earnings to vector
added flight to vector
added dual to vector
added carrier to vector
added treatments to vector
added dog to vector
added id to vector
document_type_name not recognized; using random instead.
document_subset_id not recognized; using id instead.
added documents to vector
added document to vector
added structure to vector
added code to vector
added documents to vector
added document to vector
added status to vector
added code to vector
dnumber not recognized; using random instead.
dname not recognized; using random instead.
added division to vector
added team to vector
added half to vector
added div to vector
added win to vector
added team to vector
added half to vector
added div to vector
added id to vector
device_id not recognized; using id instead.
added device to vector
added reference to vector
added detention to vector
added type to vector
added detention to vector
added type to vector
added description to vector
added detention to vector
added detention to vector
added summary to vector
added detention to vector
added train to vector
added destination to vector
added departments to vector
added department to vector
added store to vector
added id to vector
added department to vector
added stores to vector
added department to vector
added store to vector
added chain to vector
added id to vector
added flight to vector
added stop to vector
added departure to vector
added time to vector
added degrees to vector
added student to vector
added enrolment to vector
added degree to vector
added program to vector
added id to vector
debate_id not recognized; using id instead.
added days to vector
added date to vector
added day to vector
added day to vector
added number to vector
added days to vector
added day to vector
added name to vector
added detention to vector
datetime not recognized; using random instead.
added detention to vector
added start to vector
added detention to vector
added detention to vector
added end to vector
date_opened not recognized; using random instead.
added properties to vector
added date to vector
added on to vector
added market to vector
date_claim_settled not recognized; using random instead.
date_claim_made not recognized; using random instead.
datasetid not recognized; using id instead.
customers_and_services_id not recognized; using id instead.
added customers to vector
added customer to vector
added number to vector
added customers to vector
added customer to vector
added middle to vector
added initial to vector
customer_interaction_id not recognized; using id instead.
customer_event_id not recognized; using id instead.
cust_id not recognized; using id instead.
crs_code not recognized; using random instead.
added matches to vector
added creation to vector
added student to vector
added course to vector
added registrations to vector
added course to vector
added schedule to vector
added id to vector
added customers to vector
added coupon to vector
added id to vector
countrycode not recognized; using random instead.
added players to vector
added country to vector
added code to vector
added checkin to vector
added count to vector
added circulation to vector
added history to vector
added copy to vector
added number to vector
added votes to vector
added contestant to vector
added number to vector
added contents to vector
conference_id not recognized; using id instead.
added conference to vector
conductorname not recognized; using random instead.
conductor_id not recognized; using id instead.
concert_id not recognized; using id instead.
composername not recognized; using random instead.
added tracks to vector
added composer to vector
added food to vector
added service to vector
added compartment to vector
added colors to vector
added products to vector
added color to vector
added code to vector
collection_subset_id not recognized; using id instead.
coach_id not recognized; using id instead.
clubid not recognized; using id instead.
club_name not recognized; using random instead.
added clients to vector
added fare to vector
added basis to vector
added class to vector
added type to vector
class_code not recognized; using random instead.
added claims to vector
claim_stage_id not recognized; using id instead.
city_channel_id not recognized; using id instead.
added citizenship to vector
added cite to vector
circuitid not recognized; using id instead.
cinema_id not recognized; using id instead.
church_id not recognized; using id instead.
added phone to vector
added chip to vector
added model to vector
added checkin to vector
added characteristics to vector
added characteristic to vector
added type to vector
added code to vector
added product to vector
added characteristics to vector
added characteristic to vector
added id to vector
added channels to vector
added participants to vector
added position to vector
added professionals to vector
added cell to vector
added number to vector
added film to vector
added category to vector
added category to vector
added id to vector
added catalog to vector
added structure to vector
added catalog to vector
added id to vector
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added catalog to vector
added entry to vector
added id to vector
added carrier to vector
added customers to vector
added cards to vector
added card to vector
added number to vector
added financial to vector
added transactions to vector
added card to vector
added id to vector
added customer to vector
added card to vector
added credit to vector
added captain to vector
added tracks to vector
added bytes to vector
added mill to vector
added built to vector
added year to vector
added builder to vector
budget_type_code not recognized; using random instead.
added bronze to vector
added dogs to vector
added breed to vector
added code to vector
added camera to vector
added lens to vector
added brand to vector
added bookings to vector
added apartment to vector
added bookings to vector
added booking to vector
added status to vector
added code to vector
added apartment to vector
added bookings to vector
added booking to vector
added start to vector
added date to vector
added apartment to vector
added bookings to vector
added booking to vector
added end to vector
added date to vector
added fare to vector
added basis to vector
added booking to vector
added class to vector
added culture to vector
added company to vector
added book to vector
added club to vector
added id to vector
added pitching to vector
added postseason to vector
added bk to vector
added trip to vector
added bike to vector
added id to vector
added pitching to vector
added postseason to vector
added bfp to vector
bdate not recognized; using random instead.
added pitching to vector
added postseason to vector
baopp not recognized; using random instead.
added bandmate to vector
added view to vector
added unit to vector
added status to vector
added available to vector
added yes to vector
added or to vector
added no to vector
added authors to vector
authorid not recognized; using id instead.
added documents to vector
added author to vector
added name to vector
authid not recognized; using id instead.
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added attribute to vector
added id to vector
attraction_type_code not recognized; using random instead.
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added status to vector
added code to vector
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added start to vector
added date to vector
assets_billion not recognized; using random instead.
added assets to vector
artwork_id not recognized; using id instead.
added artists to vector
added flight to vector
added stop to vector
added arrival to vector
added time to vector
added mill to vector
added architect to vector
added id to vector
added view to vector
added unit to vector
added status to vector
added apartment to vector
added booking to vector
added id to vector
added appointment to vector
appelation not recognized; using random instead.
amount_settled not recognized; using random instead.
added customer to vector
added payments to vector
added amount to vector
added payment to vector
added customers to vector
added amount to vector
added outstanding to vector
added has to vector
added amenity to vector
added amenity to vector
added id to vector
added allergy to vector
added airlines to vector
added airline to vector
added id to vector
added albums to vector
airportcode not recognized; using random instead.
added airport to vector
added airport to vector
added name to vector
added airlines to vector
added flight to vector
added airline to vector
added code to vector
added flight to vector
added aircraft to vector
added code to vector
added sequence to vector
added equipment to vector
added sequence to vector
added aircraft to vector
added code to vector
agent_id not recognized; using id instead.
added users to vector
added age to vector
added category to vector
added code to vector
added author to vector
added list to vector
added affiliation to vector
added id to vector
added reference to vector
added address to vector
added types to vector
added address to vector
added type to vector
added description to vector
added customer to vector
added addresses to vector
added address to vector
added type to vector
actual_delivery_date not recognized; using random instead.
added achievements to vector
added achievement to vector
added type to vector
added code to vector
added accounts to vector
added account to vector
added name to vector
added a to vector
added fielding to vector
added zr to vector
years_played not recognized; using random instead.
years_opened not recognized; using random instead.
years_of_experiencing not recognized; using random instead.
years_as_tallest not recognized; using random instead.
added hall to vector
added of to vector
added fame to vector
yearid not recognized; using id instead.
year_working not recognized; using random instead.
year_profits_billion not recognized; using random instead.
year_opened not recognized; using random instead.
year_of_work not recognized; using random instead.
year_of_founded not recognized; using random instead.
year_entered_competition not recognized; using random instead.
year_awarded not recognized; using random instead.
ycard not recognized; using random instead.
added e to vector
added x to vector
added team to vector
added ws to vector
added win to vector
added wrestler to vector
would_take_again not recognized; using random instead.
added workshop to vector
works_on not recognized; using random instead.
added works to vector
added program to vector
added course to vector
added workload to vector
working_year_starts not recognized; using random instead.
working_horses not recognized; using random instead.
wins_count not recognized; using random instead.
added winnings to vector
winning_team not recognized; using random instead.
winning_pilot not recognized; using random instead.
winning_driver not recognized; using random instead.
winning_aircraft not recognized; using random instead.
added matches to vector
added winner to vector
added seed to vector
added matches to vector
added winner to vector
added rank to vector
added points to vector
added matches to vector
added winner to vector
added rank to vector
added matches to vector
added winner to vector
added name to vector
added matches to vector
added winner to vector
added ioc to vector
added matches to vector
added winner to vector
added id to vector
added matches to vector
added winner to vector
added ht to vector
added matches to vector
added winner to vector
added hand to vector
added matches to vector
added winner to vector
added entry to vector
added matches to vector
added winner to vector
added age to vector
added aircraft to vector
added wing to vector
added span to vector
added winery to vector
added wine to vector
added weekly to vector
added weather to vector
added wind to vector
added speed to vector
added mph to vector
added weather to vector
added wind to vector
added dir to vector
added degrees to vector
stats1 not recognized; using random instead.
added win to vector
added wifi to vector
added paintings to vector
added width to vector
added mm to vector
added catalog to vector
added contents to vector
added width to vector
added aircraft to vector
added wide to vector
added body to vector
wholoves not recognized; using random instead.
wholikes not recognized; using random instead.
whoisloved not recognized; using random instead.
whoisliked not recognized; using random instead.
added white to vector
added where to vector
added wheels to vector
weeks_on_top not recognized; using random instead.
weekly_weather not recognized; using random instead.
added wednesday to vector
added wedding to vector
web_client_accelerator not recognized; using random instead.
added weather to vector
added team to vector
added wc to vector
added win to vector
added warehouses to vector
added warehouse to vector
wardsplaced not recognized; using random instead.
wardskilled not recognized; using random instead.
wardsbought not recognized; using random instead.
added customer to vector
added voucher to vector
added credit to vector
voting_record not recognized; using random instead.
added hall to vector
added of to vector
added fame to vector
votedby not recognized; using random instead.
vote_percent not recognized; using random instead.
added votes to vector
added vote to vector
added id to vector
volume_issue not recognized; using random instead.
volume_id not recognized; using id instead.
added player to vector
added attributes to vector
added volleys to vector
added performance to vector
added score to vector
added voice to vector
added sound to vector
added quality to vector
added vocals to vector
visits_restaurant not recognized; using random instead.
added visits to vector
added visitors to vector
visitor_id not recognized; using id instead.
added visitor to vector
added engineer to vector
added visits to vector
added visit to vector
added start to vector
visit_id not recognized; using id instead.
added engineer to vector
added visits to vector
added visit to vector
added end to vector
visit_details not recognized; using random instead.
visit_date not recognized; using random instead.
added visit to vector
visionscore not recognized; using random instead.
added player to vector
added attributes to vector
added vision to vector
viewers_m not recognized; using random instead.
view_unit_status not recognized; using random instead.
view_product_availability not recognized; using random instead.
video_games not recognized; using random instead.
vice_president_vote not recognized; using random instead.
vice_manager_name not recognized; using random instead.
version_number not recognized; using random instead.
added matches to vector
added version to vector
venuename not recognized; using random instead.
added velocity to vector
added renting to vector
added history to vector
added vehicles to vector
added id to vector
vehicle_flight_number not recognized; using random instead.
vehicle_driver not recognized; using random instead.
added vehicles to vector
added vehicle to vector
added details to vector
added vehicle to vector
vault_points not recognized; using random instead.
added product to vector
added categories to vector
added vat to vector
added rating to vector
added machine to vector
added value to vector
added points to vector
added value to vector
valid_answers not recognized; using random instead.
valid_answer_text not recognized; using random instead.
added staff to vector
added username to vector
user_searches not recognized; using random instead.
user_property_history not recognized; using random instead.
user_profiles not recognized; using random instead.
added users to vector
added user to vector
added name to vector
added users to vector
added user to vector
added login to vector
added ref to vector
added user to vector
added categories to vector
added user to vector
added category to vector
added description to vector
added users to vector
added user to vector
added address to vector
added id to vector
added user to vector
added screen to vector
added mode to vector
added used to vector
added kb to vector
us_senate not recognized; using random instead.
unsure_rate not recognized; using random instead.
university_name not recognized; using random instead.
units_sold_millions not recognized; using random instead.
added reference to vector
added product to vector
added categories to vector
added unit to vector
added of to vector
added measure to vector
uniqname not recognized; using random instead.
added union to vector
added undergraduate to vector
added undergoes to vector
added unavailable to vector
added organizations to vector
added uk to vector
added vat to vector
added number to vector
added products to vector
added typical to vector
added selling to vector
added price to vector
added products to vector
added typical to vector
added buying to vector
added price to vector
type_of_thing_code not recognized; using random instead.
type_of_restaurant not recognized; using random instead.
type_of_question_code not recognized; using random instead.
type_of_powertrain not recognized; using random instead.
added tweets to vector
added tv to vector
added show to vector
added tv to vector
added show to vector
added name to vector
tv_show not recognized; using random instead.
tv_channel not recognized; using random instead.
turretkills not recognized; using random instead.
added tuesday to vector
added tryout to vector
truedmgtochamp not recognized; using random instead.
truedmgtaken not recognized; using random instead.
truedmgdealt not recognized; using random instead.
added trucks to vector
added trucks to vector
added truck to vector
added licence to vector
added number to vector
added trucks to vector
added truck to vector
added details to vector
added triple to vector
added kills to vector
added trip to vector
added trinket to vector
added tries to vector
added treatments to vector
treatment_types not recognized; using random instead.
added treatment to vector
added types to vector
added treatment to vector
added type to vector
added description to vector
added treatments to vector
added treatment to vector
added id to vector
added treatment to vector
treasurer_vote not recognized; using random instead.
added river to vector
added traverse to vector
added ground to vector
added service to vector
added transport to vector
added type to vector
added transmitter to vector
transit_passengers not recognized; using random instead.
added student to vector
added record to vector
added transfer to vector
added source to vector
added transcripts to vector
added transcript to vector
added details to vector
added transcripts to vector
added transcript to vector
added date to vector
transcript_contents not recognized; using random instead.
transactions_lots not recognized; using random instead.
added transactions to vector
added reference to vector
added transaction to vector
added types to vector
added transaction to vector
added type to vector
added description to vector
trained_in not recognized; using random instead.
train_station not recognized; using random instead.
added train to vector
added train to vector
added number to vector
train_num not recognized; using random instead.
trade_name not recognized; using random instead.
added tracks to vector
tracklists not recognized; using random instead.
added fielding to vector
added postseason to vector
added tp to vector
harrykills not recognized; using random instead.
added rankings to vector
added tours to vector
added matches to vector
added tourney to vector
added name to vector
added matches to vector
added tourney to vector
added level to vector
added matches to vector
added tourney to vector
added id to vector
added matches to vector
added tourney to vector
added date to vector
tourist_details not recognized; using random instead.
tourist_attractions not recognized; using random instead.
tourist_attraction_features not recognized; using random instead.
tough_tests not recognized; using random instead.
tough_grader not recognized; using random instead.
totunitshealed not recognized; using random instead.
totminionskilled not recognized; using random instead.
totheal not recognized; using random instead.
totdmgtochamp not recognized; using random instead.
totdmgtaken not recognized; using random instead.
totdmgdealt not recognized; using random instead.
totcctimedealt not recognized; using random instead.
totalenrollment_ay not recognized; using random instead.
total_wl not recognized; using random instead.
added product to vector
added suppliers to vector
added total to vector
added value to vector
added purchased to vector
total_spent not recognized; using random instead.
total_production not recognized; using random instead.
total_pounds not recognized; using random instead.
total_points not recognized; using random instead.
added renting to vector
added history to vector
added total to vector
added hours to vector
total_horses not recognized; using random instead.
added student to vector
added total to vector
added gpa to vector
total_duration_min not recognized; using random instead.
total_disk_area not recognized; using random instead.
total_cattle not recognized; using random instead.
added budget to vector
added total to vector
added budget to vector
added percent to vector
added invested to vector
added budget to vector
added total to vector
added budget to vector
added percent to vector
added budgeted to vector
total_attendance not recognized; using random instead.
added product to vector
added suppliers to vector
added total to vector
added amount to vector
added purchased to vector
total_amount not recognized; using random instead.
added student to vector
added total to vector
added credits to vector
top_speed not recognized; using random instead.
took_office not recognized; using random instead.
added tip to vector
added tip to vector
added id to vector
added tip to vector
timed_status_of_things not recognized; using random instead.
timed_locations_of_things not recognized; using random instead.
timecc not recognized; using random instead.
added time to vector
added zone to vector
added time to vector
added zone to vector
added name to vector
time_zone not recognized; using random instead.
time_slot not recognized; using random instead.
time_of_purchase not recognized; using random instead.
time_of_day not recognized; using random instead.
time_interval not recognized; using random instead.
added flight to vector
added time to vector
added elapsed to vector
added postseason to vector
added ties to vector
ticket_price not recognized; using random instead.
added thursday to vector
added player to vector
added throws to vector
third_party_companies not recognized; using random instead.
added things to vector
theme_parks not recognized; using random instead.
theme_park_id not recognized; using id instead.
theme_park_details not recognized; using random instead.
added textbook to vector
added assessment to vector
added notes to vector
added text to vector
added of to vector
added notes to vector
added student to vector
added tests to vector
added taken to vector
added test to vector
added result to vector
added student to vector
added record to vector
added test to vector
added id to vector
added management to vector
added temporary to vector
added acting to vector
added templates to vector
template_type_description not recognized; using random instead.
template_details not recognized; using random instead.
added temperature to vector
added technician to vector
added team to vector
added team to vector
added short to vector
added name to vector
added team to vector
added team to vector
added long to vector
added name to vector
team_leader not recognized; using random instead.
added postseason to vector
added team to vector
added id to vector
added winner to vector
added team to vector
added team to vector
added id to vector
added retro to vector
added postseason to vector
added team to vector
added id to vector
added loser to vector
added team to vector
added team to vector
added id to vector
lahman45 not recognized; using random instead.
added team to vector
added team to vector
added id to vector
added br to vector
team_half not recognized; using random instead.
team_franchise not recognized; using random instead.
team_driver not recognized; using random instead.
team_attributes not recognized; using random instead.
added teaches to vector
added teachers to vector
added teacher to vector
added details to vector
added teacher to vector
added tasks to vector
added tasks to vector
added task to vector
added id to vector
added tasks to vector
added task to vector
added details to vector
added trust to vector
added target to vector
added user to vector
added id to vector
added takes to vector
added tags to vector
added t9 to vector
added t8 to vector
added t7 to vector
added t6 to vector
added t5 to vector
added t4 to vector
added t3 to vector
added t2 to vector
added t10 to vector
added t1 to vector
swimmer_id not recognized; using id instead.
added swimmer to vector
added constructors to vector
added constructor to vector
added reference to vector
surfacearea not recognized; using random instead.
added matches to vector
added surface to vector
supportrepid not recognized; using id instead.
added customers to vector
added support to vector
added rep to vector
added id to vector
support_rate not recognized; using random instead.
added suppliers to vector
added suppliers to vector
added supplier to vector
added phone to vector
added suppliers to vector
added supplier to vector
added name to vector
added assets to vector
added supplier to vector
added company to vector
added id to vector
supplier_addresses not recognized; using random instead.
super_ssn not recognized; using random instead.
added sunday to vector
added sum to vector
added trip to vector
added subscription to vector
added type to vector
added submission to vector
added subjects to vector
added subjects to vector
added subject to vector
added name to vector
subject_code not recognized; using random instead.
sub_tittle not recognized; using random instead.
added studio to vector
students_in_detention not recognized; using random instead.
students_addresses not recognized; using random instead.
student_tests_taken not recognized; using random instead.
student_record not recognized; using random instead.
student_loans not recognized; using random instead.
added student to vector
added loans to vector
added student to vector
added loan to vector
added id to vector
student_events not recognized; using random instead.
student_enrolment_courses not recognized; using random instead.
student_enrolment not recognized; using random instead.
student_course_enrolment not recognized; using random instead.
student_course_attendance not recognized; using random instead.
student_course_assignments not recognized; using random instead.
added dorm to vector
added student to vector
added capacity to vector
student_assessments not recognized; using random instead.
student_answers not recognized; using random instead.
student_addresses not recognized; using random instead.
added students to vector
added addresses to vector
added student to vector
added address to vector
added id to vector
student_address not recognized; using random instead.
stu_transfer not recognized; using random instead.
stu_phone not recognized; using random instead.
stu_lname not recognized; using random instead.
stu_init not recognized; using random instead.
stu_hrs not recognized; using random instead.
stu_gpa not recognized; using random instead.
stu_fname not recognized; using random instead.
stu_dob not recognized; using random instead.
stu_class not recognized; using random instead.
added player to vector
added attributes to vector
added strength to vector
street_name not recognized; using random instead.
street_markets not recognized; using random instead.
added storm to vector
added stories to vector
added stores to vector
store_product not recognized; using random instead.
added department to vector
added stores to vector
added store to vector
added email to vector
store_district not recognized; using random instead.
added department to vector
added stores to vector
added store to vector
added address to vector
added flight to vector
added stops to vector
added restriction to vector
added stopovers to vector
added flight to vector
added stop to vector
added stop to vector
added time to vector
added flight to vector
added stop to vector
added stop to vector
added number to vector
added flight to vector
added stop to vector
added stop to vector
added days to vector
added flight to vector
added stop to vector
added stop to vector
added airport to vector
added pitstops to vector
added stop to vector
staystart not recognized; using random instead.
stayid not recognized; using id instead.
stayend not recognized; using random instead.
status_of_thing_code not recognized; using random instead.
station_name not recognized; using random instead.
station_company not recognized; using random instead.
added statements to vector
statement_details not recognized; using random instead.
state_province not recognized; using random instead.
added customers to vector
added state to vector
added county to vector
added province to vector
state_county not recognized; using random instead.
added all to vector
added star to vector
added starting to vector
added pos to vector
start_time not recognized; using random instead.
added trip to vector
added start to vector
added station to vector
added name to vector
added trip to vector
added start to vector
added station to vector
added id to vector
added time to vector
added slot to vector
added start to vector
added minute to vector
added time to vector
added slot to vector
added start to vector
added hour to vector
start_from not recognized; using random instead.
added meetings to vector
added start to vector
added date to vector
added time to vector
added start to vector
added rating to vector
added rating to vector
added stars to vector
staring_date not recognized; using random instead.
added ref to vector
added hotel to vector
added star to vector
added ratings to vector
added star to vector
added rating to vector
added description to vector
added player to vector
added attributes to vector
added standing to vector
added tackle to vector
added player to vector
added attributes to vector
added stamina to vector
stageposition not recognized; using random instead.
added performance to vector
added score to vector
added stage to vector
added presence to vector
staff_roles not recognized; using random instead.
added reference to vector
added staff to vector
added roles to vector
added staff to vector
added role to vector
added description to vector
added staff to vector
added staff to vector
added last to vector
added name to vector
staff_in_processes not recognized; using random instead.
staff_in_meetings not recognized; using random instead.
added staff to vector
added staff to vector
added gender to vector
added staff to vector
added staff to vector
added first to vector
added name to vector
staff_department_assignments not recognized; using random instead.
added staff to vector
added staff to vector
added address to vector
added id to vector
teamid not recognized; using id instead.
matchid not recognized; using id instead.
added routes to vector
added source to vector
added airport to vector
added id to vector
added routes to vector
added source to vector
added airport to vector
added flights to vector
added source to vector
added player to vector
added attributes to vector
added sprint to vector
added speed to vector
sportsinfo not recognized; using random instead.
sportname not recognized; using random instead.
sponsor_name not recognized; using random instead.
spokesman_district not recognized; using random instead.
added spokesman to vector
added spent to vector
speed_knots not recognized; using random instead.
added speed to vector
added film to vector
added special to vector
added features to vector
speach_title not recognized; using random instead.
added home to vector
added game to vector
added span to vector
added last to vector
added home to vector
added game to vector
added span to vector
added first to vector
sourceairport not recognized; using random instead.
added trust to vector
added source to vector
added user to vector
added id to vector
added cmi to vector
added cross to vector
added references to vector
added source to vector
added system to vector
added code to vector
added source to vector
added performance to vector
added score to vector
added songs to vector
added id to vector
song_release_year not recognized; using random instead.
song_id not recognized; using id instead.
software_platform not recognized; using random instead.
added team to vector
added soa to vector
added snatch to vector
added smoking to vector
small_silver not recognized; using random instead.
added slots to vector
added player to vector
added attributes to vector
added sliding to vector
added tackle to vector
sleephabits not recognized; using random instead.
skills_required_to_fix not recognized; using random instead.
added skills to vector
added skills to vector
added skill to vector
added description to vector
added skills to vector
added skill to vector
added code to vector
added sizes to vector
added sizes to vector
added size to vector
added description to vector
singles_wl not recognized; using random instead.
singer_in_concert not recognized; using random instead.
added silver to vector
added clients to vector
added sic to vector
added code to vector
added properties to vector
added shp to vector
added feature to vector
added 3 to vector
added properties to vector
added shp to vector
added feature to vector
added 2 to vector
added properties to vector
added shp to vector
added feature to vector
added 1 to vector
show_times_per_day not recognized; using random instead.
show_id not recognized; using id instead.
added show to vector
added player to vector
added attributes to vector
added shot to vector
added power to vector
added player to vector
added attributes to vector
added short to vector
added passing to vector
added shops to vector
shop_name not recognized; using random instead.
shop_details not recognized; using random instead.
added customer to vector
added orders to vector
added shipping to vector
added method to vector
added code to vector
added reference to vector
added shipping to vector
added agents to vector
added shipping to vector
added agent to vector
added name to vector
added reference to vector
added shipping to vector
added agents to vector
added shipping to vector
added agent to vector
added description to vector
shipmentid not recognized; using id instead.
added ship to vector
added ship to vector
added type to vector
sheep_and_goats not recognized; using random instead.
added transactions to vector
added share to vector
added count to vector
settlement_amount not recognized; using random instead.
services_and_channels_details not recognized; using random instead.
service_type_description not recognized; using random instead.
added services to vector
added service to vector
added descriptio to vector
added party to vector
added services to vector
added service to vector
added service to vector
added tv to vector
added channel to vector
added series to vector
added name to vector
added protein to vector
added sequence to vector
added length to vector
added protein to vector
added sequence to vector
added identity to vector
added to to vector
added human to vector
added protein to vector
added sep to vector
added documents to vector
added sent to vector
added date to vector
added sender to vector
added semesters to vector
added semester to vector
added start to vector
added date to vector
added semesters to vector
added semester to vector
added end to vector
added date to vector
added semesters to vector
added semester to vector
added description to vector
added select to vector
added sections to vector
added document to vector
added sections to vector
added section to vector
added title to vector
added document to vector
added sections to vector
added section to vector
added sequence to vector
section_number not recognized; using random instead.
added sections to vector
added section to vector
added name to vector
added sections to vector
added section to vector
added description to vector
added document to vector
added sections to vector
added section to vector
added code to vector
added section to vector
secretary_vote not recognized; using random instead.
added seats to vector
added seating to vector
added seasons to vector
added matches to vector
seasonid not recognized; using id instead.
added user to vector
added searches to vector
added search to vector
added string to vector
added user to vector
added searches to vector
added search to vector
added seq to vector
added user to vector
added searches to vector
added search to vector
added sculptures to vector
sculptureid not recognized; using id instead.
sculptorid not recognized; using id instead.
added scores to vector
added scientists to vector
added scientist to vector
school_year not recognized; using random instead.
school_performance not recognized; using random instead.
school_name not recognized; using random instead.
school_details not recognized; using random instead.
school_colors not recognized; using random instead.
school_code not recognized; using random instead.
school_bus not recognized; using random instead.
added schedule to vector
added savings to vector
added restriction to vector
added saturday to vector
added stay to vector
added required to vector
added saturday to vector
added sales to vector
added sales to vector
added transaction to vector
added id to vector
sales_in_billion not recognized; using random instead.
added sales to vector
added sales to vector
added details to vector
saleprice not recognized; using random instead.
sale_amount not recognized; using random instead.
added sailors to vector
s_id not recognized; using id instead.
royal_family_id not recognized; using id instead.
royal_family_details not recognized; using random instead.
royal_family not recognized; using random instead.
added routes to vector
added delivery to vector
added routes to vector
added route to vector
added name to vector
added route to vector
added fare to vector
added round to vector
added trip to vector
added required to vector
added fare to vector
added round to vector
added trip to vector
added cost to vector
round_id not recognized; using id instead.
roomtype not recognized; using random instead.
roomnumber not recognized; using random instead.
roomname not recognized; using random instead.
roomid not recognized; using id instead.
added ref to vector
added room to vector
added types to vector
added room to vector
added type to vector
added description to vector
added rooms to vector
added room to vector
added size to vector
rom_mib not recognized; using random instead.
roller_coaster_id not recognized; using id instead.
roller_coaster not recognized; using random instead.
role_name not recognized; using random instead.
added road to vector
rnag_mhz not recognized; using random instead.
added river to vector
added river to vector
added name to vector
added river to vector
rings_points not recognized; using random instead.
added performance to vector
added score to vector
added rhythm to vector
added tempo to vector
added reviewer to vector
review_id not recognized; using id instead.
added business to vector
added review to vector
added count to vector
added revenue to vector
added products to vector
added booked to vector
added returned to vector
added yes to vector
added or to vector
added no to vector
added products to vector
added booked to vector
added returned to vector
added late to vector
added yes to vector
added or to vector
added no to vector
added bookings to vector
added returned to vector
added damaged to vector
added yes to vector
added or to vector
added no to vector
added rental to vector
added return to vector
added date to vector
added player to vector
added retro to vector
added id to vector
added results to vector
resultid not recognized; using id instead.
restypename not recognized; using random instead.
restypedescription not recognized; using random instead.
added restriction to vector
restaurant_type not recognized; using random instead.
restaurant_id not recognized; using id instead.
added documents to vector
added response to vector
added received to vector
added date to vector
added respected to vector
added song to vector
added resolution to vector
resname not recognized; using random instead.
residents_services not recognized; using random instead.
residents_per_officer not recognized; using random instead.
added residents to vector
added residence to vector
added reserves to vector
added reservations to vector
research_staff not recognized; using random instead.
research_point not recognized; using random instead.
research_outcomes not recognized; using random instead.
reputation_point not recognized; using random instead.
representative_name not recognized; using random instead.
added representative to vector
reportsto not recognized; using random instead.
added employees to vector
added reports to vector
added to to vector
added problems to vector
added reported to vector
added by to vector
added staff to vector
added id to vector
added report to vector
added film to vector
added replacement to vector
added cost to vector
added student to vector
added record to vector
added repeat to vector
added term to vector
repair_assignment not recognized; using random instead.
added repair to vector
renting_history not recognized; using random instead.
added film to vector
added rental to vector
added rate to vector
added film to vector
added rental to vector
added duration to vector
added rental to vector
added rental to vector
added date to vector
added rental to vector
rent_arrears not recognized; using random instead.
added remarks to vector
added song to vector
releasedate not recognized; using random instead.
added release to vector
added relationship to vector
related_document_object_id not recognized; using id instead.
related_collection_id not recognized; using id instead.
added reign to vector
regular_orders not recognized; using random instead.
regular_order_products not recognized; using random instead.
regoin not recognized; using random instead.
added registered to vector
register_year not recognized; using random instead.
added regions to vector
regional_population not recognized; using random instead.
region_code not recognized; using random instead.
added publication to vector
added reference to vector
added num to vector
ref_user_categories not recognized; using random instead.
ref_transaction_types not recognized; using random instead.
ref_template_types not recognized; using random instead.
ref_staff_roles not recognized; using random instead.
ref_shipping_agents not recognized; using random instead.
ref_service_types not recognized; using random instead.
ref_room_types not recognized; using random instead.
ref_progress_status not recognized; using random instead.
ref_product_categories not recognized; using random instead.
ref_payment_methods not recognized; using random instead.
ref_locations not recognized; using random instead.
ref_incident_type not recognized; using random instead.
ref_hotel_star_ratings not recognized; using random instead.
ref_feature_types not recognized; using random instead.
ref_event_types not recognized; using random instead.
ref_document_status not recognized; using random instead.
ref_colors not recognized; using random instead.
ref_characteristic_types not recognized; using random instead.
ref_calendar not recognized; using random instead.
ref_budget_codes not recognized; using random instead.
ref_attraction_types not recognized; using random instead.
ref_assignment_status not recognized; using random instead.
ref_age_categories not recognized; using random instead.
ref_achievement_type not recognized; using random instead.
added fault to vector
added log to vector
added recorded to vector
added by to vector
added staff to vector
added id to vector
record_id not recognized; using id instead.
record_company not recognized; using random instead.
added behavior to vector
added incident to vector
added recommendations to vector
added recipient to vector
added receipts to vector
receiptnumber not recognized; using random instead.
added documents to vector
added receipt to vector
added number to vector
added documents to vector
added receipt to vector
added date to vector
added receipt to vector
readers_in_million not recognized; using random instead.
added player to vector
added attributes to vector
added reactions to vector
ratingdate not recognized; using random instead.
rating_in_percent not recognized; using random instead.
added rate to vector
added rankings to vector
added rankings to vector
added ranking to vector
added points to vector
added rankings to vector
added ranking to vector
added date to vector
rank_position not recognized; using random instead.
rank_in_series not recognized; using random instead.
rank_in_round not recognized; using random instead.
added aircraft to vector
added range to vector
added miles to vector
ram_mib not recognized; using random instead.
railway_manage not recognized; using random instead.
radio_mhz not recognized; using random instead.
added radio to vector
racing_series not recognized; using random instead.
added races to vector
race_name not recognized; using random instead.
race_id not recognized; using id instead.
added team to vector
added ra to vector
added matches to vector
queueid not recognized; using id instead.
questions_in_exams not recognized; using random instead.
added questions to vector
question_text not recognized; using random instead.
added machine to vector
added quality to vector
added rank to vector
added qualifying to vector
qualifyid not recognized; using id instead.
added candidate to vector
added assessments to vector
added qualification to vector
added quadra to vector
added kills to vector
added circuits to vector
added url to vector
added circuits to vector
added altitude to vector
added circuits to vector
added longitude to vector
added meetings to vector
added purpose to vector
added of to vector
added meeting to vector
added purchases to vector
purchaseprice not recognized; using random instead.
added cyclists to vector
added own to vector
added bikes to vector
added purchase to vector
added year to vector
added purchases to vector
added purchase to vector
added transaction to vector
added id to vector
added purchases to vector
added purchase to vector
added details to vector
added purchase to vector
publication_keyword not recognized; using random instead.
publication_id not recognized; using id instead.
publication_date not recognized; using random instead.
added province to vector
added provides to vector
added providers to vector
added provider to vector
added protein to vector
added protein to vector
added name to vector
added protein to vector
added aircraft to vector
added propulsion to vector
property_photos not recognized; using random instead.
property_features not recognized; using random instead.
added properties to vector
added property to vector
added description to vector
added properties to vector
added property to vector
added address to vector
added id to vector
project_staff not recognized; using random instead.
project_outcomes not recognized; using random instead.
added ref to vector
added progress to vector
added status to vector
added progress to vector
added status to vector
added description to vector
added assignment to vector
added progress to vector
added progress to vector
added report to vector
added date to vector
program_requirement not recognized; using random instead.
program_course not recognized; using random instead.
profits_in_billion not recognized; using random instead.
added professor to vector
added professionals to vector
prof_office not recognized; using random instead.
prof_high_degree not recognized; using random instead.
prof_extension not recognized; using random instead.
products_in_events not recognized; using random instead.
products_for_hire not recognized; using random instead.
products_booked not recognized; using random instead.
added product to vector
added categories to vector
added product to vector
added type to vector
added description to vector
added invoice to vector
added line to vector
added items to vector
added product to vector
added title to vector
product_suppliers not recognized; using random instead.
added catalog to vector
added contents to vector
added product to vector
added stock to vector
added number to vector
product_in_event_id not recognized; using id instead.
product_characteristics not recognized; using random instead.
added product to vector
added characteristics to vector
added product to vector
added characteristic to vector
added value to vector
added reference to vector
added product to vector
added categories to vector
added product to vector
added category to vector
added description to vector
product_categories not recognized; using random instead.
added producer to vector
added process to vector
added status to vector
added process to vector
added status to vector
added description to vector
process_status not recognized; using random instead.
process_outcomes not recognized; using random instead.
added process to vector
added outcomes to vector
added process to vector
added outcome to vector
added description to vector
added business to vector
added processes to vector
added process to vector
added name to vector
added business to vector
added processes to vector
added process to vector
added description to vector
added problems to vector
added problem to vector
added status to vector
added codes to vector
added problem to vector
added status to vector
added description to vector
problem_status_codes not recognized; using random instead.
added problem to vector
added log to vector
added problem to vector
added log to vector
added id to vector
problem_log not recognized; using random instead.
added problems to vector
added problem to vector
added description to vector
added problem to vector
added category to vector
added codes to vector
added problem to vector
added category to vector
added description to vector
problem_category_codes not recognized; using random instead.
principal_activities not recognized; using random instead.
primaryaffiliation not recognized; using random instead.
added hotels to vector
added price to vector
added range to vector
added properties to vector
added price to vector
added min to vector
added properties to vector
added price to vector
added max to vector
added catalog to vector
added contents to vector
added price to vector
added in to vector
added pounds to vector
added catalog to vector
added contents to vector
added price to vector
added in to vector
added euros to vector
added catalog to vector
added contents to vector
added price to vector
added in to vector
added dollars to vector
price_in_dollar not recognized; using random instead.
added financial to vector
added transactions to vector
added previous to vector
added transaction to vector
added id to vector
added catalog to vector
added contents to vector
added previous to vector
added entry to vector
added id to vector
added aircraft to vector
added pressurized to vector
added press to vector
president_vote not recognized; using random instead.
added prescribes to vector
added prerequisite to vector
added prerequisite to vector
added id to vector
prereq not recognized; using random instead.
prepnurse not recognized; using random instead.
added fare to vector
added basis to vector
added premium to vector
added premises to vector
added premises to vector
added type to vector
added premises to vector
added premises to vector
added premise to vector
added details to vector
added artist to vector
added preferred to vector
added genre to vector
added player to vector
added attributes to vector
added preferred to vector
added foot to vector
added preferences to vector
added student to vector
added predicted to vector
added graduation to vector
added semester to vector
added weather to vector
added precipitation to vector
added inches to vector
added weekly to vector
added weather to vector
added precipitation to vector
added course to vector
added prerequisite to vector
added pre to vector
added course to vector
added id to vector
added ppos to vector
added team to vector
added ppf to vector
added powertrain to vector
added power to vector
added player to vector
added attributes to vector
added potential to vector
added postseason to vector
positionorder not recognized; using random instead.
added player to vector
added attributes to vector
added positioning to vector
added porphyria to vector
added participants to vector
added popularity to vector
pop_quiz not recognized; using random instead.
pommel_horse_points not recognized; using random instead.
poll_source not recognized; using random instead.
added countries to vector
added politics to vector
added score to vector
added policies to vector
police_officers not recognized; using random instead.
police_force not recognized; using random instead.
pole_position not recognized; using random instead.
poker_player_id not recognized; using id instead.
poker_player not recognized; using random instead.
pnumber not recognized; using random instead.
added pno to vector
added manager to vector
plyr not recognized; using random instead.
added mgr to vector
plocation not recognized; using random instead.
plays_games not recognized; using random instead.
playlisttrack not recognized; using random instead.
added playlists to vector
playlist_tracks not recognized; using random instead.
added playlist to vector
added tracks to vector
added playlist to vector
added id to vector
added playlist to vector
added players to vector
player_college not recognized; using random instead.
player_coach not recognized; using random instead.
player_award_vote not recognized; using random instead.
player_award not recognized; using random instead.
player_attributes not recognized; using random instead.
added matches to vector
platformid not recognized; using id instead.
platform_name not recognized; using random instead.
added platform to vector
planned_destruction_date not recognized; using random instead.
planetid not recognized; using id instead.
added place to vector
added pixels to vector
pixel_aspect_ratio_par not recognized; using random instead.
added pitstops to vector
pitching_postseason not recognized; using random instead.
added pitching to vector
pinksbought not recognized; using random instead.
pilotskills not recognized; using random instead.
pilot_record not recognized; using random instead.
added pigs to vector
added pieces to vector
added piece to vector
added staff to vector
added picture to vector
physicaldmgdealt not recognized; using random instead.
physdmgtochamp not recognized; using random instead.
physdmgtaken not recognized; using random instead.
added property to vector
added photos to vector
added photo to vector
added title to vector
added property to vector
added photos to vector
added photo to vector
added seq to vector
photo_id not recognized; using id instead.
added property to vector
added photos to vector
added photo to vector
added filename to vector
added property to vector
added photos to vector
added photo to vector
added description to vector
phone_market not recognized; using random instead.
pettype not recognized; using random instead.
added hotels to vector
added pets to vector
added allowed to vector
added yn to vector
added pets to vector
added pets to vector
added pet to vector
added age to vector
personfriend not recognized; using random instead.
added people to vector
added addresses to vector
added person to vector
added address to vector
added id to vector
added person to vector
perpetrator_id not recognized; using id instead.
added perpetrator to vector
added students to vector
added permanent to vector
added address to vector
added id to vector
added time to vector
added interval to vector
added period to vector
performers_in_bookings not recognized; using random instead.
added performers to vector
performance_score not recognized; using random instead.
added percentage to vector
people_addresses not recognized; using random instead.
pentakills not recognized; using random instead.
added player to vector
added attributes to vector
added penalties to vector
added pcp to vector
added payments to vector
added payment to vector
added type to vector
added code to vector
added reference to vector
added payment to vector
added methods to vector
added payment to vector
added method to vector
added description to vector
added payments to vector
added payment to vector
added details to vector
added payment to vector
pay_per_view_ppv not recognized; using random instead.
added aircraft to vector
added pay to vector
added load to vector
pattern_recognition not recognized; using random instead.
party_theme not recognized; using random instead.
party_services not recognized; using random instead.
added parties to vector
added party to vector
added phone to vector
party_name not recognized; using random instead.
party_host not recognized; using random instead.
party_forms not recognized; using random instead.
party_events not recognized; using random instead.
added parties to vector
added party to vector
added email to vector
party_details not recognized; using random instead.
party_addresses not recognized; using random instead.
added parts to vector
added user to vector
added profiles to vector
added partition to vector
added id to vector
parties_in_events not recognized; using random instead.
added participation to vector
participates_in not recognized; using random instead.
participants_in_events not recognized; using random instead.
participant_type_code not recognized; using random instead.
participant_details not recognized; using random instead.
added parts to vector
added part to vector
added name to vector
part_faults not recognized; using random instead.
added properties to vector
added parking to vector
added lots to vector
parking_fines not recognized; using random instead.
added parking to vector
added park to vector
added park to vector
added name to vector
added park to vector
added park to vector
added alias to vector
parent_service_type_code not recognized; using random instead.
added organizations to vector
added parent to vector
added organization to vector
added id to vector
added functional to vector
added areas to vector
added parent to vector
added functional to vector
added area to vector
added code to vector
added catalog to vector
added contents to vector
added parent to vector
added entry to vector
added id to vector
added document to vector
added structures to vector
added parent to vector
added document to vector
added structure to vector
added code to vector
parent_document_object_id not recognized; using id instead.
parent_collection_id not recognized; using id instead.
parallel_bars_points not recognized; using random instead.
added paragraphs to vector
paragraph_text not recognized; using random instead.
paragraph_id not recognized; using id instead.
added papers to vector
paperkeyphrase not recognized; using random instead.
paperdataset not recognized; using random instead.
added paintings to vector
paintingid not recognized; using id instead.
painterid not recognized; using id instead.
added product to vector
added pages to vector
added per to vector
added minute to vector
added color to vector
added pages to vector
packagenumber not recognized; using random instead.
package_version not recognized; using random instead.
package_option not recognized; using random instead.
added package to vector
added oxen to vector
ownjunglekills not recognized; using random instead.
added owners to vector
added properties to vector
added owner to vector
added user to vector
added id to vector
owned_since not recognized; using random instead.
added countries to vector
added overall to vector
added score to vector
added player to vector
added attributes to vector
added overall to vector
added rating to vector
added project to vector
added outcomes to vector
added outcome to vector
added details to vector
added research to vector
added outcomes to vector
added outcome to vector
added description to vector
added engineer to vector
added visits to vector
added other to vector
added visit to vector
added details to vector
added users to vector
added other to vector
added user to vector
added details to vector
added shipments to vector
added other to vector
added shipment to vector
added details to vector
added delivery to vector
added routes to vector
added other to vector
added route to vector
added details to vector
added rooms to vector
added other to vector
added room to vector
added details to vector
other_property_features not recognized; using random instead.
added assignment to vector
added progress to vector
added other to vector
added progress to vector
added details to vector
added products to vector
added other to vector
added product to vector
added details to vector
added problems to vector
added other to vector
added problem to vector
added details to vector
added student to vector
added course to vector
added assignments to vector
added other to vector
added placement to vector
added details to vector
added parts to vector
added other to vector
added part to vector
added details to vector
added order to vector
added items to vector
added other to vector
added order to vector
added item to vector
added details to vector
added problem to vector
added log to vector
added other to vector
added log to vector
added details to vector
added hotels to vector
added other to vector
added hotel to vector
added details to vector
added courses to vector
added scheduled to vector
added other to vector
added course to vector
added schedule to vector
added details to vector
added courses to vector
added offered to vector
added other to vector
added course to vector
added offering to vector
added details to vector
added maintenance to vector
added contracts to vector
added other to vector
added contract to vector
added details to vector
added third to vector
added party to vector
added companies to vector
added other to vector
added company to vector
added details to vector
added characteristics to vector
added other to vector
added characteristic to vector
added details to vector
added customers to vector
added cards to vector
added other to vector
added card to vector
added details to vector
other_available_features not recognized; using random instead.
added assets to vector
added other to vector
added asset to vector
added details to vector
added properties to vector
added oth to vector
added feature to vector
added 3 to vector
added properties to vector
added oth to vector
added feature to vector
added 2 to vector
added properties to vector
added oth to vector
added feature to vector
added 1 to vector
added film to vector
added original to vector
added language to vector
added id to vector
added songs to vector
added original to vector
added artist to vector
original_airdate not recognized; using random instead.
organized_by not recognized; using random instead.
added organizations to vector
added organization to vector
added name to vector
added organizations to vector
added organization to vector
added details to vector
organization_contact_individuals not recognized; using random instead.
added organization to vector
added organisations to vector
organisation_types not recognized; using random instead.
added organisation to vector
added types to vector
added organisation to vector
added type to vector
added description to vector
added organisations to vector
added organisation to vector
added details to vector
added ordinal to vector
order_year not recognized; using random instead.
added customer to vector
added orders to vector
added order to vector
added shipping to vector
added charges to vector
added customer to vector
added orders to vector
added order to vector
added placed to vector
added order to vector
added items to vector
added order to vector
added item to vector
added status to vector
added order to vector
added items to vector
added order to vector
added item to vector
added details to vector
order_deliveries not recognized; using random instead.
added customer to vector
added orders to vector
added order to vector
added delivered to vector
added or to vector
oppose_rate not recognized; using random instead.
operating_system not recognized; using random instead.
operate_company not recognized; using random instead.
openning_year not recognized; using random instead.
added home to vector
added game to vector
added openings to vector
opening_hours not recognized; using random instead.
added opened to vector
onscholarship not recognized; using random instead.
added fare to vector
added one to vector
added direction to vector
added cost to vector
oncallstart not recognized; using random instead.
oncallend not recognized; using random instead.
on_call not recognized; using random instead.
added on to vector
added omim to vector
official_ratings_(millions) not recognized; using random instead.
official_name not recognized; using random instead.
official_languages not recognized; using random instead.
office_locations not recognized; using random instead.
offering_instructor_id not recognized; using id instead.
offering_instructor not recognized; using random instead.
added oct to vector
added occupation to vector
objectnumber not recognized; using random instead.
ny_phil not recognized; using random instead.
numciting not recognized; using random instead.
numcitedby not recognized; using random instead.
numcc not recognized; using random instead.
number_team not recognized; using random instead.
number_products not recognized; using random instead.
number_of_stories not recognized; using random instead.
number_of_product_category not recognized; using random instead.
number_of_platforms not recognized; using random instead.
number_of_matches not recognized; using random instead.
number_of_hosts not recognized; using random instead.
number_of_championships not recognized; using random instead.
number_in_season not recognized; using random instead.
number_deaths not recognized; using random instead.
number_city_affected not recognized; using random instead.
number_cities not recognized; using random instead.
num_semesters not recognized; using random instead.
num_reviews not recognized; using random instead.
num_of_ticket not recognized; using random instead.
num_of_stock not recognized; using random instead.
num_of_shaff_in_charge not recognized; using random instead.
added tv to vector
added series to vector
added num to vector
added of to vector
added seasons to vector
num_of_pieces not recognized; using random instead.
num_of_factories not recognized; using random instead.
num_of_employees not recognized; using random instead.
num_of_component not recognized; using random instead.
num_enrolled not recognized; using random instead.
num_employees not recognized; using random instead.
added nov to vector
added assessment to vector
added notes to vector
added notes to vector
added id to vector
added not to vector
added nominee to vector
added nomination to vector
no_skip not recognized; using random instead.
added customer to vector
added number to vector
added of to vector
added loans to vector
added bank to vector
added no to vector
added of to vector
added customers to vector
added restriction to vector
added no to vector
added discounts to vector
added fare to vector
added basis to vector
added night to vector
next_show_name not recognized; using random instead.
added business to vector
added processes to vector
added next to vector
added process to vector
added id to vector
added catalog to vector
added contents to vector
added next to vector
added entry to vector
added id to vector
next_claim_stage_id not recognized; using id instead.
news_report not recognized; using random instead.
neutralminionskilled not recognized; using random instead.
added station to vector
added network to vector
added name to vector
net_worth_millions not recognized; using random instead.
added neighbourhood to vector
added neighbourhood to vector
added name to vector
added neighbourhood to vector
added neighborhood to vector
added negative to vector
added hall to vector
added of to vector
added fame to vector
added needed to vector
added note to vector
added hall to vector
added of to vector
added fame to vector
added needed to vector
added cyclist to vector
added nation to vector
added player to vector
added name to vector
added last to vector
added player to vector
added name to vector
added given to vector
added college to vector
added name to vector
added full to vector
added player to vector
added name to vector
added first to vector
added team to vector
added franchise to vector
added na to vector
added assoc to vector
musictype not recognized; using random instead.
added musical to vector
added museums to vector
museum_details not recognized; using random instead.
added museum to vector
added multiracial to vector
added mpg to vector
movietheaters not recognized; using random instead.
added movies to vector
added office to vector
added locations to vector
added move to vector
added in to vector
added year to vector
added mountain to vector
added mountain to vector
added name to vector
added mountain to vector
added mountain to vector
added altitude to vector
added genre to vector
added most to vector
added popular to vector
added in to vector
added student to vector
added addresses to vector
added monthly to vector
added rental to vector
month_profits_billion not recognized; using random instead.
added month to vector
added month to vector
added name to vector
money_requested not recognized; using random instead.
money_rank not recognized; using random instead.
added monday to vector
modelid not recognized; using id instead.
model_year not recognized; using random instead.
model_name not recognized; using random instead.
model_list not recognized; using random instead.
mission_id not recognized; using id instead.
added mission to vector
added airport to vector
added service to vector
added minutes to vector
added distant to vector
added matches to vector
added minutes to vector
minor_in not recognized; using random instead.
added student to vector
added minor to vector
added minit to vector
added minister to vector
added restriction to vector
added minimum to vector
added stay to vector
added airport to vector
added minimum to vector
added connect to vector
added time to vector
added weather to vector
added min to vector
added visibility to vector
added miles to vector
added weather to vector
added min to vector
added temperature to vector
added f to vector
added weather to vector
added min to vector
added sea to vector
added level to vector
added pressure to vector
added inches to vector
min_salary not recognized; using random instead.
added weather to vector
added min to vector
added humidity to vector
added weather to vector
added min to vector
added dew to vector
added point to vector
added f to vector
added program to vector
added requirement to vector
added min to vector
added credit to vector
added min to vector
added mill to vector
added airport to vector
added service to vector
added miles to vector
added distant to vector
mgr_start_date not recognized; using random instead.
mgr_ssn not recognized; using random instead.
added swimmer to vector
added meter to vector
added 700 to vector
added swimmer to vector
added meter to vector
added 600 to vector
added swimmer to vector
added meter to vector
added 500 to vector
added swimmer to vector
added meter to vector
added 400 to vector
added swimmer to vector
added meter to vector
added 300 to vector
added swimmer to vector
added meter to vector
added 200 to vector
added swimmer to vector
added meter to vector
added 100 to vector
added menu to vector
memory_in_g not recognized; using random instead.
membership_register_branch not recognized; using random instead.
membership_card not recognized; using random instead.
added branch to vector
added membership to vector
added amount to vector
member_of_club not recognized; using random instead.
member_of not recognized; using random instead.
member_name not recognized; using random instead.
member_in_charge_id not recognized; using id instead.
member_attendance not recognized; using random instead.
added meetings to vector
added meetings to vector
added meeting to vector
added type to vector
added meetings to vector
added meeting to vector
added outcome to vector
mediumon not recognized; using random instead.
added medicine to vector
added enzyme to vector
added interaction to vector
added medicine to vector
added id to vector
medicine_enzyme_interaction not recognized; using random instead.
added medicine to vector
mediatype not recognized; using random instead.
media_types not recognized; using random instead.
added tracks to vector
added media to vector
added type to vector
added id to vector
added weather to vector
added mean to vector
added wind to vector
added speed to vector
added mph to vector
added weather to vector
added mean to vector
added visibility to vector
added miles to vector
added weather to vector
added mean to vector
added temperature to vector
added f to vector
added weather to vector
added mean to vector
added sea to vector
added level to vector
added pressure to vector
added inches to vector
added weather to vector
added mean to vector
added humidity to vector
added weather to vector
added mean to vector
added dew to vector
added point to vector
added f to vector
added food to vector
added service to vector
added meal to vector
added number to vector
added food to vector
added service to vector
added meal to vector
added description to vector
maxoccupancy not recognized; using random instead.
added restriction to vector
added maximum to vector
added stay to vector
max_wind_speed_mph not recognized; using random instead.
added weather to vector
added max to vector
added visibility to vector
added miles to vector
added weather to vector
added max to vector
added temperature to vector
added f to vector
max_team_number not recognized; using random instead.
max_speed not recognized; using random instead.
added weather to vector
added max to vector
added sea to vector
added level to vector
added pressure to vector
added inches to vector
max_salary not recognized; using random instead.
added product to vector
added max to vector
added page to vector
added size to vector
added weather to vector
added max to vector
added humidity to vector
added weather to vector
added max to vector
added gust to vector
added speed to vector
added mph to vector
max_gross_weight not recognized; using random instead.
max_disk_loading not recognized; using random instead.
added weather to vector
added max to vector
added dew to vector
added point to vector
added f to vector
added camera to vector
added lens to vector
added max to vector
added aperture to vector
added max to vector
added bike to vector
added material to vector
match_season not recognized; using random instead.
match_result not recognized; using random instead.
added matches to vector
added match to vector
added num to vector
added mascot to vector
added player to vector
added attributes to vector
added marking to vector
marketing_regions not recognized; using random instead.
marketing_region_name not recognized; using random instead.
marketing_region_descriptrion not recognized; using random instead.
market_value_in_billion not recognized; using random instead.
market_value_billion not recognized; using random instead.
market_value not recognized; using random instead.
added browser to vector
added market to vector
added share to vector
market_rate not recognized; using random instead.
market_district not recognized; using random instead.
market_details not recognized; using random instead.
added mar to vector
added screen to vector
added mode to vector
added map to vector
added manufacturers to vector
added store to vector
added manager to vector
added staff to vector
added id to vector
manager_half not recognized; using random instead.
manager_award_vote not recognized; using random instead.
manager_award not recognized; using random instead.
added management to vector
male_id not recognized; using id instead.
making_year not recognized; using random instead.
makeid not recognized; using id instead.
major_record_format not recognized; using random instead.
major_ranking not recognized; using random instead.
major_name not recognized; using random instead.
major_code not recognized; using random instead.
maintenance_engineers not recognized; using random instead.
maintenance_contracts not recognized; using random instead.
added maintenance to vector
added contracts to vector
added maintenance to vector
added contract to vector
added company to vector
added id to vector
main_services not recognized; using random instead.
main_industry not recognized; using random instead.
added dual to vector
added carrier to vector
added main to vector
added airline to vector
added campaigns to vector
added start to vector
added date to vector
added campaigns to vector
added name to vector
added campaigns to vector
added end to vector
added date to vector
mailshot_customers not recognized; using random instead.
added customers to vector
added customer to vector
added date to vector
mailshot_campaigns not recognized; using random instead.
added documents to vector
added mailed to vector
added mailing to vector
added date to vector
added documents to vector
added mailed to vector
added mailed to vector
added to to vector
added address to vector
added id to vector
magicdmgtochamp not recognized; using random instead.
magicdmgtaken not recognized; using random instead.
magicdmgdealt not recognized; using random instead.
made_by not recognized; using random instead.
machine_series not recognized; using random instead.
added machine to vector
lyric_fm_mhz not recognized; using random instead.
highlow not recognized; using random instead.
added lowest to vector
added point to vector
added lowest to vector
added elevation to vector
added lowest to vector
added weekly to vector
added weather to vector
added low to vector
added temperature to vector
added dual to vector
added carrier to vector
added low to vector
added flight to vector
added number to vector
low_estimate not recognized; using random instead.
added loves to vector
added lots to vector
added lots to vector
added lot to vector
added details to vector
added ship to vector
added lost to vector
added in to vector
added battle to vector
added postseason to vector
added losses to vector
added matches to vector
added loser to vector
added seed to vector
added matches to vector
added loser to vector
added rank to vector
added points to vector
added matches to vector
added loser to vector
added rank to vector
added matches to vector
added loser to vector
added name to vector
added matches to vector
added loser to vector
added ioc to vector
added matches to vector
added loser to vector
added id to vector
added matches to vector
added loser to vector
added ht to vector
added matches to vector
added loser to vector
added hand to vector
added matches to vector
added loser to vector
added entry to vector
added matches to vector
added loser to vector
added age to vector
added longest to vector
added time to vector
added spent to vector
added living to vector
added player to vector
added attributes to vector
added long to vector
added shots to vector
added player to vector
added attributes to vector
added long to vector
added passing to vector
long_lectures not recognized; using random instead.
added station to vector
added longitude to vector
added problem to vector
added log to vector
added log to vector
added entry to vector
added fix to vector
added problem to vector
added log to vector
added log to vector
added entry to vector
added description to vector
added problem to vector
added log to vector
added log to vector
added entry to vector
added date to vector
location_of_office not recognized; using random instead.
location_description not recognized; using random instead.
added delivery to vector
added route to vector
added locations to vector
added location to vector
added address to vector
added id to vector
localname not recognized; using random instead.
added station to vector
added local to vector
added authority to vector
added loan to vector
added loan to vector
added type to vector
loan_id not recognized; using id instead.
added loan to vector
added races to vector
added time to vector
lives_in not recognized; using random instead.
added list to vector
added addresses to vector
added line to vector
added 3 to vector
added area to vector
added locality to vector
added addresses to vector
added line to vector
added 2 to vector
added number to vector
added street to vector
added limit to vector
added likes to vector
added liked to vector
added id to vector
added like to vector
added lifespan to vector
lifeexpectancy not recognized; using random instead.
lieutenant_governor not recognized; using random instead.
added team to vector
added lg to vector
added win to vector
added grade to vector
added conversion to vector
added letter to vector
added grade to vector
added lessons to vector
added lessons to vector
added lesson to vector
added time to vector
added lessons to vector
added lesson to vector
added status to vector
added code to vector
added lessons to vector
added lesson to vector
added id to vector
added lessons to vector
added lesson to vector
added date to vector
added bridge to vector
added length to vector
added meters to vector
added bridge to vector
added length to vector
added feet to vector
legendarykills not recognized; using random instead.
added flight to vector
added leg to vector
added leg to vector
added number to vector
added flight to vector
added leg to vector
added leg to vector
added flight to vector
left_office not recognized; using random instead.
added postseason to vector
added league to vector
added id to vector
added winner to vector
added postseason to vector
added league to vector
added id to vector
added loser to vector
leader_name not recognized; using random instead.
launched_year not recognized; using random instead.
launch_year not recognized; using random instead.
launch_date not recognized; using random instead.
added launch to vector
added battle to vector
added latin to vector
added commander to vector
last_year not recognized; using random instead.
added largest to vector
added multi to vector
added kill to vector
added largest to vector
added killing to vector
added spree to vector
largestcrit not recognized; using random instead.
added lake to vector
added lake to vector
added name to vector
added lake to vector
added kills to vector
added killing to vector
added sprees to vector
added kids to vector
keyphrasename not recognized; using random instead.
keyphrase not recognized; using random instead.
added countries to vector
added justice to vector
added score to vector
added jun to vector
added player to vector
added attributes to vector
added jumping to vector
added jul to vector
journalname not recognized; using random instead.
added journalist to vector
journal_committee not recognized; using random instead.
added joined to vector
join_year not recognized; using random instead.
added join to vector
added jobs to vector
added staff to vector
added department to vector
added assignments to vector
added job to vector
added title to vector
added code to vector
job_history not recognized; using random instead.
added person to vector
added job to vector
added jan to vector
added items to vector
added order to vector
added items to vector
added item to vector
added status to vector
added code to vector
added order to vector
added items to vector
added item to vector
added order to vector
added quantity to vector
added order to vector
added items to vector
added item to vector
added id to vector
added order to vector
added items to vector
added item to vector
added delivered to vector
item6 not recognized; using random instead.
item5 not recognized; using random instead.
item4 not recognized; using random instead.
item3 not recognized; using random instead.
item2 not recognized; using random instead.
item1 not recognized; using random instead.
added issues to vector
issue_date not recognized; using random instead.
isofficial not recognized; using random instead.
isava not recognized; using random instead.
added users to vector
added is to vector
added seller to vector
added business to vector
added is to vector
added open to vector
is_new not recognized; using random instead.
is_main_in_charge not recognized; using random instead.
is_full_time not recognized; using random instead.
is_free not recognized; using random instead.
is_first_director not recognized; using random instead.
added users to vector
added is to vector
added buyer to vector
invoicelineid not recognized; using id instead.
invoiceline not recognized; using random instead.
invoicedate not recognized; using random instead.
added invoices to vector
added invoice to vector
added status to vector
added code to vector
added invoices to vector
added invoice to vector
added status to vector
invoice_lines not recognized; using random instead.
invoice_line_items not recognized; using random instead.
invoice_items not recognized; using random instead.
invoice_item_id not recognized; using id instead.
added invoice to vector
added investors to vector
investor_details not recognized; using random instead.
added investor to vector
added invested to vector
added inventory to vector
added program to vector
added introduction to vector
added intersect to vector
added student to vector
added internship to vector
added internet to vector
international_passengers not recognized; using random instead.
added product to vector
added interface to vector
added player to vector
added attributes to vector
added interceptions to vector
added medicine to vector
added enzyme to vector
added interaction to vector
added interaction to vector
added type to vector
integration_platform_id not recognized; using id instead.
integration_platform_details not recognized; using random instead.
integration_platform not recognized; using random instead.
insuranceid not recognized; using id instead.
added instruments to vector
added instrument to vector
institution_name not recognized; using random instead.
added station to vector
added installation to vector
added date to vector
added inst to vector
added inspirational to vector
injury_accident not recognized; using random instead.
added injury to vector
added individuals to vector
inidividual not recognized; using random instead.
added phone to vector
added hall to vector
added of to vector
added fame to vector
added inducted to vector
added individuals to vector
added individuals to vector
added individual to vector
added middle to vector
added name to vector
added individuals to vector
added individual to vector
added last to vector
added name to vector
added individuals to vector
added individual to vector
added first to vector
added name to vector
added individuals to vector
added individual to vector
added email to vector
added individuals to vector
added individual to vector
added address to vector
indepyear not recognized; using random instead.
added reference to vector
added incident to vector
added type to vector
added incident to vector
added type to vector
added description to vector
added behavior to vector
added incident to vector
added incident to vector
added summary to vector
in_office not recognized; using random instead.
added in to vector
added images to vector
added images to vector
added image to vector
added url to vector
added images to vector
added image to vector
added name to vector
added images to vector
added image to vector
added alt to vector
added text to vector
ihsaa_football_class not recognized; using random instead.
ihsaa_class not recognized; using random instead.
if_full_time not recognized; using random instead.
if_first_show not recognized; using random instead.
if_affirmative_win not recognized; using random instead.
if_active not recognized; using random instead.
idauthor not recognized; using random instead.
added works to vector
id2 not recognized; using random instead.
ice_cream not recognized; using random instead.
added properties to vector
added hse to vector
added feature to vector
added 3 to vector
added properties to vector
added hse to vector
added feature to vector
added 2 to vector
added properties to vector
added hse to vector
added feature to vector
added 1 to vector
added hs to vector
added team to vector
added hra to vector
how_to_get_there not recognized; using random instead.
added student to vector
added record to vector
added how to vector
house_number not recognized; using random instead.
hoursperweek not recognized; using random instead.
hours_played not recognized; using random instead.
added time to vector
added zone to vector
added hours to vector
added from to vector
added gmt to vector
added hotels to vector
added hotels to vector
added hotel to vector
added id to vector
added hosts to vector
hosting_city not recognized; using random instead.
host_city_id not recognized; using id instead.
host_city not recognized; using random instead.
added horsepower to vector
horizontal_bar_points not recognized; using random instead.
home_team not recognized; using random instead.
home_games not recognized; using random instead.
home_game not recognized; using random instead.
home_conference not recognized; using random instead.
home_city not recognized; using random instead.
added home to vector
added town to vector
added hispanic to vector
added hiring to vector
hiredate not recognized; using random instead.
added hilarious to vector
highway_fuel_economy_rate not recognized; using random instead.
hight_definition_tv not recognized; using random instead.
highschooler not recognized; using random instead.
highest_position not recognized; using random instead.
added highest to vector
added point to vector
added highest to vector
added elevation to vector
added highest to vector
added weekly to vector
added weather to vector
added high to vector
added temperature to vector
added dual to vector
added carrier to vector
added high to vector
added flight to vector
added number to vector
high_estimate not recognized; using random instead.
helpfulness_score not recognized; using random instead.
added paintings to vector
added height to vector
added mm to vector
height_feet not recognized; using random instead.
heavy_reading not recognized; using random instead.
heavy_papers not recognized; using random instead.
heavy_assignments not recognized; using random instead.
added cyclist to vector
added heat to vector
added countries to vector
added health to vector
added score to vector
headquartered_city not recognized; using random instead.
added headquarter to vector
added headphone to vector
headofstate not recognized; using random instead.
added player to vector
added attributes to vector
added heading to vector
added accuracy to vector
added having to vector
has_projects not recognized; using random instead.
has_pet not recognized; using random instead.
has_lab not recognized; using random instead.
has_final_project not recognized; using random instead.
has_final_exam not recognized; using random instead.
has_exams not recognized; using random instead.
has_discussion not recognized; using random instead.
has_clearance not recognized; using random instead.
has_amenity not recognized; using random instead.
has_allergy not recognized; using random instead.
banturn not recognized; using random instead.
hardware_model_name not recognized; using random instead.
hardware_colours not recognized; using random instead.
happy_hour_member not recognized; using random instead.
happy_hour not recognized; using random instead.
added hanzi to vector
hanyu_pinyin not recognized; using random instead.
added hangar to vector
added players to vector
added hand to vector
hall_of_fame not recognized; using random instead.
added team to vector
added ha to vector
gymnast_id not recognized; using id instead.
added gymnast to vector
added guests to vector
added guests to vector
added guest to vector
added last to vector
added name to vector
added guests to vector
added guest to vector
added first to vector
added name to vector
gtype not recognized; using random instead.
added gsi to vector
group_projects not recognized; using random instead.
added group to vector
ground_service not recognized; using random instead.
added ground to vector
added service to vector
added ground to vector
added fare to vector
gross_worldwide not recognized; using random instead.
gross_in_dollar not recognized; using random instead.
added results to vector
added rank to vector
added fielding to vector
added outfield to vector
added grf to vector
graphics_mode not recognized; using random instead.
added grapes to vector
added grants to vector
added grants to vector
added grant to vector
added start to vector
added date to vector
added grants to vector
added grant to vector
added end to vector
added date to vector
added grants to vector
added grant to vector
added amount to vector
graduation_college not recognized; using random instead.
added graduate to vector
gradient_from not recognized; using random instead.
added grade to vector
added conversion to vector
added grade to vector
added point to vector
gradeconversion not recognized; using random instead.
added all to vector
added star to vector
added gp to vector
added governor to vector
governmentform not recognized; using random instead.
government_website not recognized; using random instead.
added goods to vector
added customers to vector
added good to vector
added or to vector
added bad to vector
added customer to vector
good_lecture not recognized; using random instead.
good_feedback not recognized; using random instead.
goldspent not recognized; using random instead.
goldearned not recognized; using random instead.
added goals to vector
gnpold not recognized; using random instead.
added gnp to vector
gname not recognized; using random instead.
added fielding to vector
added outfield to vector
added glf to vector
added player to vector
added attributes to vector
added gk to vector
added reflexes to vector
added player to vector
added attributes to vector
added gk to vector
added positioning to vector
added player to vector
added attributes to vector
added gk to vector
added kicking to vector
added player to vector
added attributes to vector
added gk to vector
added handling to vector
added player to vector
added attributes to vector
added gk to vector
added diving to vector
added courses to vector
added scheduled to vector
added given to vector
added by to vector
added staff to vector
added id to vector
added team to vector
ghome not recognized; using random instead.
added geographic to vector
added genres to vector
added song to vector
added genre to vector
added is to vector
added tracks to vector
added genre to vector
added id to vector
added gdp to vector
added fielding to vector
added outfield to vector
added gcf to vector
gas_station not recognized; using random instead.
added properties to vector
added garage to vector
added yn to vector
gamesplayed not recognized; using random instead.
added home to vector
added game to vector
added games to vector
game_player not recognized; using random instead.
added all to vector
added star to vector
added game to vector
added num to vector
added appearances to vector
added g to vector
added ss to vector
added appearances to vector
added g to vector
added rf to vector
added appearances to vector
added g to vector
added pr to vector
added appearances to vector
added g to vector
added ph to vector
added appearances to vector
added g to vector
added p to vector
added appearances to vector
added g to vector
added of to vector
added genre to vector
added genre to vector
added name to vector
added appearances to vector
added g to vector
added lf to vector
added appearances to vector
added g to vector
added dh to vector
added appearances to vector
added g to vector
added defense to vector
added appearances to vector
added g to vector
added cf to vector
added appearances to vector
added g to vector
added c to vector
added appearances to vector
added g to vector
added batting to vector
added appearances to vector
added g to vector
added all to vector
added appearances to vector
added g to vector
added 3b to vector
added appearances to vector
added g to vector
added 2b to vector
added appearances to vector
added g to vector
added 1b to vector
furniture_manufacte not recognized; using random instead.
added furniture to vector
functional_areas not recognized; using random instead.
added functional to vector
added areas to vector
added functional to vector
added area to vector
added description to vector
fullname not recognized; using random instead.
added business to vector
added full to vector
added address to vector
fuel_propulsion not recognized; using random instead.
fte_ay not recognized; using random instead.
from_year not recognized; using random instead.
added friend to vector
added friend to vector
added id to vector
added friday to vector
freight_metric_tonnes not recognized; using random instead.
added player to vector
added attributes to vector
added free to vector
added kick to vector
added accuracy to vector
added team to vector
added franchise to vector
added franchise to vector
added name to vector
added franchise to vector
added team to vector
added fp to vector
added founder to vector
added forms to vector
added files to vector
added formats to vector
added format to vector
added forms to vector
added form to vector
added type to vector
added code to vector
added party to vector
added forms to vector
added form to vector
added status to vector
added code to vector
added forms to vector
added form to vector
added number to vector
added forms to vector
added form to vector
added name to vector
added forms to vector
added form to vector
added description to vector
added constructors to vector
added constructor to vector
added id to vector
food_type not recognized; using random instead.
food_service not recognized; using random instead.
added food to vector
added follows to vector
added user to vector
added profiles to vector
added followers to vector
added camera to vector
added lens to vector
added focal to vector
added length to vector
added mm to vector
added floors to vector
floor_exercise_points not recognized; using random instead.
added flight to vector
added flight to vector
added number to vector
flightnumber not recognized; using random instead.
flightno not recognized; using random instead.
flight_stop not recognized; using random instead.
added flight to vector
added flight to vector
added number to vector
flight_leg not recognized; using random instead.
flight_fare not recognized; using random instead.
added flight to vector
added flight to vector
added days to vector
fleet_series not recognized; using random instead.
added properties to vector
added fld to vector
added feature to vector
added 3 to vector
added properties to vector
added fld to vector
added feature to vector
added 2 to vector
added properties to vector
added fld to vector
added feature to vector
added 1 to vector
added flavor to vector
added flag to vector
firstharry not recognized; using random instead.
towerkills not recognized; using random instead.
dragonkills not recognized; using random instead.
baronkills not recognized; using random instead.
inhibkills not recognized; using random instead.
first_year not recognized; using random instead.
first_notification_of_loss not recognized; using random instead.
first_elected not recognized; using random instead.
added player to vector
added attributes to vector
added finishing to vector
added finances to vector
final_table_made not recognized; using random instead.
added player to vector
added final to vector
added game to vector
film_text not recognized; using random instead.
film_market_estimation not recognized; using random instead.
film_category not recognized; using random instead.
film_actor not recognized; using random instead.
added files to vector
added filename to vector
added files to vector
added file to vector
added size to vector
fielding_postseason not recognized; using random instead.
fielding_outfield not recognized; using random instead.
added fielding to vector
few_tests not recognized; using random instead.
festival_name not recognized; using random instead.
festival_detail not recognized; using random instead.
female_id not recognized; using id instead.
added feb to vector
added property to vector
added features to vector
added feature to vector
added value to vector
added reference to vector
added feature to vector
added types to vector
added feature to vector
added type to vector
added name to vector
feature_details not recognized; using random instead.
fda_approved not recognized; using random instead.
added part to vector
added faults to vector
added fault to vector
added short to vector
added name to vector
fault_log_parts not recognized; using random instead.
added fault to vector
added log to vector
added fault to vector
added log to vector
added entry to vector
fault_log not recognized; using random instead.
added fate to vector
fastestlaptime not recognized; using random instead.
fastestlapspeed not recognized; using random instead.
fastestlap not recognized; using random instead.
fastest_qualifying not recognized; using random instead.
fastest_lap not recognized; using random instead.
farm_competition not recognized; using random instead.
added farm to vector
fare_basis not recognized; using random instead.
added fare to vector
added fare to vector
added airline to vector
added fare to vector
famous_title not recognized; using random instead.
famous_release_date not recognized; using random instead.
faculty_participates_in not recognized; using random instead.
added apartment to vector
added facilities to vector
added facility to vector
added code to vector
added follows to vector
added follower to vector
added id to vector
added follows to vector
added user to vector
added id to vector
extra_credit not recognized; using random instead.
exhibition_record not recognized; using random instead.
added exhibition to vector
added except to vector
added exams to vector
examinationroom not recognized; using random instead.
exam_name not recognized; using random instead.
exam_date not recognized; using random instead.
events_number not recognized; using random instead.
added reference to vector
added event to vector
added types to vector
added event to vector
added type to vector
added description to vector
event_name not recognized; using random instead.
event_details not recognized; using random instead.
added student to vector
added events to vector
added event to vector
added date to vector
event_attendance not recognized; using random instead.
added evaluation to vector
estimation_id not recognized; using id instead.
erp_kw not recognized; using random instead.
equipment_sequence not recognized; using random instead.
added equipment to vector
added episode to vector
added medicine to vector
added enzyme to vector
added interaction to vector
added enzyme to vector
added id to vector
added enzyme to vector
entrepreneur_id not recognized; using id instead.
added entrepreneur to vector
added entrant to vector
added student to vector
added entered to vector
added as to vector
added enrollments to vector
enrolled_in not recognized; using random instead.
enroll_grade not recognized; using random instead.
added enroll to vector
added college to vector
added enrollment to vector
added songs to vector
added english to vector
added translation to vector
added aircraft to vector
added engines to vector
engineer_visits not recognized; using random instead.
added engineer to vector
added visits to vector
added engineer to vector
added visit to vector
added id to vector
engineer_skills not recognized; using random instead.
added engine to vector
enforced_requirement not recognized; using random instead.
enemyjunglekills not recognized; using random instead.
added endowment to vector
added endowment to vector
added id to vector
added trip to vector
added end to vector
added station to vector
added name to vector
added trip to vector
added end to vector
added station to vector
added id to vector
added time to vector
added slot to vector
added end to vector
added minute to vector
added time to vector
added slot to vector
added end to vector
added hour to vector
added meetings to vector
added end to vector
added date to vector
added time to vector
added end to vector
added employment to vector
added research to vector
added staff to vector
added employer to vector
added organisation to vector
added id to vector
added employees to vector
added employee to vector
added phone to vector
added employees to vector
added employee to vector
added address to vector
added id to vector
emp_lname not recognized; using random instead.
emp_jobcode not recognized; using random instead.
emp_initial not recognized; using random instead.
emp_hiredate not recognized; using random instead.
emp_fname not recognized; using random instead.
emp_dob not recognized; using random instead.
email_adress not recognized; using random instead.
elimination_move not recognized; using random instead.
elimination_id not recognized; using id instead.
added elimination to vector
eliminated_by not recognized; using random instead.
added airports to vector
added elevation to vector
added electoral to vector
added register to vector
added electoral to vector
added register to vector
added id to vector
electoral_register not recognized; using random instead.
election_cycle not recognized; using random instead.
added eg to vector
added agree to vector
added objectives to vector
added countries to vector
added education to vector
added score to vector
added editor to vector
edispl not recognized; using random instead.
added fare to vector
added basis to vector
added economy to vector
added countries to vector
added economics to vector
added score to vector
easiness_score not recognized; using random instead.
earpads not recognized; using random instead.
added student to vector
added record to vector
added earn to vector
added credit to vector
added dual to vector
added carrier to vector
added dual to vector
added airline to vector
added routes to vector
added destination to vector
added airport to vector
added id to vector
added routes to vector
added destination to vector
added airport to vector
added flights to vector
added destination to vector
driverstandingsid not recognized; using id instead.
driverstandings not recognized; using random instead.
added drivers to vector
driverref not recognized; using random instead.
driver_name not recognized; using random instead.
added order to vector
added deliveries to vector
added driver to vector
added employee to vector
added id to vector
driver-matched_db not recognized; using random instead.
added drink to vector
added player to vector
added attributes to vector
added dribbling to vector
added matches to vector
added draw to vector
added size to vector
drama_workshop_groups not recognized; using random instead.
championid not recognized; using id instead.
draft_pick_number not recognized; using random instead.
added document to vector
added drafts to vector
added draft to vector
added details to vector
draft_copies not recognized; using random instead.
draft_class not recognized; using random instead.
added product to vector
added dpi to vector
dphone not recognized; using random instead.
download_rank not recognized; using random instead.
doubles_wl not recognized; using random instead.
added double to vector
added kills to vector
added dose to vector
added dorm to vector
added dorm to vector
added name to vector
dorm_amenity not recognized; using random instead.
added dorm to vector
added endowment to vector
added donator to vector
added name to vector
domestic_passengers not recognized; using random instead.
domain_publication not recognized; using random instead.
domain_keyword not recognized; using random instead.
domain_journal not recognized; using random instead.
domain_conference not recognized; using random instead.
domain_author not recognized; using random instead.
added domain to vector
added dogs to vector
documents_with_expenses not recognized; using random instead.
documents_to_be_destroyed not recognized; using random instead.
documents_processes not recognized; using random instead.
documents_mailed not recognized; using random instead.
documents_in_collections not recognized; using random instead.
document_types not recognized; using random instead.
document_subsets not recognized; using random instead.
document_subset_name not recognized; using random instead.
document_subset_members not recognized; using random instead.
document_subset_details not recognized; using random instead.
document_structures not recognized; using random instead.
added document to vector
added structures to vector
added document to vector
added structure to vector
added description to vector
added reference to vector
added document to vector
added status to vector
added document to vector
added status to vector
added description to vector
document_sections_images not recognized; using random instead.
document_sections not recognized; using random instead.
document_objects not recognized; using random instead.
document_locations not recognized; using random instead.
document_functional_areas not recognized; using random instead.
document_drafts not recognized; using random instead.
document_details not recognized; using random instead.
document_date not recognized; using random instead.
added status to vector
added docks to vector
added available to vector
added station to vector
added dock to vector
added count to vector
added constructors to vector
added name to vector
dnum not recognized; using random instead.
dmgtoturrets not recognized; using random instead.
dmgtoobj not recognized; using random instead.
dmgselfmit not recognized; using random instead.
dlocation not recognized; using random instead.
added protein to vector
added divergence to vector
added from to vector
added human to vector
added lineage to vector
district_name not recognized; using random instead.
added regular to vector
added orders to vector
distributer not recognized; using random instead.
added id to vector
added distinct to vector
added ship to vector
added disposition to vector
added of to vector
added ship to vector
dish_name not recognized; using random instead.
added fare to vector
added basis to vector
added discounted to vector
added renting to vector
added history to vector
added discount to vector
added id to vector
discount_coupons not recognized; using random instead.
added discount to vector
discipline_enrollments not recognized; using random instead.
added discipline to vector
director_admin not recognized; using random instead.
added airport to vector
added service to vector
added direction to vector
direct_distance not recognized; using random instead.
added product to vector
added dimensions to vector
digital_terrestrial_channel not recognized; using random instead.
added developers to vector
destruction_authorised_by_employee_id not recognized; using id instead.
destroyed_by_employee_id not recognized; using id instead.
destairport not recognized; using random instead.
added desc to vector
added invoice to vector
added line to vector
added items to vector
added derived to vector
added vat to vector
added payable to vector
added invoice to vector
added line to vector
added items to vector
added derived to vector
added total to vector
added cost to vector
added invoice to vector
added line to vector
added items to vector
added derived to vector
added product to vector
added cost to vector
added department to vector
added store to vector
added chain to vector
added department to vector
added store to vector
added chain to vector
added name to vector
dept_locations not recognized; using random instead.
dept_extension not recognized; using random instead.
dept_address not recognized; using random instead.
dependent_name not recognized; using random instead.
added dependent to vector
added flight to vector
added stop to vector
added departure to vector
added flight to vector
added number to vector
added flight to vector
added departure to vector
added date to vector
added flight to vector
added stop to vector
added departure to vector
added airline to vector
departmentid not recognized; using id instead.
department_stores not recognized; using random instead.
department_store_chain not recognized; using random instead.
added departments to vector
added department to vector
added description to vector
density_km not recognized; using random instead.
added state to vector
added density to vector
added denomination to vector
added order to vector
added deliveries to vector
added delivery to vector
added status to vector
added code to vector
delivery_routes not recognized; using random instead.
delivery_route_locations not recognized; using random instead.
added order to vector
added deliveries to vector
added delivery to vector
added date to vector
added delegate to vector
added degree to vector
added programs to vector
added degree to vector
added summary to vector
added name to vector
added degree to vector
added programs to vector
added degree to vector
added summary to vector
added description to vector
degree_programs not recognized; using random instead.
added student to vector
added degree to vector
added player to vector
added attributes to vector
added defensive to vector
added work to vector
added rate to vector
defenceteamwidthclass not recognized; using random instead.
defenceteamwidth not recognized; using random instead.
defencepressureclass not recognized; using random instead.
defencepressure not recognized; using random instead.
defencedefenderlineclass not recognized; using random instead.
defenceaggressionclass not recognized; using random instead.
defenceaggression not recognized; using random instead.
decoration_theme not recognized; using random instead.
added rooms to vector
added decor to vector
added student to vector
added declare to vector
added major to vector
added tryout to vector
added decision to vector
added dec to vector
added player to vector
added debut to vector
debate_people not recognized; using random instead.
added debate to vector
deathyear not recognized; using random instead.
added deaths to vector
added player to vector
added death to vector
added year to vector
added player to vector
added death to vector
added state to vector
added player to vector
added death to vector
added month to vector
added player to vector
added death to vector
added day to vector
added player to vector
added death to vector
added country to vector
added player to vector
added death to vector
added city to vector
added death to vector
days_held not recognized; using random instead.
added days to vector
added days to vector
added code to vector
day_or_boarding not recognized; using random instead.
added weekly to vector
added weather to vector
added day to vector
added of to vector
added week to vector
dateundergoes not recognized; using random instead.
added customer to vector
added payments to vector
added payment to vector
added user to vector
added property to vector
added history to vector
datestamp not recognized; using random instead.
dates_active not recognized; using random instead.
dateorder not recognized; using random instead.
dateexped not recognized; using random instead.
added customers to vector
added cards to vector
added date to vector
added valid to vector
added to to vector
added customers to vector
added cards to vector
added date to vector
added valid to vector
added from to vector
added student to vector
added tests to vector
added taken to vector
added date to vector
added test to vector
added taken to vector
added product to vector
added suppliers to vector
added date to vector
added supplied to vector
added to to vector
added product to vector
added suppliers to vector
added date to vector
added supplied to vector
added from to vector
date_stored not recognized; using random instead.
added properties to vector
added date to vector
added sold to vector
added residents to vector
added services to vector
added date to vector
added requested to vector
added users to vector
added date to vector
added registered to vector
added residents to vector
added services to vector
added date to vector
added provided to vector
added products to vector
added date to vector
added product to vector
added first to vector
added available to vector
added products to vector
added date to vector
added product to vector
added discontinued to vector
added problems to vector
added date to vector
added problem to vector
added reported to vector
added problems to vector
added date to vector
added problem to vector
added closed to vector
date_payment_made not recognized; using random instead.
added properties to vector
added date to vector
added off to vector
added market to vector
added treatments to vector
added date to vector
added of to vector
added treatment to vector
added transcripts to vector
added date to vector
added of to vector
added transcript to vector
added transactions to vector
added date to vector
added of to vector
added transaction to vector
date_of_settlement not recognized; using random instead.
added students to vector
added date to vector
added of to vector
added registration to vector
added catalogs to vector
added date to vector
added of to vector
added publication to vector
added assessment to vector
added notes to vector
added date to vector
added of to vector
added notes to vector
added student to vector
added loans to vector
added date to vector
added of to vector
added loan to vector
added catalogs to vector
added date to vector
added of to vector
added latest to vector
added revision to vector
added students to vector
added date to vector
added of to vector
added latest to vector
added logon to vector
added student to vector
added course to vector
added enrolment to vector
added date to vector
added of to vector
added enrolment to vector
added student to vector
added course to vector
added enrolment to vector
added date to vector
added of to vector
added completion to vector
date_of_claim not recognized; using random instead.
date_of_ceremony not recognized; using random instead.
added student to vector
added course to vector
added attendance to vector
added date to vector
added of to vector
added attendance to vector
date_of_answer not recognized; using random instead.
added residents to vector
added date to vector
added moved to vector
added out to vector
added students to vector
added date to vector
added left to vector
added university to vector
added staff to vector
added date to vector
added left to vector
added staff to vector
added students to vector
added date to vector
added left to vector
added customers to vector
added date to vector
added last to vector
added hire to vector
added staff to vector
added date to vector
added joined to vector
added staff to vector
date_join not recognized; using random instead.
added discount to vector
added coupons to vector
added date to vector
added issued to vector
added behavior to vector
added incident to vector
added date to vector
added incident to vector
added start to vector
added behavior to vector
added incident to vector
added date to vector
added incident to vector
added end to vector
date_in_locaton_to not recognized; using random instead.
date_in_location_from not recognized; using random instead.
added party to vector
added forms to vector
added date to vector
added fully to vector
added completed to vector
added organizations to vector
added date to vector
added formed to vector
added students to vector
added date to vector
added first to vector
added rental to vector
added students to vector
added date to vector
added first to vector
added registered to vector
date_effective_to not recognized; using random instead.
date_effective_from not recognized; using random instead.
added dogs to vector
added date to vector
added departed to vector
date_day not recognized; using random instead.
added organization to vector
added contact to vector
added individuals to vector
added date to vector
added contact to vector
added to to vector
added organization to vector
added contact to vector
added individuals to vector
added date to vector
added contact to vector
added from to vector
added party to vector
added forms to vector
added date to vector
added completion to vector
added started to vector
added complaints to vector
added date to vector
added complaint to vector
added raised to vector
added complaints to vector
added date to vector
added complaint to vector
added closed to vector
date_closed not recognized; using random instead.
added staff to vector
added department to vector
added assignments to vector
added date to vector
added assigned to vector
added to to vector
added staff to vector
added department to vector
added assignments to vector
added date to vector
added assigned to vector
added from to vector
added dogs to vector
added date to vector
added arrived to vector
date_and_time not recognized; using random instead.
date_and_date not recognized; using random instead.
added dogs to vector
added date to vector
added adopted to vector
added achievements to vector
added date to vector
added achievement to vector
added accounts to vector
added date to vector
added account to vector
added opened to vector
datasetname not recognized; using random instead.
added dataset to vector
damage_millions_usd not recognized; using random instead.
added products to vector
added for to vector
added hire to vector
added daily to vector
added hire to vector
added cost to vector
added cylinders to vector
cyclists_own_bikes not recognized; using random instead.
added cyclists to vector
added own to vector
added bikes to vector
added cyclist to vector
added id to vector
added cyclist to vector
customers_policies not recognized; using random instead.
customers_cards not recognized; using random instead.
customers_and_services_details not recognized; using random instead.
customers_and_services not recognized; using random instead.
added customers to vector
added customer to vector
added type to vector
added code to vector
added customers to vector
added customer to vector
added status to vector
added code to vector
customer_rating not recognized; using random instead.
customer_policy_id not recognized; using id instead.
customer_policies not recognized; using random instead.
customer_payments not recognized; using random instead.
customer_payment_methods not recognized; using random instead.
added customers to vector
added customer to vector
added password to vector
customer_order not recognized; using random instead.
customer_master_index not recognized; using random instead.
added customers to vector
added customer to vector
added login to vector
customer_interactions not recognized; using random instead.
customer_events not recognized; using random instead.
customer_event_notes not recognized; using random instead.
customer_event_note_id not recognized; using id instead.
customer_contact_channels not recognized; using random instead.
added customers to vector
added customer to vector
added code to vector
added customers to vector
added customer to vector
added address to vector
added id to vector
customer_address_history not recognized; using random instead.
added customer to vector
added customer to vector
added name to vector
added player to vector
added attributes to vector
added curve to vector
added students to vector
added current to vector
added address to vector
added id to vector
currency_code not recognized; using random instead.
culture_company not recognized; using random instead.
csu_fees not recognized; using random instead.
added aircraft to vector
added cruising to vector
added speed to vector
crs_description not recognized; using random instead.
crs_credit not recognized; using random instead.
added player to vector
added attributes to vector
added crossing to vector
crime_rate not recognized; using random instead.
added customer to vector
added credit to vector
added score to vector
added tweets to vector
added create to vector
added date to vector
created_date not recognized; using random instead.
created_by_staff_id not recognized; using id instead.
added votes to vector
added created to vector
added customer to vector
added create to vector
added date to vector
added cows to vector
courses_scheduled not recognized; using random instead.
courses_offered not recognized; using random instead.
course_tags_count not recognized; using random instead.
course_prerequisite not recognized; using random instead.
added courses to vector
added offered to vector
added course to vector
added offering to vector
added name to vector
course_offering not recognized; using random instead.
course_authors_and_tutors not recognized; using random instead.
course_arrange not recognized; using random instead.
added discount to vector
added coupons to vector
added coupon to vector
added amount to vector
added addresses to vector
added county to vector
added state to vector
added province to vector
county_public_safety not recognized; using random instead.
county_name not recognized; using random instead.
countryname not recognized; using random instead.
countrylanguage not recognized; using random instead.
countryid not recognized; using id instead.
countryabbrev not recognized; using random instead.
counties_represented not recognized; using random instead.
added bookings to vector
added count to vector
added hired to vector
council_tax not recognized; using random instead.
cost_per_25_miles not recognized; using random instead.
added treatments to vector
added cost to vector
added of to vector
added treatment to vector
added cost to vector
added copyright to vector
added coordinates to vector
added maintenance to vector
added contracts to vector
added contract to vector
added start to vector
added date to vector
added maintenance to vector
added contracts to vector
added contract to vector
added end to vector
added date to vector
continuation_of not recognized; using random instead.
added continents to vector
contid not recognized; using id instead.
added contestants to vector
added contestants to vector
added contestant to vector
added name to vector
added content to vector
added contacts to vector
added engineer to vector
added visits to vector
added contact to vector
added staff to vector
added id to vector
added contacts to vector
added contact to vector
added phone to vector
added customer to vector
added contact to vector
added channels to vector
added contact to vector
added number to vector
added contacts to vector
added contact to vector
added id to vector
constructorstandingsid not recognized; using id instead.
constructorstandings not recognized; using random instead.
added constructors to vector
constructorresultsid not recognized; using id instead.
constructorresults not recognized; using random instead.
constructorref not recognized; using random instead.
added constructor to vector
added construction to vector
consider_rate not recognized; using random instead.
added flight to vector
added connections to vector
added connection to vector
conference_participation not recognized; using random instead.
conference_name not recognized; using random instead.
added conductor to vector
added concerts to vector
concert_name not recognized; using random instead.
added concert to vector
added comptroller to vector
completed_year not recognized; using random instead.
added complaints to vector
added complaints to vector
added complaint to vector
added type to vector
added code to vector
added complaints to vector
added complaint to vector
added status to vector
added code to vector
added complaints to vector
added complaint to vector
added outcome to vector
added code to vector
added complaints to vector
added complaint to vector
added id to vector
competition_type not recognized; using random instead.
competition_result not recognized; using random instead.
competition_record not recognized; using random instead.
added accelerator to vector
added compatible to vector
added browser to vector
added compatible to vector
added since to vector
added year to vector
compartment_class not recognized; using random instead.
added third to vector
added party to vector
added companies to vector
added company to vector
added type to vector
added third to vector
added party to vector
added companies to vector
added company to vector
added address to vector
added companies to vector
added protein to vector
added common to vector
added name to vector
added committee to vector
commission_pct not recognized; using random instead.
added assignment to vector
added progress to vector
added comments to vector
added on to vector
added progress to vector
added student to vector
added course to vector
added assignments to vector
added comments to vector
added on to vector
added assignment to vector
added comments to vector
added comment to vector
added instructor to vector
added comment to vector
added text to vector
comment_instructor not recognized; using random instead.
combined_fuel_economy_rate not recognized; using random instead.
added reference to vector
added colors to vector
added color to vector
added description to vector
college_location not recognized; using random instead.
added collections to vector
collection_subsets not recognized; using random instead.
collection_subset_name not recognized; using random instead.
collection_subset_members not recognized; using random instead.
collection_name not recognized; using random instead.
collection_description not recognized; using random instead.
collecrtion_subset_details not recognized; using random instead.
added routes to vector
added code to vector
added share to vector
code_description not recognized; using random instead.
code2 not recognized; using random instead.
coach_name not recognized; using random instead.
added coach to vector
added customer to vector
added master to vector
added index to vector
added cmi to vector
added details to vector
cmi_cross_references not recognized; using random instead.
clubname not recognized; using random instead.
clublocation not recognized; using random instead.
clubdesc not recognized; using random instead.
club_rank not recognized; using random instead.
club_leader not recognized; using random instead.
club_id_2 not recognized; using random instead.
club_id_1 not recognized; using random instead.
added weather to vector
added cloud to vector
added cover to vector
added problems to vector
added closure to vector
added authorised to vector
added by to vector
added staff to vector
added id to vector
climber_id not recognized; using id instead.
added climber to vector
added clients to vector
added client to vector
added details to vector
clear_grading not recognized; using random instead.
clean_jerk not recognized; using random instead.
added classification to vector
added classes to vector
class_time not recognized; using random instead.
class_senator_vote not recognized; using random instead.
class_section not recognized; using random instead.
class_room not recognized; using random instead.
class_president_vote not recognized; using random instead.
class_of_service not recognized; using random instead.
added classes to vector
added class to vector
added id to vector
added classes to vector
added class to vector
added details to vector
added class to vector
added of to vector
added service to vector
added class to vector
added description to vector
class_address not recognized; using random instead.
class_aa not recognized; using random instead.
class_a not recognized; using random instead.
clarity_score not recognized; using random instead.
claims_processing_stages not recognized; using random instead.
claims_processing not recognized; using random instead.
claims_documents not recognized; using random instead.
claim_type_code not recognized; using random instead.
claim_status_name not recognized; using random instead.
claim_status_description not recognized; using random instead.
claim_status_code not recognized; using random instead.
claim_processing_id not recognized; using id instead.
claim_outcome_code not recognized; using random instead.
claim_headers not recognized; using random instead.
claim_header_id not recognized; using id instead.
city_town not recognized; using random instead.
city_population not recognized; using random instead.
city_fuel_economy_rate not recognized; using random instead.
city_customer not recognized; using random instead.
city_channel_tv_show not recognized; using random instead.
city_channel_radio not recognized; using random instead.
city_channel not recognized; using random instead.
city_area not recognized; using random instead.
city_agent not recognized; using random instead.
added direct to vector
added distance to vector
city2 not recognized; using random instead.
added code to vector
added direct to vector
added distance to vector
city1 not recognized; using random instead.
added code to vector
citingpaperid not recognized; using id instead.
added cite to vector
added citing to vector
citedpaperid not recognized; using id instead.
added citation to vector
added cited to vector
added paper to vector
added id to vector
added cite to vector
added cited to vector
citation_point not recognized; using random instead.
added publication to vector
added citation to vector
added num to vector
added citation to vector
circulation_history not recognized; using random instead.
added circuits to vector
circuitref not recognized; using random instead.
added cinema to vector
added church to vector
added chromosome to vector
added checkout to vector
added checking to vector
added chassis to vector
added charges to vector
added parts to vector
added chargeable to vector
added yn to vector
added parts to vector
added chargeable to vector
added amount to vector
added charges to vector
added charge to vector
added type to vector
added charges to vector
added charge to vector
added id to vector
added charges to vector
added charge to vector
added amount to vector
added characteristics to vector
added reference to vector
added characteristic to vector
added types to vector
added characteristic to vector
added type to vector
added description to vector
added characteristics to vector
added characteristic to vector
added name to vector
added characteristics to vector
added characteristic to vector
added data to vector
added type to vector
added character to vector
char_cells not recognized; using random instead.
added chapters to vector
channel_details not recognized; using random instead.
added customer to vector
added contact to vector
added channels to vector
added channel to vector
added code to vector
chancecreationshootingclass not recognized; using random instead.
chancecreationshooting not recognized; using random instead.
chancecreationpositioningclass not recognized; using random instead.
chancecreationpassingclass not recognized; using random instead.
chancecreationpassing not recognized; using random instead.
chancecreationcrossingclass not recognized; using random instead.
chancecreationcrossing not recognized; using random instead.
added champs to vector
champlvl not recognized; using random instead.
added championship to vector
chair_name not recognized; using random instead.
certificationexpires not recognized; using random instead.
certificationdate not recognized; using random instead.
added certificate to vector
census_ranking not recognized; using random instead.
added customers to vector
added cell to vector
added mobile to vector
added phone to vector
added number to vector
cell_mobile_phone not recognized; using random instead.
added death to vector
added caused to vector
added by to vector
added ship to vector
added id to vector
added category to vector
added category to vector
added name to vector
added catalogue to vector
added catalogs to vector
catalog_structure not recognized; using random instead.
added catalogs to vector
added catalog to vector
added publisher to vector
added catalogs to vector
added catalog to vector
added name to vector
added catalog to vector
added structure to vector
added catalog to vector
added level to vector
added name to vector
added catalog to vector
added contents to vector
added catalog to vector
added entry to vector
added name to vector
catalog_contents_additional_attributes not recognized; using random instead.
catalog_contents not recognized; using random instead.
added cast to vector
added cases to vector
case_burden not recognized; using random instead.
added cartoon to vector
cars_data not recognized; using random instead.
cares_for_students not recognized; using random instead.
added customers to vector
added cards to vector
added card to vector
added type to vector
added code to vector
car_owner not recognized; using random instead.
car_names not recognized; using random instead.
car_makers not recognized; using random instead.
car_# not recognized; using random instead.
captain_id not recognized; using id instead.
capacity_percentage not recognized; using random instead.
added candidates to vector
added candidates to vector
added candidate to vector
added details to vector
candidate_assessments not recognized; using random instead.
added candidate to vector
campusfee not recognized; using random instead.
added campuses to vector
added photos to vector
added camera to vector
added lens to vector
added id to vector
camera_lens not recognized; using random instead.
added airlines to vector
added call to vector
added sign to vector
calendar_date not recognized; using random instead.
added by to vector
added properties to vector
added buyer to vector
added offered to vector
added price to vector
added business to vector
added rates to vector
added business to vector
added rates to vector
added id to vector
business_rates not recognized; using random instead.
business_processes not recognized; using random instead.
added business to vector
added bulls to vector
added battle to vector
added bulgarian to vector
added commander to vector
added built to vector
buildupplayspeedclass not recognized; using random instead.
buildupplayspeed not recognized; using random instead.
buildupplaypositioningclass not recognized; using random instead.
buildupplaypassingclass not recognized; using random instead.
buildupplaypassing not recognized; using random instead.
buildupplaydribblingclass not recognized; using random instead.
buildupplaydribbling not recognized; using random instead.
added buildings to vector
added apartment to vector
added buildings to vector
added building to vector
added short to vector
added name to vector
added apartment to vector
added buildings to vector
added building to vector
added phone to vector
added apartment to vector
added buildings to vector
added building to vector
added manager to vector
added apartment to vector
added buildings to vector
added building to vector
added full to vector
added name to vector
added apartment to vector
added buildings to vector
added building to vector
added description to vector
added apartment to vector
added buildings to vector
added building to vector
added address to vector
build_year not recognized; using random instead.
added budgeted to vector
budget_type_description not recognized; using random instead.
budget_million not recognized; using random instead.
budget_invested_percent not recognized; using random instead.
budget_in_billions not recognized; using random instead.
added accelerator to vector
added compatible to vector
added browser to vector
added browser to vector
added id to vector
added browser to vector
broadcast_share not recognized; using random instead.
added broadcast to vector
added bridge to vector
added breeds to vector
added breeds to vector
added breed to vector
added name to vector
added team to vector
added bpf to vector
boys_or_girls not recognized; using random instead.
added boxes to vector
added head to vector
added born to vector
added state to vector
border_info not recognized; using random instead.
added border to vector
added info to vector
added border to vector
books_order not recognized; using random instead.
bookings_services not recognized; using random instead.
added party to vector
added services to vector
added booking to vector
added made to vector
added date to vector
added products to vector
added booked to vector
added booked to vector
added count to vector
added products to vector
added booked to vector
added booked to vector
added amount to vector
book_title not recognized; using random instead.
book_series not recognized; using random instead.
book_club not recognized; using random instead.
added bonus to vector
body_builder_id not recognized; using id instead.
body_builder not recognized; using random instead.
added boats to vector
added bank to vector
bname not recognized; using random instead.
added bluetooth to vector
added block to vector
added black to vector
birthyear not recognized; using random instead.
added player to vector
added birthday to vector
added birthdate to vector
added player to vector
added birth to vector
added state to vector
birth_place not recognized; using random instead.
added player to vector
added birth to vector
added month to vector
added player to vector
added birth to vector
added day to vector
added player to vector
added birth to vector
added country to vector
added students to vector
added bio to vector
added data to vector
billingstate not recognized; using random instead.
billingpostalcode not recognized; using random instead.
billingcountry not recognized; using random instead.
billingcity not recognized; using random instead.
billingaddress not recognized; using random instead.
added invoices to vector
added billing to vector
added state to vector
added invoices to vector
added billing to vector
added postal to vector
added code to vector
added invoices to vector
added billing to vector
added country to vector
added invoices to vector
added billing to vector
added city to vector
added invoices to vector
added billing to vector
added address to vector
added meetings to vector
added billable to vector
added yn to vector
added status to vector
added bikes to vector
added available to vector
added bike to vector
big_silver not recognized; using random instead.
added between to vector
added matches to vector
added best to vector
added of to vector
best_finish not recognized; using random instead.
benefits_overpayments not recognized; using random instead.
added behaviour to vector
added monitoring to vector
added behaviour to vector
added monitoring to vector
added id to vector
added behaviour to vector
added monitoring to vector
added behaviour to vector
added monitoring to vector
added details to vector
behaviour_monitoring not recognized; using random instead.
behavior_incident not recognized; using random instead.
added time to vector
added interval to vector
added begin to vector
added time to vector
bedtype not recognized; using random instead.
added rooms to vector
added beds to vector
added apartments to vector
added bedroom to vector
added count to vector
added player to vector
bbref not recognized; using random instead.
added id to vector
added team to vector
added bba to vector
added battle to vector
batting_postseason not recognized; using random instead.
added batting to vector
added player to vector
added bats to vector
added apartments to vector
added bathroom to vector
added count to vector
basketball_match not recognized; using random instead.
added fare to vector
added basis to vector
added basis to vector
added days to vector
added aircraft to vector
added basic to vector
added type to vector
baseprice not recognized; using random instead.
teamid not recognized; using id instead.
added participants to vector
matchid not recognized; using id instead.
added bank to vector
bandmateid not recognized; using id instead.
added band to vector
added hall to vector
added of to vector
added fame to vector
added ballots to vector
added player to vector
added attributes to vector
added ball to vector
added control to vector
away_team not recognized; using random instead.
added award to vector
added avg to vector
average_attendance not recognized; using random instead.
added average to vector
available_policies not recognized; using random instead.
added authorship to vector
authorname not recognized; using random instead.
authorder not recognized; using random instead.
author_tutor_atb not recognized; using random instead.
author_or_editor not recognized; using random instead.
author_list not recognized; using random instead.
author_book not recognized; using random instead.
added aug to vector
added audio to vector
added catalog to vector
added contents to vector
added additional to vector
added attributes to vector
added attribute to vector
added value to vector
added attribute to vector
added definitions to vector
added attribute to vector
added name to vector
attribute_definitions not recognized; using random instead.
added attribute to vector
added definitions to vector
added attribute to vector
added data to vector
added type to vector
attraction_type_description not recognized; using random instead.
attorney_general not recognized; using random instead.
added player to vector
added attributes to vector
added attacking to vector
added work to vector
added rate to vector
added assists to vector
assistingnurse not recognized; using random instead.
added ref to vector
added assignment to vector
added status to vector
added assignment to vector
added status to vector
added description to vector
assignment_progress not recognized; using random instead.
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added due to vector
added date to vector
added student to vector
added course to vector
added assignments to vector
added assignment to vector
added completed to vector
added date to vector
assignedto not recognized; using random instead.
added problem to vector
added log to vector
added assigned to vector
added to to vector
added staff to vector
added id to vector
assets_in_events not recognized; using random instead.
assets_in_billion not recognized; using random instead.
asset_parts not recognized; using random instead.
added assets to vector
added asset to vector
added model to vector
added assets to vector
added asset to vector
added make to vector
added assets to vector
added asset to vector
added disposed to vector
added date to vector
added assets to vector
added asset to vector
added details to vector
added assets to vector
added asset to vector
added acquired to vector
added date to vector
assessment_notes not recognized; using random instead.
added candidate to vector
added assessments to vector
added assessment to vector
added date to vector
added assessment to vector
added asian to vector
added candidate to vector
added assessments to vector
asessment not recognized; using random instead.
added outcome to vector
added code to vector
added asc to vector
added as to vector
added artwork to vector
added flight to vector
added stop to vector
added arrival to vector
added flight to vector
added number to vector
added flight to vector
added arrival to vector
added date to vector
added flight to vector
added stop to vector
added arrival to vector
added airline to vector
added arrival to vector
area_size not recognized; using random instead.
area_km_2 not recognized; using random instead.
area_km not recognized; using random instead.
area_code_state not recognized; using random instead.
added area to vector
added code to vector
added state to vector
added area to vector
added code to vector
added architect to vector
added apartments to vector
added apartment to vector
added type to vector
added code to vector
added apartments to vector
added apartment to vector
added number to vector
added properties to vector
added apt to vector
added feature to vector
added 3 to vector
added properties to vector
added apt to vector
added feature to vector
added 2 to vector
added properties to vector
added apt to vector
added feature to vector
added 1 to vector
added apr to vector
appt_type not recognized; using random instead.
added apps to vector
appointmentid not recognized; using id instead.
added applications to vector
added restriction to vector
added application to vector
added appellations to vector
added appearances to vector
added airports to vector
added airport to vector
added id to vector
added apartments to vector
apartment_facilities not recognized; using random instead.
apartment_buildings not recognized; using random instead.
apartment_bookings not recognized; using random instead.
annual_interchanges not recognized; using random instead.
annual_fuel_cost not recognized; using random instead.
annual_entry_exit not recognized; using random instead.
added and to vector
analytical_layer_type_code not recognized; using random instead.
analytical_layer not recognized; using random instead.
analytical_id not recognized; using id instead.
analogue_terrestrial_channel not recognized; using random instead.
amount_piad not recognized; using random instead.
added bookings to vector
added amount to vector
added payable to vector
added payments to vector
added amount to vector
added paid to vector
added in to vector
added full to vector
added yn to vector
added payments to vector
added amount to vector
added paid to vector
added transactions to vector
added amount to vector
added of to vector
added transaction to vector
added bookings to vector
added amount to vector
added of to vector
added refund to vector
added student to vector
added loans to vector
added amount to vector
added of to vector
added loan to vector
added bookings to vector
added amount to vector
added of to vector
added discount to vector
added payments to vector
added amount to vector
added due to vector
added amerindian to vector
added dorm to vector
added amenity to vector
added amenity to vector
added name to vector
added altitude to vector
added races to vector
added url to vector
allow_audit not recognized; using random instead.
allergytype not recognized; using random instead.
allergy_type not recognized; using random instead.
all_star not recognized; using random instead.
all_road not recognized; using random instead.
all_neutral not recognized; using random instead.
all_home not recognized; using random instead.
all_games_percent not recognized; using random instead.
all_games not recognized; using random instead.
all_documents not recognized; using random instead.
added tracks to vector
added album to vector
added id to vector
added album to vector
airportname not recognized; using random instead.
airport_service not recognized; using random instead.
added airport to vector
added airport to vector
added location to vector
airport_aircraft not recognized; using random instead.
added airline to vector
added airline to vector
added name to vector
added flight to vector
added airline to vector
added flight to vector
aircraft_movements not recognized; using random instead.
added aircraft to vector
added aircraft to vector
added description to vector
air_date not recognized; using random instead.
added agreements to vector
added properties to vector
added agreed to vector
added selling to vector
added price to vector
added player to vector
added attributes to vector
added agility to vector
added player to vector
added attributes to vector
added aggression to vector
added agent to vector
added agencies to vector
added agency to vector
added details to vector
added agencies to vector
added ref to vector
added age to vector
added categories to vector
added age to vector
added category to vector
added description to vector
added affirmative to vector
affiliated_with not recognized; using random instead.
affected_region not recognized; using random instead.
advisory_requirement not recognized; using random instead.
added restriction to vector
added advance to vector
added purchase to vector
added adults to vector
added student to vector
added admit to vector
added term to vector
address_road not recognized; using random instead.
added customers to vector
added address to vector
added line to vector
added 2 to vector
added addresses to vector
added address to vector
added content to vector
added address to vector
address2 not recognized; using random instead.
added program to vector
added requirement to vector
added additional to vector
req not recognized; using random instead.
actual_orders not recognized; using random instead.
actual_order_products not recognized; using random instead.
added actual to vector
added orders to vector
added actual to vector
added order to vector
added date to vector
actual_destruction_date not recognized; using random instead.
added activity to vector
added activity to vector
added name to vector
added activity to vector
added customer to vector
added contact to vector
added channels to vector
added active to vector
added to to vector
added date to vector
added customer to vector
added contact to vector
added channels to vector
added active to vector
added from to vector
added date to vector
added achievements to vector
added reference to vector
added achievement to vector
added type to vector
added achievement to vector
added type to vector
added description to vector
added achievements to vector
added achievement to vector
added id to vector
added achievements to vector
added achievement to vector
added details to vector
accreditation_type not recognized; using random instead.
accreditation_level not recognized; using random instead.
accountnumber not recognized; using random instead.
account_details not recognized; using random instead.
added protein to vector
added accession to vector
added number to vector
added documents to vector
added access to vector
added count to vector
added acceptance to vector
added accelerator to vector
added compatible to vector
added browser to vector
added accelerator to vector
added id to vector
accelerator_compatible_browser not recognized; using random instead.
added player to vector
added attributes to vector
added acceleration to vector
added accelerate to vector
added customer to vector
added account to vector
added type to vector
acc_road not recognized; using random instead.
acc_regular_season not recognized; using random instead.
acc_percent not recognized; using random instead.
acc_home not recognized; using random instead.
added customer to vector
added account to vector
added balance to vector
added publication to vector
added abstract to vector
added abbreviation to vector
added dogs to vector
added abandoned to vector
added yes to vector
added or to vector
added no to vector
added review to vector
added a to vector
added id to vector
>= not recognized; using random instead.
added > to vector
added = to vector
<= not recognized; using random instead.
added < to vector
2fm_mhz not recognized; using random instead.
18_49_rating_share not recognized; using random instead.
added . to vector
added , to vector
added ) to vector
added ( to vector
%_change_2007 not recognized; using random instead.
!= not recognized; using random instead.
added unk to vector
sequence_start not recognized; using random instead.
sequence_end not recognized; using random instead.
copy_word not recognized; using random instead.
copy_schema not recognized; using random instead.
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b0abf7590>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4af01a8210>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab40d4e90>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49a1417910>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b081aa4d0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab4099850>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac33d290>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac6d2510>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac6d2e50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49f8be6690>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab403ded0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49a151bf50>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f499ec1eb50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4af00df090>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab411a390>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b0004b6d0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b00046690>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b00414b50>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8ad7ffd0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab4151f90>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac055490>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49f8be4f10>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49a118bd50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4af04c0bd0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49fa537b50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab435d150>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac48ed50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b00261090>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab44fd890>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac639710>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8ae15f10>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac36bbd0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b00043fd0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac5df8d0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab403ba10>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f499efaced0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8ace6a50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a88517e90>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab42d7f10>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab4201f90>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac1ab350>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4af0045090>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac1cb890>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab40a52d0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8aef2650>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab4559d10>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49f9dc3e50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b0051fa50>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8aef99d0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49a130d710>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b0038cdd0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f499d1affd0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab409bf90>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a18122f50>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b00301350>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f499fe17150>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4ab4066890>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac641710>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49f9b4a910>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4af04e5a90>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac586390>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4af05799d0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac7f79d0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f499d17eb50>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f499ff78810>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a001cc790>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8ace4d90>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f499eb68a90>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac6526d0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a887d2a90>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8ad3e910>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac6686d0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4aac4a2650>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a001f7910>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b007939d0>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b001365d0>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f49f8d5ff50>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
make mask data provider args: ['data/datasets/data_final_processed/train/train_encode.txt'] ['data/datasets/data_final_processed/train/train_decode.txt'] ['data/datasets/data_final_processed/train/train_decoder_mask.txt'] True None    
source_files ['data/datasets/data_final_processed/train/train_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4a8aebf510>
schema data source ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/train/train_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("train_input_fn/random_shuffle_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'train_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
variable name model/att_seq2seq/encode/source_embedding/W:0
variable name model/att_seq2seq/decode/target_embedding/W:0
make mask data provider args: ['data/datasets/data_final_processed/dev/dev_encode.txt'] ['data/datasets/data_final_processed/dev/dev_decode.txt'] ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt'] False 1    
source_files ['data/datasets/data_final_processed/dev/dev_encode.txt']
<seq2seq.data.split_tokens_decoder.SplitTokensDecoder object at 0x7f4b000e5090>
schema data source ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
schemas.data_sources ['data/datasets/data_final_processed/dev/dev_decoder_mask.txt']
items2 [u'target_tokens', u'target_len']
data_target Tensor("dev_input_fn/parallel_read_1/common_queue_Dequeue:1", dtype=string)
items_schema [u'decoder_mask']
tensor_schema [<tf.Tensor 'dev_input_fn/StringToNumber:0' shape=(?,) dtype=float32>]
